README.md

	La idea de escribir este texto es mostrar el proceso de inferencia que tomo para afrontar como resuelvo un problema.

clarity code challenge:

El desafío consiste en dos ejercicios

1. Parse the data with a time_init, time_end
Build a tool, that given the name of a file (with the format described above), an ​init_datetime​, an end_datetime​, and a ​Hostname​, returns:
a list of hostnames connected to the given host during the given period

ANALISIS del problema:

	Teniendo estos datos eb ficheros log:

	<unix_timestamp> <from-hostname> <to-hostname>
	1366815793 quark garak
	1366815795 brunt quark
	1366815811 lilac garak
	...

	Tengo que sacar los hostnames (from-hostname) partiendo de los siguientes datos de entrada:

		​init_datetime​, end_datetime​, to-hostname

	Es decir, dado una entrada tal que asi:

		​init_datetime​,end_datetime​,to-hostname
		1366815790,1366815811,garak

	La salida es:

		quark 1366815793
		lilac 1366815811

	Igual habría que convertir ese unix_timestamp en algo legible por humanos, pero por ahora lo dejo así porque voy a partir del hecho que para hacer los calculos entre ventanas de uso, 
	tendré tambien como parametro de entrada otro unix_timestamp que no deja de ser un numero Long, ideal para trabajar con estos datos. Solo hay que sumarlos, restarlos, comparar si son mayores 
	o menores...

	O una entrada tal que asi:

		1366815790,1366815795,quark

	La salida es:

		brunt 1366815795


	Esto parece un ejemplo perfecto para usar sparksql y dataframes, incluso podría pensar en una solucion de streaming de datos porque el fichero log puede crecer a cada segundo, por lo que,
	usar spark structured streaming para cargar el fichero tal y como se produce en una máquina remota es perfectamente factible y sólo habría que hacer mínimos cambios en el código para crear el 
	Dataframe. Por motivos de simplicidad, asumiré la primera opción, cargar un fichero csv de datos estático para crear el Dataframe y poder hacer una consulta SQL que resuelva el primer problema.

	La razón principal es que estos ficheros log pueden ser realmente gigantescos, de GB o TB diarios, por lo que su mejor lugar es un sistema distribuido de ficheros donde podamos distribuir dicha carga y trabajar con ellos en la memoria de cada uno de los nodos.
	Si necesitamos procesar ficheros más grandes, añadimos mas nodos al cluster (scale out) y/o ampliar la capacidad de cada uno de los nodos en memoria/cpu/hdd (scale up). 

	No tiene sentido crear una herramienta ad hoc para resolver un problema asi porque literalmente para tratar con ficheros que potencialmente son gigantescos, necesitaria crear un framework de procesamiento distribuido y si quisiera que fuese rápido y eficiente en tiempo, necesitaría que fuese un framework que trabaje con los datos en la memoria RAM de cada nodo. 

	Literalmente esa es la definicion de uso del framework Apache Spark, procesamiento distribuido en memoria de grandes cantidades de datos.

	Además, el problema se arregla montando una sentencia SQL de manera natural.

	Considero que no trabajar para este tipo de problemas con algo que no sea Apache Spark es como viajar al pasado viviendo en el presente.

en principio hay que 

	1) crear un esquema adecuado para esos campos. 
	2) crear un Dataframe con la informacion de ese fichero csv, usando el esquema anterior.
	3) registrar una tabla en memoria , 
	4) crear una sql tal que así más o menos:

		SELECT from,unix FROM LOGS WHERE $init_datetime < unix AND unix <=$end_datetime AND to = '${to_hostname}
	5) Mostrar datos, guardarlo en formato parquet, crear una tabla hive...



Entonces, el script completo, asumiendo que estoy detras de una spark-shell.

	import org.apache.spark.sql.types.StructType
	import org.apache.spark.sql.types.StringType
	import org.apache.spark.sql.types.LongType
	import org.apache.spark.sql.SparkSession

	// not needed using a spark-shell
	val spark = SparkSession.builder().appName("Spark SQL clarity.io examples").getOrCreate()

	val schema = new StructType().add("unix",LongType).add("from",StringType).add("to",StringType)

	val path = "/Users/aironman/gitProjects/alonsoir.github.io/test-clarity.csv"

	val dfLogsWithSchema = spark.sqlContext.read.format("csv").option("delimiter"," ").option("quote","").option("header", "false").schema(schema).load(path)

	dfLogsWithSchema.registerTempTable("LOGS")

	spark.sql("SELECT * FROM LOGS").show

	val init_datetime = 1366815790l
	
	val end_datetime = 1366815811l
	
	val to_hostname = "garak"
	
	println(s"USING to_hostname: $to_hostname" + s" init_datetime: $init_datetime" + s" end_datetime: $end_datetime")

	// i am printing data, but, i could save to a parquet file or whatever we have to.
	spark.sql(s"""SELECT from,unix FROM LOGS WHERE $init_datetime < unix AND unix <=$end_datetime AND to = '${to_hostname}'""").show(false)

Obviamente esto no es un programa completo, hay que añadir las variables para recoger por parametro el init_datetime, el end_datetime y to_hostname, falta la estructura del proyecto, 
las dependencias, construir el jar con maven o sbt y añadir, quizás, a la invocacion de spark-submit
los parametros de optimizacion necesario que variarán en función del cluster que esté en produccion. 
Lo ideal siempre es que el conjunto de datos al completo quepa en la RAM de todos los nodos, pero sobre todo en el Driver, por lo que debe estar adecuadamente provisionado. 

links

	https://stackoverflow.com/questions/39281152/spark-unix-timestamp-data-type-mismatch
	https://stackoverflow.com/questions/39926411/provide-schema-while-reading-csv-file-as-a-dataframe
	https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/
	https://stackoverflow.com/questions/29704333/spark-load-csv-file-as-dataframe

 2. Unlimited Input Parser
The tool should both parse previously written log files and terminate or collect input from a new log file while it's being written and run indefinitely.
The script will output, once every hour:
● a list of hostnames connected to a given (configurable) host during the last hour
● a list of hostnames received connections from a given (configurable) host during the last hour
● the hostname that generated most connections in the last hour
Both the number of log lines and hostnames can be very high. Consider implementing a CPU and memory-efficient solution. Please feel free to make assumptions as necessary with proper documentation.


Es decir, tenemos multiples ficheros log como el anterior, por lo que tengo que suponer que existen esos ficheros en multiples sistemas de ficheros externos,
y cada hora, tengo que agregar todos esos ficheros de log en uno sólo, de manera que, tengo que generar una salida que:

	1. mostrar una lista de maquinas que se hayan conectado a una maquina (configurable por parametro) durante la ultima hora.

		Algo asi, partiendo del primer ejercicio, capturando una ventana de tiempo agregado de una hora en un paso anterior del flujo de procesos, en el que hay que agregar los ficheros log producidos
		cada hora en un único fichero csv, crear un Dataframe con esos datos teniendo en cuenta el esquema que vaya a tener el fichero final, crear una tabla temporal donde podamos lanzar las sentencias sql,
		algo como así. Notese que ahora esa tabla LOGS, solo contendrá el agregado horario, por lo que no sería necesario añadir mas filtros al WHERE.

		SELECT from,unix FROM LOGS WHERE to = '${to_hostname}'

	2. Mostrar una lista de maquinas que hayan recibido  la conexion durante la ultima hora. Lo mismo que la anterior pero mostrando lo contrario.

		Algo asi, pero teniendo que tener en cuenta cuanto dura una hora...

		SELECT to,unix FROM LOGS WHERE from = '${from_hostname}'

	3. Mostrar el nombre de la máquina que haya generado mas conexiones en la última hora...

		Por definir, que quiere decir esto? quieren el nombre de la maquina from que haya generado más conexiones?
		el nombre de la maquina to que haya generado más conexiones? el mayor de los dos?

		Asumiendo que quieren el mayor de los entre los hosts, to y from, hay que hacer dos consultas en el que hay que sacar el numero de hosts, junto con el nombre del host, ordenarlos de manera decreciente y quedarte con el primero, de manera que tendremos la maquina from o to con mayor conexiones, y luego solo tendríamos que decidir cual de las dos es mayor.

		SELECT from, count(from) as cont FROM LOGS WHERE from = '${from_hostname}' ORDER BY from GROUP BY from,count

		en spark sería algo asi:

			df.select(df.col("from"), count("from")).filter(df.col("from") = {'${from_hostname}'}).groupBy(df.col("from")).agg(max(count("from"))).cache().show()

			df.select(df.col("to"), count("to")).filter(df.col("to") = {'${to_hostname}'}).groupBy(df.col("to")).agg(max(count("to"))).cache().show()

		Obviamente, mostrando los datos no voy a sacar el contador, pero puedo volcarlo a un fichero de texto y luego leerlo de ahí para operar con ellos. Tendría que mirar como hacer esto de manera más efectiva, admito que no es ideal.  

	¿Cómo lo resuelvo?, ¿Como agrego esos ficheros de log remotos en uno solo cada hora? 

	con alguna herramienta de streaming que haga la recoleccion cada hora? 
	usar el termino cada hora implica que un planificador de tareas como quartz debería ejecutar una tarea de recoleccion de datos de cada sistema de ficheros externo
	y agregarlos en un único fichero de datos donde se acabarían ejecutando los tres puntos anteriores. 
	Se puede hacer algo, 
		
		1. cada hora me conecto a cada sistema de ficheros externo y me traigo cada uno para finalmente agregarlo en uno final. Una vez agregados, renombro los ficheros de ese directorio
			de una manera que no vuelvan a ser procesados de nuevo, o los muevo a otro directorio. El caso es que no se vuelvan a procesar

			Para ello, primero necesito un job que se conecte a un sistema de ficheros externos y me traiga esos datos. Cada uno de esos ficheros estarán 
			en una máquina dados una ip y un puerto.
			Por ejemplo, usar algo como el job LogFileRecoveryTool.scala que está en este mismo repositorio Github.

				https://github.com/alonsoir/alonsoir.github.io/blob/master/LogFileRecoveryTool.scala

		2. Cada hora recolecto los datos almacenados en cada sistema de ficheros creados por cada Job LogFileRecoveryTool, agregando los datos en un fichero final.

			Para esto, puedo usar algun scheduler de tareas, de manera que se active cada hora, escaneando un directorio donde se hayan dejado los ficheros log de cada uno de los Job 
			spark streaming del punto anterior, cogerá esos ficheros log y creará un agregado de ficheros log final con todas las lineas. Se puede crear un fichero parquet, avro, csv sin comprimir...
			Este scheduler de tareas puede ser desde un simple trabajo CRON, hasta usar algún proceso java o scala que use alguna libreria como quartz. 

		3. Una vez que tengo ese fichero final, crear un proceso final spark, crear el DataFrame con ese fichero, crear la tabla temporal y hacer las consultas necesarias descritas en los tres puntos anteriores.

	Alternativas

	Seguramente esto se puede hacer de mejores maneras, por ejemplo, usando software propietario ya diseñado expresamente para este tipo de tareas. Al no tener acceso en el momento de escribir esto 
	sobre ese software propietario, decido usar software open source (apache spark, java y scala) que esté al alcance de todo el mundo.

	Alternativas dentro del mundo open source? se podría usar apache kafka (kafka-streams, ksql) para recolectar en cada topic particionado un fichero log externo para luego hacer el merge de todos los topics y trabajar de la misma manera que he usado spark para montar la consulta sql. Aunque en el momento de escribir esto, no veía claro como gestionar el merge cada hora, supongo que haciendo uso de una ventana de tiempo. Igual, seguramente podría pasar de usar quartz como planificador de tareas para delegar en spark o en kafka para hacer la agregacion cada hora de cada log. Con más tiempo, haría varias versiones de todas estas ideas para quedarme con la mejor idea consensuada con el equipo y la que de mejor rendimiento con menor uso de recursos y facilidad de uso.

	Se podría usar kibana y elasticsearch para hacer las consultas necesarias para resolver el problema. No tengo mucha experiencia con estas tecnologias, tengo proyectos en github que usan estas tecnologias
	pero no las he usado en un par de años, por lo que necesitaria un poco de tiempo en refrescar conocimiento.

	También merecería la pena echar un vistazo a este producto, https://www.datadoghq.com/dg/logs/benefits/?utm_source=Advertisement&utm_medium=TwitterAds&utm_campaign=TwitterAds-LwLVideoBenefits
	y a este otro: https://www.graylog.org
	 
links

	http://www.quartz-scheduler.org

	https://medium.com/search?q=spark%20streaming

	https://databricks.gitbooks.io/databricks-spark-reference-applications/content/logs_analyzer/chapter1/streaming.html

	https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala

	https://www.baeldung.com/spring-quartz-schedule

	https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html

	https://www.google.com/search?client=safari&rls=en&sxsrf=ALeKk0313EQfbDJdv-_r9IWg0GhCXuFzyQ%3A1584619451466&ei=u19zXoeOHI-elwTWoIiwAw&q=spark+streaming+consuming+log+files&oq=spark+streaming+consuming+log+files&gs_l=psy-ab.3...25984.28445..28818...0.0..0.93.1398.19......0....1..gws-wiz.......35i39j33i10.0n_Yp-DgSgA&ved=0ahUKEwjH14i8v6boAhUPz4UKHVYQAjYQ4dUDCAo&uact=5

	https://stackoverflow.com/questions/33009935/window-in-spark-streaming?rq=1

	https://www.google.com/search?client=safari&rls=en&sxsrf=ALeKk00X0b8koqoi2akRPmfDTsXV7tFqtQ%3A1584620095532&ei=P2JzXpWTIIu-aJuZmaAO&q=collecting+log+files+every+hour+using+structured+streaming&oq=collecting+log+files+every+hour+using+structured+streaming&gs_l=psy-ab.3...10051.14403..15546...2.2..0.378.1411.9j2j0j1......0....1..gws-wiz.......0i71j33i10.1o_8NKJxqzE&ved=0ahUKEwjVrpfvwaboAhULHxoKHZtMBuQQ4dUDCAo&uact=5

	http://www.diegocalvo.es/funciones-estadisticas-de-dataframes-en-scala/	

	https://allaboutscala.com/big-data/spark/#dataframe-sql-group-by-count-filter






