# Arquitectura Hexagonal.

# Descripción

El proposito de este fichero es describir como es la arquitectura hexagonal. Propositos puramente educativos.

tres paquetes principales, config, domain, infrastructure, colgando tanto de src.main.java, como de src.test.java.
Lo mejor siempre será tener test unitarios para todo.

# Estructura general de los paquetes.

src
	main
		java
			myapp
				config
				domain
					model
					ports
						primary
						secondary
					service
				infrastructure
					api
					entry
					repository
						event
							kafka
								adapter
								config
								consumer
								producer
					oracle
						dao
						dto
						mapper
					rest	
src
	test
		java
src
	main
		resources
src
	test
		resources

# DESCRIPCION DETALLADA DEL CONTENIDO DE LOS PAQUETES.

	myapp.config
	
		README: Add your global configuration classes here.
	
	myapp.domain.model

			README: pojos lombok, ejemplos: ConnectionResult, FileResult, etc...
			Usado por los mapeadores descritos en el paquete infrastructure.repository.oracle.mapper

			ej)

				package myapp.domain.model;

				import java.time.LocalDateTime;
				import lombok.Builder;
				import lombok.Data;

				@Data
				@Builder(toBuilder = true)
				public class HealthCheckResult {

				    private Integer personId;
				    private LocalDateTime createDate;
				    private String operation;
				}


	myapp.domain.ports.primary
	
				README: Put your inbound ports here. Interfaces 
				Usado por los controller declarados en infrastructure.api como atributo private final
				Usado por los UseCase declarados en domain.service como interfaz a implementar. 
	
				ej) 

				package myapp.domain.ports.primary;

				import myapp.domain.model.HealthCheckResult;
				import java.util.List;


				public interface IHealthCheckPrimaryPort {

				    List<HealthCheckResult> getLastItem();

				    String produceEvent(String payload);

				    String consumeEvent();
				}


	myapp.domain.ports.secondary
	
				README: Put your outbounds ports here. Interfaces Repository marcadas con @Component
				
				Usado por los UseCase declarados en domain.service como atributo private final
				Usado por las clases Repository declaradas en infrastructure.repository.oracle como interfaz a implementar.

				ej)

				package myapp.domain.ports.secondary;

				import myapp.domain.model.HealthCheckResult;
				import java.util.List;
				import org.springframework.stereotype.Component;

				@Component
				public interface IHealthCheckRepository {

				    List<HealthCheckResult> getLastItem();
				}


	myapp.domain.service
	
				README:Put your Use Case Logic here. Clases marcadas con @Service, implementan un puerto primario, tienen inyectado un puerto secundario, un repositorio 
				
				ej)

				package myapp.domain.service;

				import myapp.domain.model.HealthCheckResult;
				import myapp.domain.ports.primary.IHealthCheckPrimaryPort;
				import myapp.domain.ports.secondary.IHealthCheckEventRepository;
				import myapp.domain.ports.secondary.IHealthCheckRepository;
				import java.util.List;
				import lombok.RequiredArgsConstructor;
				import lombok.extern.slf4j.Slf4j;
				import org.springframework.beans.factory.annotation.Autowired;
				import org.springframework.stereotype.Service;

				@Service
				@Slf4j
				@RequiredArgsConstructor
				public class HealthCheckUseCase implements IHealthCheckPrimaryPort {

				    @Autowired
				    private final IHealthCheckRepository iHealthCheckRepository;
				    @Autowired
				    private final IHealthCheckEventRepository iHealthCheckEventRepository;

				    @Override
				    public List<HealthCheckResult> getLastItem() {

				        return iHealthCheckRepository.getLastItem();//.orElseThrow(AnnotationException::new);

				    }

				    @Override
				    public String produceEvent(String payload) {
				        return iHealthCheckEventRepository.publish(payload);
				    }

				    @Override
				    public String consumeEvent() {
				        return iHealthCheckEventRepository.consume();
				    }
				}
 

	myapp.infrastructure.api
		
			README: Put your api resources here. Controller classes. Marcadas con @RestController, @RequestMapping, usaran Swagger para documentar y generar métodos. Usados por el Test que controle la calidad del código.
		
			ej)

			package myapp.infrastructure.api;

			import myapp.domain.model.HealthCheckResult;
			import myapp.domain.ports.primary.IHealthCheckPrimaryPort;
			import myapp.infrastructure.rest.HealthCheckResponse;
			import io.swagger.v3.oas.annotations.Operation;
			import io.swagger.v3.oas.annotations.media.Content;
			import io.swagger.v3.oas.annotations.media.Schema;
			import io.swagger.v3.oas.annotations.responses.ApiResponse;
			import io.swagger.v3.oas.annotations.responses.ApiResponses;
			import java.text.DateFormat;
			import java.text.ParseException;
			import java.util.Date;
			import java.util.List;
			import java.util.Optional;
			import lombok.RequiredArgsConstructor;
			import lombok.extern.slf4j.Slf4j;
			import org.apache.commons.lang3.exception.ExceptionUtils;
			import org.springframework.beans.factory.annotation.Autowired;
			import org.springframework.http.MediaType;
			import org.springframework.web.bind.annotation.GetMapping;
			import org.springframework.web.bind.annotation.RequestMapping;
			import org.springframework.web.bind.annotation.ResponseBody;
			import org.springframework.web.bind.annotation.RestController;

			@RequiredArgsConstructor
			@RequestMapping("/healthcheck")
			@RestController

			@Slf4j
			public class HealthCheckController {


			    @Autowired
			    private final IHealthCheckPrimaryPort iHealthCheckPrimaryPort;

			    @Operation(summary = "Test server deployed")
			    @ApiResponse(responseCode = ApiConstants.RESPONSE_CODE_OK, description = ApiConstants.STATUS_OK,
			        content = {@Content(
			            mediaType = MediaType.APPLICATION_JSON_VALUE, schema = @Schema(implementation = HealthCheckResponse.class)
			        )})
			    @GetMapping
			    @ResponseBody
			    public HealthCheckResponse getHealthCheck() throws ParseException {
			        log.info("server deployed");
			        DateFormat dateTimeInstance = DateFormat.getDateTimeInstance(
			            DateFormat.SHORT, DateFormat.SHORT);

			        //ResponseEntity.ok();

			        return new HealthCheckResponse(ApiConstants.STATUS_OK, "Service is deployed",
			            dateTimeInstance.parse(dateTimeInstance.format(new Date())));

			    }


			    @Operation(summary = "Test access FOS")
			    @ApiResponses(value = {
			        @ApiResponse(responseCode = ApiConstants.RESPONSE_CODE_OK, description = ApiConstants.STATUS_OK,
			            content = {@Content(
			                mediaType = MediaType.APPLICATION_JSON_VALUE, schema = @Schema(implementation = HealthCheckResponse.class)
			            )}),
			        @ApiResponse(description = ApiConstants.STATUS_KO,
			            content = {@Content(
			                mediaType = MediaType.APPLICATION_JSON_VALUE, schema = @Schema(implementation = HealthCheckResponse.class)
			            )})
			    })

			    @GetMapping("fos")
			    @ResponseBody
			    public HealthCheckResponse getHealthFOS() throws ParseException {
			        log.info("server deployed");
			        DateFormat dateTimeInstance = DateFormat.getDateTimeInstance(
			            DateFormat.SHORT, DateFormat.SHORT);
			        try {
			            List<HealthCheckResult> result = iHealthCheckPrimaryPort.getLastItem();

			            String personId = result.stream().findFirst()
			                .flatMap(f -> Optional.ofNullable(f.getPersonId().toString())).orElse(null);

			            return new HealthCheckResponse(ApiConstants.STATUS_OK, personId,
			                dateTimeInstance.parse(dateTimeInstance.format(new Date())));
			        } catch (Exception e) {
			            log.error(">>>>> " + ExceptionUtils.getStackTrace(e));
			            log.error(">>>> " + e.getMessage());
			            return new HealthCheckResponse(ApiConstants.STATUS_KO, ExceptionUtils.getStackTrace(e),
			                dateTimeInstance.parse(dateTimeInstance.format(new Date())));
			        }

			    }


			    @Operation(summary = "Test launch exception")
			    @ApiResponse(responseCode = ApiConstants.RESPONSE_CODE_OK, description = ApiConstants.STATUS_KO,
			        content = {@Content(
			            mediaType = MediaType.APPLICATION_JSON_VALUE, schema = @Schema(implementation = HealthCheckResponse.class)
			        )})
			    @GetMapping("exception")
			    @ResponseBody
			    public HealthCheckResponse getHealthCheckException() throws ParseException {
			        DateFormat dateTimeInstance = DateFormat.getDateTimeInstance(
			            DateFormat.SHORT, DateFormat.SHORT);
			        log.info("launch exception");
			        try {
			            throw new Exception("Test exception");
			        } catch (Exception e) {
			            return new HealthCheckResponse(ApiConstants.STATUS_KO, e.getMessage(),
			                dateTimeInstance.parse(dateTimeInstance.format(new Date())));
			        }


			    }


			    /*
			    @Nonnull
			    @ExceptionHandler
			    public ResponseEntity<String> handleCustomException(@Nonnull final CustomException exception) {
			        return new ResponseEntity<>("Custom Exception occured", HttpStatus.NO_CONTENT);
			    }

			    public class CustomException extends Throwable {

			    }
			    */


			}


	myapp.infrastructure.entry

			README: Son clases Controller, no acaban en Controller como las que están en api. Marcadas con @RestController, @RequestMapping, usaran Swagger para documentar y generar métodos. 

			PREGUNTA: ¿Qué diferencia hay entre los controladores de api y estos?
		
			ej)

			package myapp.infrastructure.entry;

			import myapp.domain.ports.primary.IHealthCheckPrimaryPort;
			import myapp.infrastructure.api.ApiConstants;
			import myapp.infrastructure.rest.HealthCheckResponse;
			import io.swagger.v3.oas.annotations.Operation;
			import io.swagger.v3.oas.annotations.media.Content;
			import io.swagger.v3.oas.annotations.media.Schema;
			import io.swagger.v3.oas.annotations.responses.ApiResponse;
			import java.text.DateFormat;
			import java.text.ParseException;
			import java.util.Date;
			import lombok.RequiredArgsConstructor;
			import lombok.extern.slf4j.Slf4j;
			import org.springframework.beans.factory.annotation.Autowired;
			import org.springframework.http.MediaType;
			import org.springframework.web.bind.annotation.GetMapping;
			import org.springframework.web.bind.annotation.PostMapping;
			import org.springframework.web.bind.annotation.RequestMapping;
			import org.springframework.web.bind.annotation.RequestParam;
			import org.springframework.web.bind.annotation.ResponseBody;
			import org.springframework.web.bind.annotation.RestController;

			@RequiredArgsConstructor
			@RequestMapping("/eventshealthcheck")
			@RestController

			@Slf4j
			public class HealthCheckEntry {

			    @Autowired
			    private final IHealthCheckPrimaryPort iHealthCheckPrimaryPort;

			    @Operation(summary = "Test producer event")
			    @ApiResponse(responseCode = ApiConstants.RESPONSE_CODE_OK, description = ApiConstants.STATUS_OK,
			        content = {@Content(
			            mediaType = MediaType.APPLICATION_JSON_VALUE, schema = @Schema(implementation = HealthCheckResponse.class)
			        )})
			    @PostMapping("producer")
			    @ResponseBody
			    public HealthCheckResponse setProducerEvent(final @RequestParam("message") String message)
			        throws ParseException {
			        log.info("setProducerEvent");
			        DateFormat dateTimeInstance = DateFormat.getDateTimeInstance(
			            DateFormat.SHORT, DateFormat.SHORT);

			        String response = iHealthCheckPrimaryPort.produceEvent(message);

			        return new HealthCheckResponse(ApiConstants.STATUS_OK, response,
			            dateTimeInstance.parse(dateTimeInstance.format(new Date())));

			    }

			    @Operation(summary = "Test consumer event")
			    @ApiResponse(responseCode = ApiConstants.RESPONSE_CODE_OK, description = ApiConstants.STATUS_OK,
			        content = {@Content(
			            mediaType = MediaType.APPLICATION_JSON_VALUE, schema = @Schema(implementation = HealthCheckResponse.class)
			        )})
			    @GetMapping("consumer")
			    @ResponseBody
			    public HealthCheckResponse setConsumerEvent() throws ParseException {
			        log.info("setConsumerEvent");
			        DateFormat dateTimeInstance = DateFormat.getDateTimeInstance(
			            DateFormat.SHORT, DateFormat.SHORT);

			        String response = iHealthCheckPrimaryPort.consumeEvent();

			        return new HealthCheckResponse(ApiConstants.STATUS_OK, response,
			            dateTimeInstance.parse(dateTimeInstance.format(new Date())));

			    }

			    /
			    *private final AtomicBoolean closed = new AtomicBoolean(false);
			    private KafkaConsumer<String, String> consumer;

			    public void consume() {
			        Properties props = new Properties();

			        // ClientId
			        props.put(ConsumerConfig.CLIENT_ID_CONFIG, "testclientid");

			        // Groupid
			        props.put(ConsumerConfig.GROUP_ID_CONFIG, "testgroupid");

			        // Kafka connection
			        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka-dev-0.europe.intranet:9092");

			        // Deserializer
			        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
			        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

			        consumer = new KafkaConsumer<>(props);
			        consumer.subscribe(Arrays.asList("testtopic"));
			        try {
			            while (!closed.get()) {
			                ConsumerRecords<String, String> records = consumer.poll(100);
			                for (ConsumerRecord<String, String> record : records) {
			                    final Object value = record.value();
			                    // Your event processing implementation
			                    // Etc...

			                    // Kafka Headers - Retrieve Kafka headers from Consumer record
			                    Iterator<Header> iterator = record.headers().iterator();
			                    while(iterator.hasNext()) {
			                        Header header = iterator.next();
			                        if(header.key().equals("signature")){
			                            // TODO : process signature header
			                        }
			                    }
			                }
			            }
			        } catch (WakeupException e) {
			            // Ignore Exception if closing
			            if (!closed.get()) throw e;
			        } finally {
			            consumer.close();
			        }
			    }

			    // Close the consumer
			    @PreDestroy
			    private void closeConsumer() {
			        closed.set(true);
			        consumer.wakeup();
			    }*/
			}


	myapp.infrastructure.repository.event.kafka.adapter
					
						Clases marcadas con @Component, implementan puertos secundarios. 

						Tienen como atributos privados a los consumidores y productores guardados
						en los paquetes infrastructure.repository.event.kafka.consumer y infrastructure.repository.event.kafka.producer

						PREGUNTA!! Esta clase adapter es la encargada de enriquecer con FOS?
			
						ej)

						package myapp.infrastructure.repository.event.kafka.adapter;

						import myapp.domain.ports.secondary.IHealthCheckEventRepository;
						import myapp.infrastructure.repository.event.kafka.consumer.HealthCheckEventConsumer;
						import myapp.infrastructure.repository.event.kafka.producer.HealthCheckEventProducer;
						import java.util.Objects;
						import lombok.RequiredArgsConstructor;
						import lombok.extern.slf4j.Slf4j;
						import org.springframework.stereotype.Component;

						@Slf4j
						@Component
						@RequiredArgsConstructor
						public class HealthCheckEventAdapter implements IHealthCheckEventRepository {

						    private final HealthCheckEventProducer healthCheckEventProducer;
						    private final HealthCheckEventConsumer healthCheckEventConsumer;


						    @Override
						    public String publish(String payload) {
						        Objects.requireNonNull(payload, "The payload cannot be null");
						        log.info("---------------------------------------------------------");
						        log.info("Event Produced: {}", payload);
						        log.info("---------------------------------------------------------");

						        healthCheckEventProducer.sendMessage(payload);
						        return null;
						    }

						    @Override
						    public String consume() {
						        return healthCheckEventConsumer.consume();
						    }


						}


	myapp.infrastructure.repository.event.kafka.config
					
						Clases marcadas con @EnableKafka y @Configuration para las configuracions de los consumidores y productores kafka. También puede venir una clase para procesar el properties, normalmente un pojo marcado con @Data, @Configuration y @ConfigurationProperties(prefix = "kafka") junto con los atributos privados que representan dicha configuración. 

						Usados directamente (KafkaConfigProperties) como atributo private final en las clases KafkaConsumerConfig y KafkaProducerConfig y en las clases Consumer declaradas en infrastructure.repository.event.kafka.consumer y en las clases Producer declaradas en 
						infrastructure.repository.event.kafka.producer.

						PREGUNTA!! @ConfigurationProperties es la encargada de leer el yml?
						
						ej)

						package myapp.infrastructure.repository.event.kafka.config;

						import lombok.Data;
						import org.springframework.boot.autoconfigure.kafka.KafkaProperties.Ssl;
						import org.springframework.boot.context.properties.ConfigurationProperties;
						import org.springframework.context.annotation.Configuration;

						@Data
						@Configuration
						@ConfigurationProperties(prefix = "kafka")
						public class KafkaConfigProperties {

						    private String bootstrapServers;
						    private String schemaRegistryUrl;

						    private String clientId;
						    private String groupId;
						    private String audOpeningsTopic;

						    private Ssl ssl;
						    private String encryptionKey;


						}

						package myapp.infrastructure.repository.event.kafka.config;

						import com.ing.apisdk.toolkit.connectivity.kafka.avro.serde.DecryptingKafkaAvroDeserializer;
						import com.ing.apisdk.toolkit.connectivity.kafka.avro.serde.EncryptionAwareSerDeConfig;
						import io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;
						import java.util.HashMap;
						import java.util.Map;
						import org.apache.avro.generic.GenericRecord;
						import org.apache.kafka.clients.consumer.ConsumerConfig;
						import org.apache.kafka.common.serialization.StringDeserializer;
						import org.springframework.beans.factory.annotation.Autowired;
						import org.springframework.context.annotation.Bean;
						import org.springframework.context.annotation.Configuration;
						import org.springframework.kafka.annotation.EnableKafka;
						import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
						import org.springframework.kafka.core.ConsumerFactory;
						import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
						import org.springframework.kafka.listener.LoggingErrorHandler;
						import org.springframework.kafka.support.serializer.ErrorHandlingDeserializer;

						@EnableKafka
						@Configuration
						public class KafkaConsumerConfig {

						    private static final String SCHEMA_REGISTRY_URL = "schema.registry.url";
						    private final KafkaConfigProperties kafkaConfigProperties;

						    @Autowired
						    public KafkaConsumerConfig(KafkaConfigProperties kafkaConfigProperties) {
						        this.kafkaConfigProperties = kafkaConfigProperties;
						    }


						    @Bean
						    public Map<String, Object> consumerProperties() {
						        Map<String, Object> props = new HashMap<>();

						        props.put(ConsumerConfig.CLIENT_ID_CONFIG, kafkaConfigProperties.getClientId());
						        props.put(ConsumerConfig.GROUP_ID_CONFIG, kafkaConfigProperties.getGroupId());

						        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
						            kafkaConfigProperties.getBootstrapServers());
						        props.put(SCHEMA_REGISTRY_URL, kafkaConfigProperties.getSchemaRegistryUrl());
						        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
						        // Esto no lo entiendo. Por qué para gestionar la clave y el payload del mensaje a la hora de consumir el mensaje se usa
						        //    ErrorHandlingDeserializer?
						        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
						            ErrorHandlingDeserializer.class.getName());
						        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
						            ErrorHandlingDeserializer.class.getName());

						        //delegate
						        props.put(ErrorHandlingDeserializer.KEY_DESERIALIZER_CLASS,
						            StringDeserializer.class.getName());
						        props.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS,
						            DecryptingKafkaAvroDeserializer.class.getName());

						        props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, "true");
						//        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
						//        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
						        props.put(EncryptionAwareSerDeConfig.SHARED_SECRET_CONFIG,
						            kafkaConfigProperties.getEncryptionKey());

						////        props.put(ConsumerConfig.CLIENT_ID_CONFIG, kafkaConfigProperties.getClientId());
						//            props.put(ConsumerConfig.GROUP_ID_CONFIG, kafkaConfigProperties.getGroupId());
						//
						//            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
						//                kafkaConfigProperties.getBootstrapServers());
						//            props.put(SCHEMA_REGISTRY_URL, kafkaConfigProperties.getSchemaRegistryUrl());
						//            props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
						//            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
						//                ErrorHandlingDeserializer.class.getName());
						//            props.put("spring.deseriaizer.value.delegate.class", StringDeserializer.class.getName());
						//            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
						//                ErrorHandlingDeserializer.class.getName());
						//            props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, "true");
						////        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
						//            props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
						//            props.put(EncryptionAwareSerDeConfig.SHARED_SECRET_CONFIG,
						//                kafkaConfigProperties.getEncryptionKey());

						        return props;
						    }

						   /* @Bean
						    public DefaultKafkaConsumerFactory<String, String> consumerFactory() {
						        return new DefaultKafkaConsumerFactory<>(consumerProperties());
						    }

						    @Bean
						    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
						        ConcurrentKafkaListenerContainerFactory<String, String> factory =
						            new ConcurrentKafkaListenerContainerFactory<>();
						        factory.setConsumerFactory(consumerFactory());

						        return factory;
						    }*/

						    @Bean
						    public ConsumerFactory<String, GenericRecord> consumerFactory() {
						        return new DefaultKafkaConsumerFactory<>(consumerProperties());
						    }

						    @Bean("avroListenerFactory")
						    public ConcurrentKafkaListenerContainerFactory<String, GenericRecord> kafkaListenerContainerFactory() {
						        ConcurrentKafkaListenerContainerFactory<String, GenericRecord> factory =
						            new ConcurrentKafkaListenerContainerFactory<>();
						        factory.setConsumerFactory(consumerFactory());

						        return factory;
						    }

						    /**
						     * Boot will autowire this into the container factory.
						     */
						    @Bean
						    public LoggingErrorHandler errorHandler() {
						        return new LoggingErrorHandler();
						    }
						}

						package myapp.infrastructure.repository.event.kafka.config;

						import java.util.HashMap;
						import java.util.Map;
						import lombok.extern.slf4j.Slf4j;
						import org.apache.kafka.clients.producer.ProducerConfig;
						import org.apache.kafka.common.config.SslConfigs;
						import org.apache.kafka.common.serialization.StringSerializer;
						import org.springframework.beans.factory.annotation.Autowired;
						import org.springframework.context.annotation.Bean;
						import org.springframework.context.annotation.Configuration;
						import org.springframework.kafka.core.DefaultKafkaProducerFactory;
						import org.springframework.kafka.core.KafkaTemplate;
						import org.springframework.kafka.core.ProducerFactory;

						@Slf4j
						@Configuration
						public class KafkaProducerConfig {

						    private static final String SCHEMA_REGISTRY_URL = "schema.registry.url";
						    private static final String SECURITY_PROTOCOL = "security.protocol";
						    private final KafkaConfigProperties kafkaConfigProperties;

						    @Autowired
						    public KafkaProducerConfig(KafkaConfigProperties kafkaConfigProperties) {
						        this.kafkaConfigProperties = kafkaConfigProperties;
						    }

						    @Bean
						    public Map<String, Object> producerConfig() {

						        Map<String, Object> props = new HashMap<>();

						        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
						            kafkaConfigProperties.getBootstrapServers());
						        props.put(SCHEMA_REGISTRY_URL, kafkaConfigProperties.getSchemaRegistryUrl());
						        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
						        //  props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
						        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
						        if (kafkaConfigProperties.getSsl() != null) {
						            log.warn("TECH DOUBT: REVIEW SSL PROPERTIES!!!");
						            props.put(SECURITY_PROTOCOL, kafkaConfigProperties.getSsl().getProtocol());
						//            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG,
						//                kafkaConfigProperties.getSsl().getTruststore().getLocation());
						            props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG,
						                kafkaConfigProperties.getSsl().getTrustStorePassword());
						//            props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG,
						//                kafkaConfigProperties.getSsl().getKeystore().getLocation());
						            props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG,
						                kafkaConfigProperties.getSsl().getKeyStorePassword());
						            props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG,
						                kafkaConfigProperties.getSsl().getKeyPassword());
						        }

						        return props;
						    }

						    @Bean
						    public ProducerFactory<String, String> producerFactory() {
						        return new DefaultKafkaProducerFactory<>(producerConfig());
						    }

						    @Bean
						    public KafkaTemplate<String, String> kafkaTemplate() {
						        return new KafkaTemplate<>(producerFactory());
						    }

						    /*@Bean
						    public ProducerFactory<String, GenericRecord> producerFactory() {
						        return new DefaultKafkaProducerFactory<>(producerConfig());
						    }

						    @Bean
						    public KafkaTemplate<String, GenericRecord> kafkaTemplate() {
						        return new KafkaTemplate<>(producerFactory());
						    }
						    */

						}

	myapp.infrastructure.repository.event.kafka.consumer

						Clase marcada con @Component, tienen un método listen marcado por     
						@KafkaListener(topics = "${kafka.audOpeningsTopic}", containerFactory = "avroListenerFactory")

						PREGUNTA!! debe devolver void? debería devolver el evento consumido. Su tarea debería ser consumir, puede que filtrar si en dicho topic vienen eventos de muchos tipos...
						
						Usado por los Adapter (HealthCheckEventAdapter) declarados en infrastructure.repository.event.event.kafka.adapter

						ej)

						package myapp.infrastructure.repository.event.kafka.consumer;

						import com.ing.eventbus.avro.catalog.es.fos.AudOpenings;
						import lombok.RequiredArgsConstructor;
						import lombok.extern.slf4j.Slf4j;
						import org.apache.kafka.clients.consumer.ConsumerRecord;
						import org.springframework.kafka.annotation.KafkaListener;
						import org.springframework.stereotype.Component;

						@Slf4j
						@Component
						@RequiredArgsConstructor
						public class EsAudOpeningsEventConsumer {

						    
						    @KafkaListener(topics = "${kafka.audOpeningsTopic}", containerFactory = "avroListenerFactory")
						    public void listen(ConsumerRecord<String, AudOpenings> aud) throws Exception {
						        log.info(">> Received Message in topic {}: ", aud.topic());
						        AudOpenings receivedAudOpening = aud.value();
						    	...
						    }


						}

						package myapp.infrastructure.repository.event.kafka.consumer;

						import myapp.infrastructure.repository.event.kafka.config.KafkaConfigProperties;
						import myapp.infrastructure.repository.event.kafka.config.KafkaConsumerConfig;
						import java.time.Duration;
						import java.util.Collections;
						import java.util.Iterator;
						import java.util.List;
						import java.util.concurrent.atomic.AtomicBoolean;
						import javax.annotation.PreDestroy;
						import lombok.RequiredArgsConstructor;
						import lombok.extern.slf4j.Slf4j;
						import org.apache.kafka.clients.consumer.ConsumerRecord;
						import org.apache.kafka.clients.consumer.ConsumerRecords;
						import org.apache.kafka.clients.consumer.KafkaConsumer;
						import org.apache.kafka.clients.consumer.OffsetAndMetadata;
						import org.apache.kafka.common.errors.WakeupException;
						import org.springframework.stereotype.Component;

						@Slf4j
						@Component
						@RequiredArgsConstructor
						public class HealthCheckEventConsumer {

						    private final KafkaConfigProperties kafkaConfigProperties;
						    private final KafkaConsumerConfig kafkaConsumerConfig;
						    private final AtomicBoolean closed = new AtomicBoolean(false);
						    private KafkaConsumer<String, String> consumer;

							@KafkaListener(topics = "${kafka.defaultTopic}", groupId = "${kafka.groupId}")
							@KafkaListener(groupId = "${kafka.groupId}",topicPartitions = @TopicPartition(topic = "${kafka.defaultTopic}", partitionOffsets = {@PartitionOffset(partition = "0", initialOffset = "0")}))
							public void listen(ConsumerRecord<?, ?> cr, @Payload String message,@Header(KafkaHeaders.RECEIVED_PARTITION_ID) int partition,@Header(KafkaHeaders.OFFSET) int offset) throws Exception {
							
								log.info(">> Received Message in topic {}: {}", cr.topic(), cr.toString());
								log.info(">> Received message [{}] from partition-{} with offset-{}", message, partition,offset);
							}


						    public String consume() {
						        int numMsgReceived = 0;
						        try {
						            consumer = new KafkaConsumer<>(kafkaConsumerConfig.consumerProperties());

						            consumer.assign(Collections
						                .singleton(new org.apache.kafka.common.TopicPartition(
						                    kafkaConfigProperties.getAudOpeningsTopic(), 1)));

						//            consumer.subscribe(Arrays.asList(kafkaConfigProperties.getDefaultTopic()));

						            StringBuffer messages = new StringBuffer();
						            while (!closed.get()) {
						                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
						                for (org.apache.kafka.common.TopicPartition partition : records.partitions()) {
						                    List<ConsumerRecord<String, String>> partitionRecords = records
						                        .records(partition);
						                    for (ConsumerRecord<String, String> record : partitionRecords) {
						                        //for (ConsumerRecord<String, String> record : records) {
						                        numMsgReceived++;
						                        final Object value = record.value();
						                        log.info(
						                            ">> consumed: key = %s, value = %s, partition id= %s, offset = %s%n",
						                            record.key(), record.value(), record.partition(), record.offset());
						                        messages.append(value);
						                        messages.append(" ## ");
						                        // Your event processing implementation
						                        // Etc...

						                        // Kafka Headers - Retrieve Kafka headers from Consumer record
						                        Iterator<org.apache.kafka.common.header.Header> iterator = record.headers()
						                            .iterator();
						                        while (iterator.hasNext()) {
						                            org.apache.kafka.common.header.Header header = iterator.next();
						                            if (header.key().equals("signature")) {
						                                // TODO : process signature header
						                            }
						                        }
						                    }
						                    long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset();
						                    consumer.commitSync(
						                        Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1)));
						                    if (numMsgReceived > 2) {
						                        break;
						                    }
						                }


						            }
						            return messages.toString();
						        } catch (WakeupException e) {
						            e.printStackTrace();
						            // Ignore Exception if closing
						            if (!closed.get()) {
						                throw e;
						            }
						        } catch (Exception e1) {
						            e1.printStackTrace();
						            if (!closed.get()) {
						                throw e1;
						            }
						        } finally {
						            consumer.close();
						        }

						        return "KO";
						    }

						    // Close the consumer
						    @PreDestroy
						    private void closeConsumer() {
						        closed.set(true);
						        consumer.wakeup();
						    }
						}


	myapp.infrastructure.repository.event.kafka.producer

						Clases marcadas con @Component. Tendrá inyectado KafkaTemplate<KEY-class,VALUE-class> o Producer, asi como lo necesario para tenerlos configurados, como KafkaConfigProperties y KafkaProducerConfig. 

						Usados por las clases Adapter (HealthCheckEventAdapter) declaradas en infrastructure.repository.event.kafka.adapter

						ej)
						No estoy muy seguro de ser el mejor ejemplo porque usa tanto KafkaTemplate como Producer, clases commodity que realizan 
						el envio del mensaje. Un Productor debería usar uno u otro, no los dos a la vez.

						package myapp.infrastructure.repository.event.kafka.producer;

						import myapp.infrastructure.repository.event.kafka.config.KafkaConfigProperties;
						import myapp.infrastructure.repository.event.kafka.config.KafkaProducerConfig;
						import javax.annotation.PreDestroy;
						import lombok.RequiredArgsConstructor;
						import lombok.extern.slf4j.Slf4j;
						import org.apache.kafka.clients.producer.Producer;
						import org.springframework.beans.factory.annotation.Autowired;
						import org.springframework.kafka.core.KafkaTemplate;
						import org.springframework.kafka.support.SendResult;
						import org.springframework.stereotype.Component;
						import org.springframework.util.concurrent.ListenableFuture;
						import org.springframework.util.concurrent.ListenableFutureCallback;

						@Slf4j
						@Component
						@RequiredArgsConstructor
						public class HealthCheckEventProducer {

						    private final KafkaConfigProperties kafkaConfigProperties;
						    private final KafkaProducerConfig kafkaProducerConfig;

						    @Autowired
						    private KafkaTemplate<String, String> kafkaTemplate;

						    Producer<String, String> producer;


						    public void sendMessage(String payload) {

						        try {

						            ListenableFuture<SendResult<String, String>> future =
						                kafkaTemplate.send(kafkaConfigProperties.getAudOpeningsTopic(), payload);

						            future.addCallback(new ListenableFutureCallback<SendResult<String, String>>() {

						                @Override
						                public void onSuccess(SendResult<String, String> result) {
						                    System.out.println("Sent message=[" + payload +
						                        "] with offset=[" + result.getRecordMetadata().offset() + "]");
						                }

						                @Override
						                public void onFailure(Throwable ex) {
						                    System.out.println("Unable to send message=["
						                        + payload + "] due to : " + ex.getMessage());
						                }
						            });

						           /* producer = new KafkaProducer<String, String>(kafkaProducerConfig.producerConfig());

						            //Produce event
						            RecordMetadata recordMetadata = producer
						                .send(new ProducerRecord<>(kafkaConfigProperties.getDefaultTopic(), payload)).get();
						            log.info(
						                "Event correctly sent to {} topic with offset: {}, to partition: {}",
						                recordMetadata.topic(), recordMetadata.offset(), recordMetadata.partition());*/
						        } catch (Exception e) {
						            log.error("Error sending Event to Kafka", e.getMessage());
						            e.printStackTrace();
						        }

						        ;

						    }


						    // Close the producer
						    @PreDestroy
						    private void closeProducer() {
						        producer.close();
						    }


						/*
						    private void sendTransactionRawEvent(GenericRecord body) {
						        send(body).addCallback(this::onSuccess, this::onFailure);
						    }

						    private ListenableFuture<SendResult<String, GenericRecord>> send(GenericRecord message) {
						        return kafkaTemplate
						            .send(kafkaConfigProperties.getDefaultTopic(), message);
						    }

						    private void onFailure(Throwable throwable) {
						        log.error("Error sending TransactionRaw Event to Kafka", throwable);
						    }

						    private void onSuccess(SendResult<String, GenericRecord> eventResult) {
						        log.info(
						            "TransactionRaw correctly sent to outgoing topic with offset: {}, to partition: {}",
						            eventResult.getRecordMetadata().offset(),
						            eventResult.getProducerRecord().partition());
						    }
						*/
						}


			
	myapp.infrastructure.repository.oracle.dao
			
					Interface marcada con @CircuitBreakerConfig(type = DATABASE_PROVIDER.SQL) propietaria, extiende MerakMapper que tiene un metodo que hace log. Basicamente es un DAO que usa Mybatis para lanzar consultas, por lo que el método de la interfaz viene marcado con @Select y @Results. Los campos a los que hace referencia en @Results son los campos del DTO.

					Usado por clases Repository (HealthCheckRepository) declaradas en el paquete myapp.infrastructure.repository.oracle

					ej)

					package myapp.infrastructure.repository.oracle.dao;

					import com.ing.apisdk.merak.mybatis.annotation.CircuitBreakerConfig;
					import com.ing.apisdk.merak.mybatis.annotation.DATABASE_PROVIDER;
					import com.ing.apisdk.merak.mybatis.aop.MerakMapper;
					import myapp.infrastructure.repository.oracle.dto.HealthCheckResponseDto;
					import java.util.List;
					import org.apache.ibatis.annotations.Result;
					import org.apache.ibatis.annotations.Results;
					import org.apache.ibatis.annotations.Select;

					//org.apache.ibatis.annotations.Mapper
					//@Mapper 
					@CircuitBreakerConfig(type = DATABASE_PROVIDER.SQL)
					public interface HealthCheckRepoDaoMapper extends MerakMapper {
					  
					    @Select("select PERSONID, OPERATION, CREATEDATE "
					        + "from AUD_OPENINGS "
					        + "where rownum<2  "
					        + "order by CREATEDATE desc")
					    @Results({
					        @Result(property = "personId", column = "PERSONID"),
					        @Result(property = "createDate", column = "CREATEDATE"),
					        @Result(property = "operation", column = "OPERATION")
					    })
					    List<HealthCheckResponseDto> getLastItem();
					}


	myapp.infrastructure.repository.oracle.dto

					Clases DTO que usan etiquetas @Builder(toBuilder = true), @Data, @AllArgsConstructor, @NoArgsConstructor, 
					@ErrorHandlerAnnotation(errorHandler = ErrorRowHandler.class)
					Usa Lombok y etiquetas privadas de ING (ErrorHandlerAnnotation).
					con Atributos privados.

					Usado por los Mapper (HealthCheckRepoMapper) declarados en myapp.infrastructure.repository.oracle.mapper

					ej)
					package myapp.infrastructure.repository.oracle.dto;

					import com.ing.apisdk.merak.mybatis.mrpc.annotation.ErrorHandlerAnnotation;
					import com.ing.apisdk.merak.mybatis.mrpc.error.ErrorRowHandler;
					import java.time.LocalDateTime;
					import lombok.AllArgsConstructor;
					import lombok.Builder;
					import lombok.Data;
					import lombok.NoArgsConstructor;

					@Builder(toBuilder = true)
					@Data
					@AllArgsConstructor
					@NoArgsConstructor
					@ErrorHandlerAnnotation(errorHandler = ErrorRowHandler.class)
					public class HealthCheckResponseDto {

					    Integer personId;
					    LocalDateTime createDate;
					    String operation;

					}



	myapp.infrastructure.repository.oracle.mapper

					Clase marcada con @Component que hace el mapeo de DTO a una clase de Doman.model.
					Se suelen usar las etiquetas de lombok @Data, @Builder(toBuilder = true)

					Usado por las clases Repository (HealthCheckRepository) declaradas en myapp.infrastructure.repository.oracle

					ej)

					package myapp.infrastructure.repository.oracle.mapper;


					import myapp.domain.model.HealthCheckResult;
					import myapp.infrastructure.repository.oracle.dto.HealthCheckResponseDto;
					import java.util.List;
					import java.util.stream.Collectors;
					import org.springframework.stereotype.Component;

					@Component
					public class HealthCheckRepoMapper {

					    public List<HealthCheckResult> mapToDomain(List<HealthCheckResponseDto> response) {
					        return response.stream()
					            .map(HealthCheckRepoMapper::addDtoToList)
					            .collect(Collectors.toList());
					    }


					    private static HealthCheckResult addDtoToList(HealthCheckResponseDto response) {

					        return HealthCheckResult.builder().createDate(response.getCreateDate())
					            .operation(response.getOperation())
					            .personId(response.getPersonId())
					            .build();

					    }
					}

	myapp.infrastructure.repository.oracle

					Clases de tipo repositorio que implementan una interfaz definida en domain.ports.secondary
					
					Usado por 

					ej)

					package myapp.infrastructure.repository.oracle;

					import myapp..domain.model.HealthCheckResult;
					import myapp..domain.ports.secondary.IHealthCheckRepository;
					import myapp..infrastructure.repository.oracle.dao.HealthCheckRepoDaoMapper;
					import myapp..infrastructure.repository.oracle.mapper.HealthCheckRepoMapper;
					import java.util.List;
					import lombok.RequiredArgsConstructor;
					import lombok.extern.slf4j.Slf4j;
					import org.springframework.beans.factory.annotation.Autowired;
					import org.springframework.stereotype.Component;


					@Slf4j
					@Component
					@RequiredArgsConstructor
					public class HealthCheckRepository implements IHealthCheckRepository {

					    private static final String NOT_DATA_FOUND = "100004";

					    @Autowired
					    HealthCheckRepoMapper healthCheckRepoMapper;
					    @Autowired
					    HealthCheckRepoDaoMapper healthCheckRepoDaoMapper;


					    public List<HealthCheckResult> getLastItem() {
					        log.debug(">> HealthCheckRepository.getLastItem");

					//        try {

					        return healthCheckRepoMapper.mapToDomain(healthCheckRepoDaoMapper.getLastItem());

					//        } catch (Exception e) {
					//            log.error(">>>> ");
					//            if (checkException(e)) {
					//                throw e;
					//            }
					//        }
					//        return Collections.emptyList();
					    }

					//    private boolean checkException(Exception e) {
					//        log.error(">>>>>>>>>>>>>>>> " + e.getMessage());
					//        //log.error(">>>>>>>>>>>>>>>> " + e.getCause().getMessage());
					//        e.printStackTrace();
					//
					//        return !(e.getCause().getCause() instanceof SQLException) ||
					//            !Optional.ofNullable((SQLException) e.getCause().getCause()).isPresent() ||
					//            !NOT_DATA_FOUND.equals(Optional.ofNullable((SQLException) e.getCause().getCause())
					//                .map(SQLException::getSQLState).get());
					//    }


					}

	myapp.infrastructure.rest

			README: Clases que usan la etiqueta @Data de lombok para definir la respuesta gestionada por 
			los controllers definidos en infraestructure.api.

			Usado por las clases Controller (HealthCheckController) declarados en el paquete myapp.infrastructure.api: 

			ej)

			package myapp.infrastructure.rest;

			import java.util.Date;
			import lombok.Data;

			@Data
			public class HealthCheckResponse {

			    private final String status;
			    private final String message;
			    private final Date currentDate;

			}

src.main.resources
		application.yml
		application-local.yml
src.main.resources.avro
			fichero-key.avsc
			fichero-value.avsc
src.main.resources.bak
			ficheroSchema.avsc

src.test.java

	...