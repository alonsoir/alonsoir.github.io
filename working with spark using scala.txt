Install apache spark. 
	
	brew install apache-spark

	aironman@MacBook-Pro-de-Alonso ~> cd /usr/local/Cellar/apache-spark/2.4.4/libexec/conf/

	aironman@MacBook-Pro-de-Alonso /u/l/C/a/2/l/conf> mv log4j.properties.template log4j.properties

	change INFO error level to ERROR

	aironman@MacBook-Pro-de-Alonso /u/l/C/a/2/l/conf> spark-shell
	# how many lines has this file?
	scala> sc.textFile("testSpark.txt").count
	res0: Long = 1

Download this set of files. http://files.grouplens.org/datasets/movielens/ml-100k.zip

New scala project, new scala file...

build.sbt

	name := "spark-scala"

	version := "0.1"

	scalaVersion := "2.12.8"

	libraryDependencies += "org.apache.spark" %% "spark-core" % "2.4.5"

RatingsCounter.scala

	import org.apache.spark._
	import org.apache.spark.SparkContext._
	import org.apache.log4j._

	/** Count up how many of each star rating exists in the MovieLens 100K data set. */
	object RatingsCounter {
	 
	  /** Our main function where the action happens */
	  def main(args: Array[String]) {
	   
	    // Set the log level to only print errors
	    Logger.getLogger("org").setLevel(Level.ERROR)
	        
	    // Create a SparkContext using every core of the local machine, named RatingsCounter
	    val sc = new SparkContext("local[*]", "RatingsCounter")
	   
	    // Load up each line of the ratings data into an RDD
	    val lines = sc.textFile("../ml-100k/u.data")
	    
	    // Convert each line to a string, split it out by tabs, and extract the third field.
	    // (The file format is userID, movieID, rating, timestamp)
	    val ratings = lines.map(x => x.toString().split("\t")(2))
	    
	    // Count up how many times each value (rating) occurs
	    val results = ratings.countByValue()
	    
	    // Sort the resulting map of (rating, count) tuples
	    val sortedResults = results.toSeq.sortBy(_._1)
	    
	    // Print each result on its own line.
	    sortedResults.foreach(println)
	  }
	}




