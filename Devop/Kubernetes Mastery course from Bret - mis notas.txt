Conceptos k8s.

https:slides.kubernetesmastery.com/#63

0. Start the cluster (with minikube)

  minikube start --wait=false

Checking that everything is ok:

  kubectl cluster-info
  kubect get nodes

shpod:

For a consistent Kubernetes experience You can use shpod for examples

shpod provides a shell running in a pod on the cluster

It comes with many tools pre-installed (helm, stern, curl, jq...)

These tools are used in many exercises in these slides

shpod also gives you shell completion and a fancy prompt

Create it with:

  kubectl apply -f https:k8smastery.com/shpod.yaml
  namespace/shpod created
  serviceaccount/shpod created
  clusterrolebinding.rbac.authorization.k8s.io/shpod created
  pod/shpod created

Once you have applied the yaml above for the first time, you just have to run
this command the next time:

  kubectl attach -i -t shpod -n shpod

  this also works, less abbreviated:

  kubectl attach --namespace=shpod -ti shpod

    If you don't see a command prompt, try pressing enter.

  shpod:~# pwd
  /root
  shpod:~# uname -ra
    Linux shpod 4.19.202 #1 SMP Thu Sep 2 18:19:24 UTC 2021 x86_64 Linux

  Attach to shell with next command:

    kubectl attach --namespace=shpod -ti shpod

  type exit if you want to leave the shpod shell...

  After finishing course

    kubectl delete -f https:k8smastery.com/shpod.yaml

  Review the pod in their namespace shpod...

    kubectl get pods -n shpod
    NAME    READY   STATUS    RESTARTS   AGE
    shpod   1/1     Running   0          4h51m

  Do you need a second shpod shell?

    kubectl exec --namespace=shpod -ti shpod -- bash -l

1. How to Deploy Containers in k8s

With a running Kubernetes cluster, containers can now be deployed.

Using kubectl run, it allows containers to be deployed onto the cluster -

  kubectl create deployment first-deployment --image=katacoda/docker-http-server

The status of the deployment can be discovered via the running Pods -

  kubectl get pods

Once the container is running it can be exposed via different networking options, depending on requirements.
One possible solution is NodePort, that provides a dynamic port to a container.

  kubectl expose deployment first-deployment --port=80 --type=NodePort

The command below finds the allocated port and executes a HTTP request.

  export PORT=$(kubectl get svc first-deployment -o go-template='{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{"\n"}}{{end}}{{end}}')

  echo "Accessing host01:$PORT"

  curl host01:$PORT

The result is the container that processed the request.

2. Dashboard

Enable the dashboard using Minikube with the command minikube addons enable dashboard

Make the Kubernetes Dashboard available by deploying the following YAML definition.
This should only be used on Katacoda. Busca el equivalente de Bret para abrir el
dashboard.

kubectl apply -f /opt/kubernetes-dashboard.yaml

The Kubernetes dashboard allows you to view your applications in a UI.
In this deployment, the dashboard has been made available on port 30000 but may take a while to start.

To see the progress of the Dashboard starting, watch the Pods within the kube-system namespace using

  kubectl get pods -n kubernetes-dashboard -w

Once running, the URL to the dashboard is

  https:2886795282-30000-kitek05.environments.katacoda.com/

3. Pod:

A set of one or more containers accessible through an IP address.
That IP address is used to access the Pod, never the containers themselves, in fact,
the containers are not directly accessible.

A Pod is the smallest unit in a k8s cluster.

Containers in a pod share localhost and can share between them
volumes of data.

kubectl uses this config file:

zsh 1609 % pwd
/Users/aironman/.kube
[lun 21/09/27 16:40 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/.kube>
zsh 1610 % ll
Executing ls -lh
total 16
drwxr-x---  4 aironman  staff   128B 28 abr 12:39 cache
-rw-------  1 aironman  staff   6,2K 27 sep 10:28 config

zsh 1616 % kubectl get node -o wide
NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube   Ready    control-plane,master   18d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker:20.10.8
[lun 21/09/27 17:03 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]

It also works in the plural:

kubectl get nodes -o wide
NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube   Ready    control-plane,master   18d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker:20.10.8

Or using a diminutive:

zsh 1620 % kubectl get no -o wide
NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube   Ready    control-plane,master   18d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker:20.10.8

We can output the configuration in YAML format.

[lun 21/09/27 17:03 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/.kube>
zsh 1619 % kubectl get nodes -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2021-09-09T11:45:46Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: minikube
      kubernetes.io/os: linux
      minikube.k8s.io/commit: 5931455374810b1bbeb222a9713ae2c756daee10
      minikube.k8s.io/name: minikube
      minikube.k8s.io/updated_at: 2021_09_09T13_45_50_0700
      minikube.k8s.io/version: v1.23.0
      node-role.kubernetes.io/control-plane: ""
      node-role.kubernetes.io/master: ""
      node.kubernetes.io/exclude-from-external-load-balancers: ""
    name: minikube
    resourceVersion: "117812"
    uid: 5cdc10d6-f485-49f3-8a7d-51c3f4b40f86
  spec:
    podCIDR: 10.244.0.0/24
    podCIDRs:
    - 10.244.0.0/24
  status:
    addresses:
    - address: 192.168.64.3
      type: InternalIP
    - address: minikube
      type: Hostname
    allocatable:
      cpu: "2"
      ephemeral-storage: 17784752Ki
      hugepages-2Mi: "0"
      memory: 5952468Ki
      pods: "110"
    capacity:
      cpu: "2"
      ephemeral-storage: 17784752Ki
      hugepages-2Mi: "0"
      memory: 5952468Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2021-09-27T15:00:54Z"
      lastTransitionTime: "2021-09-17T01:39:31Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2021-09-27T15:00:54Z"
      lastTransitionTime: "2021-09-17T01:39:31Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2021-09-27T15:00:54Z"
      lastTransitionTime: "2021-09-17T01:39:31Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2021-09-27T15:00:54Z"
      lastTransitionTime: "2021-09-17T01:39:31Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - bretfisher/shpod@sha256:eb2cb1e1fc39478242e2e201057181618ea2eb0bee68aa50de5d60a4474e19f8
      - bretfisher/shpod:latest
      sizeBytes: 558201235
    - names:
      - k8s.gcr.io/etcd@sha256:9ce33ba33d8e738a5b85ed50b5080ac746deceed4a7496c550927a7a19ca3b6d
      - k8s.gcr.io/etcd:3.5.0-0
      sizeBytes: 294536887
    - names:
      - quay.io/radanalyticsio/spark-operator@sha256:7bfd8866134d864486ae18de72d79bf2699b1d08dab37a0e095bc86643863b8d
      - quay.io/radanalyticsio/spark-operator:latest-released
      sizeBytes: 272154571
    - names:
      - kubernetesui/dashboard@sha256:7f80b5ba141bead69c4fee8661464857af300d7d7ed0274cf7beecedc00322e6
      sizeBytes: 225733746
    - names:
      - kubernetesui/dashboard@sha256:ec27f462cf1946220f5a9ace416a84a57c18f98c777876a8054405d1428cc92e
      - kubernetesui/dashboard:v2.3.1
      sizeBytes: 220033604
    - names:
      - k8s.gcr.io/kube-apiserver@sha256:6862d5a70cea8f3ef49213d6a36b7bfbbf90f99fb37f7124505be55f0ef51364
      - k8s.gcr.io/kube-apiserver:v1.22.1
      sizeBytes: 128446877
    - names:
      - k8s.gcr.io/kube-controller-manager@sha256:3e4274dee8a122bdd5e3f3db6b1eb8db59404deda2bf1adb0fec1da5dd95400a
      - k8s.gcr.io/kube-controller-manager:v1.22.1
      sizeBytes: 121979567
    - names:
      - nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
      - nginx:1.14.2
      sizeBytes: 109129446
    - names:
      - k8s.gcr.io/kube-proxy@sha256:efcf1d5fb2fc95d28841f534f1385a4884230c7c876fb1b7cf66d2777ad6dc56
      - k8s.gcr.io/kube-proxy:v1.22.1
      sizeBytes: 103645121
    - names:
      - k8s.gcr.io/kube-scheduler@sha256:e1a999694bf4b9198bc220216680ef651fabe406445a93c2d354f9dd7e53c1fd
      - k8s.gcr.io/kube-scheduler:v1.22.1
      sizeBytes: 52658888
    - names:
      - k8s.gcr.io/coredns/coredns@sha256:6e5a02c21641597998b4be7cb5eb1e7b02c0d8d23cce4dd09f4682d463798890
      - k8s.gcr.io/coredns/coredns:v1.8.4
      sizeBytes: 47554275
    - names:
      - kubernetesui/metrics-scraper@sha256:555981a24f184420f3be0c79d4efb6c948a85cfce84034f85a563f4151a81cbf
      sizeBytes: 36937728
    - names:
      - kubernetesui/metrics-scraper@sha256:36d5b3f60e1a144cc5ada820910535074bdf5cf73fb70d1ff1681537eef4e172
      - kubernetesui/metrics-scraper:v1.0.7
      sizeBytes: 34446077
    - names:
      - gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944
      - gcr.io/k8s-minikube/storage-provisioner:v5
      sizeBytes: 31465472
    - names:
      - k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07
      - k8s.gcr.io/pause:3.5
      sizeBytes: 682696
    nodeInfo:
      architecture: amd64
      bootID: 4525a770-36c1-4027-8bc8-197fd21510e2
      containerRuntimeVersion: docker:20.10.8
      kernelVersion: 4.19.202
      kubeProxyVersion: v1.22.1
      kubeletVersion: v1.22.1
      machineID: cea931afcb7147aa92509f298edb734c
      operatingSystem: linux
      osImage: Buildroot 2021.02.4
      systemUUID: 5a7211ec-0000-0000-ac0a-acde48001122
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

Even in json format, ideal for parsing with the jq tool: HORRIBLE!

shpod:~# kubectl get no -o json | jq ".items[] | {name:.metadata.name} + .status.capacity"
{
  "name": "minikube",
  "cpu": "2",
  "ephemeral-storage": "17784752Ki",
  "hugepages-2Mi": "0",
  "memory": "5952468Ki",
  "pods": "110"
}

Horrible in my opinion, we have the following command. no is nodes!

shpod:~# kubectl get no
NAME       STATUS   ROLES                  AGE   VERSION
minikube   Ready    control-plane,master   18d   v1.22.1

shpod:~# kubectl describe node minikube
Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5931455374810b1bbeb222a9713ae2c756daee10
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/updated_at=2021_09_09T13_45_50_0700
                    minikube.k8s.io/version=v1.23.0
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 09 Sep 2021 11:45:46 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 27 Sep 2021 15:19:27 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 27 Sep 2021 15:15:59 +0000   Fri, 17 Sep 2021 01:39:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 27 Sep 2021 15:15:59 +0000   Fri, 17 Sep 2021 01:39:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 27 Sep 2021 15:15:59 +0000   Fri, 17 Sep 2021 01:39:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 27 Sep 2021 15:15:59 +0000   Fri, 17 Sep 2021 01:39:31 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.64.3
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             5952468Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             5952468Ki
  pods:               110
System Info:
  Machine ID:                 cea931afcb7147aa92509f298edb734c
  System UUID:                5a7211ec-0000-0000-ac0a-acde48001122
  Boot ID:                    4525a770-36c1-4027-8bc8-197fd21510e2
  Kernel Version:             4.19.202
  OS Image:                   Buildroot 2021.02.4
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker:20.10.8
  Kubelet Version:            v1.22.1
  Kube-Proxy Version:         v1.22.1
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-78fcd69978-lq24r                      100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     18d
  kube-system                 etcd-minikube                                 100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         18d
  kube-system                 kube-apiserver-minikube                       250m (12%)    0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-controller-manager-minikube              200m (10%)    0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-proxy-79r5q                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-scheduler-minikube                       100m (5%)     0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  kubernetes-dashboard        dashboard-metrics-scraper-5594458c94-qbght    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h50m
  kubernetes-dashboard        kubernetes-dashboard-654cf69797-hrkmr         0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h50m
  shpod                       shpod                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h35m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>
shpod:~#

Listing types of resources available in a cluster:

shpod:~# kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v1                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                               true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
endpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta1   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta1   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
csistoragecapacities                           storage.k8s.io/v1beta1                 true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
shpod:~#

My goodness, a lot of resources, to describe what they do, we can use this command:

shpod:~# kubectl explain deployment
KIND:     Deployment
VERSION:  apps/v1

DESCRIPTION:
     Deployment enables declarative updates for Pods and ReplicaSets.

FIELDS:
   apiVersion	<string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind	<string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata	<Object>
     Standard object's metadata. More info:
     https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec	<Object>
     Specification of the desired behavior of the Deployment.

   status	<Object>
     Most recently observed status of the Deployment.

shpod:~# kubectl explain pod
     KIND:     Pod
     VERSION:  v1

     DESCRIPTION:
          Pod is a collection of containers that can run on a host. This resource is
          created by clients and scheduled onto hosts.

     FIELDS:
        apiVersion	<string>
          APIVersion defines the versioned schema of this representation of an
          object. Servers should convert recognized schemas to the latest internal
          value, and may reject unrecognized values. More info:
          https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

        kind	<string>
          Kind is a string value representing the REST resource this object
          represents. Servers may infer this from the endpoint the client submits
          requests to. Cannot be updated. In CamelCase. More info:
          https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

        metadata	<Object>
          Standard object's metadata. More info:
          https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

        spec	<Object>
          Specification of the desired behavior of the pod. More info:
          https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

        status	<Object>
          Most recently observed status of the pod. This data may not be up to date.
          Populated by the system. Read-only. More info:
          https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

shpod:~# kubectl explain  services
          KIND:     Service
          VERSION:  v1

          DESCRIPTION:
               Service is a named abstraction of software service (for example, mysql)
               consisting of local port (for example 3306) that the proxy listens on, and
               the selector that determines which pods will answer requests sent through
               the proxy.

          FIELDS:
             apiVersion	<string>
               APIVersion defines the versioned schema of this representation of an
               object. Servers should convert recognized schemas to the latest internal
               value, and may reject unrecognized values. More info:
               https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

             kind	<string>
               Kind is a string value representing the REST resource this object
               represents. Servers may infer this from the endpoint the client submits
               requests to. Cannot be updated. In CamelCase. More info:
               https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

             metadata	<Object>
               Standard object's metadata. More info:
               https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

             spec	<Object>
               Spec defines the behavior of a service.
               https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

             status	<Object>
               Most recently observed status of the service. Populated by the system.
               Read-only. More info:
               https:git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

More comprehensive!

shpod:~# kubectl explain  services --recursive
               KIND:     Service
               VERSION:  v1

               DESCRIPTION:
                    Service is a named abstraction of software service (for example, mysql)
                    consisting of local port (for example 3306) that the proxy listens on, and
                    the selector that determines which pods will answer requests sent through
                    the proxy.

               FIELDS:
                  apiVersion	<string>
                  kind	<string>
                  metadata	<Object>
                     annotations	<map[string]string>
                     clusterName	<string>
                     creationTimestamp	<string>
                     deletionGracePeriodSeconds	<integer>
                     deletionTimestamp	<string>
                     finalizers	<[]string>
                     generateName	<string>
                     generation	<integer>
                     labels	<map[string]string>
                     managedFields	<[]Object>
                        apiVersion	<string>
                        fieldsType	<string>
                        fieldsV1	<map[string]>
                        manager	<string>
                        operation	<string>
                        subresource	<string>
                        time	<string>
                     name	<string>
                     namespace	<string>
                     ownerReferences	<[]Object>
                        apiVersion	<string>
                        blockOwnerDeletion	<boolean>
                        controller	<boolean>
                        kind	<string>
                        name	<string>
                        uid	<string>
                     resourceVersion	<string>
                     selfLink	<string>
                     uid	<string>
                  spec	<Object>
                     allocateLoadBalancerNodePorts	<boolean>
                     clusterIP	<string>
                     clusterIPs	<[]string>
                     externalIPs	<[]string>
                     externalName	<string>
                     externalTrafficPolicy	<string>
                     healthCheckNodePort	<integer>
                     internalTrafficPolicy	<string>
                     ipFamilies	<[]string>
                     ipFamilyPolicy	<string>
                     loadBalancerClass	<string>
                     loadBalancerIP	<string>
                     loadBalancerSourceRanges	<[]string>
                     ports	<[]Object>
                        appProtocol	<string>
                        name	<string>
                        nodePort	<integer>
                        port	<integer>
                        protocol	<string>
                        targetPort	<string>
                     publishNotReadyAddresses	<boolean>
                     selector	<map[string]string>
                     sessionAffinity	<string>
                     sessionAffinityConfig	<Object>
                        clientIP	<Object>
                           timeoutSeconds	<integer>
                     type	<string>
                  status	<Object>
                     conditions	<[]Object>
                        lastTransitionTime	<string>
                        message	<string>
                        observedGeneration	<integer>
                        reason	<string>
                        status	<string>
                        type	<string>
                     loadBalancer	<Object>
                        ingress	<[]Object>
                           hostname	<string>
                           ip	<string>
                           ports	<[]Object>
                              error	<string>
                              port	<integer>
                              protocol	<string>

A Service is a stable endpoint for accessing a container in a pod.
You can access it through its ip. Three ways to see what you have:

shpod:~# kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18d
shpod:~# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18d
shpod:~# kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18d

A Pod is basically a cluster, a set of containers, running together on the same node.
They share resources, such as RAM, CPU, data volumes, network...

shpod:~# kubectl get pods
No resources found in default namespace.

As you can see, I don't have pods in the default namespace, so let's look at other namespaces, like shpod:

shpod:~# kubectl get pods -n shpod
NAME    READY   STATUS    RESTARTS   AGE
shpod   1/1     Running   0          23h

shpod:~# kubectl get namespaces
NAME                   STATUS   AGE
default                Active   18d
kube-node-lease        Active   18d
kube-public            Active   18d
kube-system            Active   18d
kubernetes-dashboard   Active   13d
shpod                  Active   23h

shpod:~# kubectl get pods -n kubernetes-dashboard
NAME                                         READY   STATUS    RESTARTS   AGE
dashboard-metrics-scraper-5594458c94-qbght   1/1     Running   0          24h
kubernetes-dashboard-654cf69797-hrkmr        1/1     Running   0          24h

shpod:~# kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS      AGE
coredns-78fcd69978-lq24r           1/1     Running   2 (24h ago)   18d
etcd-minikube                      1/1     Running   2 (24h ago)   18d
kube-apiserver-minikube            1/1     Running   2 (24h ago)   18d
kube-controller-manager-minikube   1/1     Running   2 (24h ago)   18d
kube-proxy-79r5q                   1/1     Running   2 (24h ago)   18d
kube-scheduler-minikube            1/1     Running   2 (24h ago)   18d
storage-provisioner                1/1     Running   4 (24h ago)   18d

As we can see, k8s has several default namespaces where some pods run.

Let's list all the pods in all the namespaces:

shpod:~# kubectl get pods --all-namespaces
NAMESPACE              NAME                                         READY   STATUS    RESTARTS      AGE
kube-system            coredns-78fcd69978-lq24r                     1/1     Running   2 (25h ago)   18d
kube-system            etcd-minikube                                1/1     Running   2 (25h ago)   18d
kube-system            kube-apiserver-minikube                      1/1     Running   2 (25h ago)   18d
kube-system            kube-controller-manager-minikube             1/1     Running   2 (25h ago)   18d
kube-system            kube-proxy-79r5q                             1/1     Running   2 (25h ago)   18d
kube-system            kube-scheduler-minikube                      1/1     Running   2 (25h ago)   18d
kube-system            storage-provisioner                          1/1     Running   4 (25h ago)   18d
kubernetes-dashboard   dashboard-metrics-scraper-5594458c94-qbght   1/1     Running   0             25h
kubernetes-dashboard   kubernetes-dashboard-654cf69797-hrkmr        1/1     Running   0             25h
shpod                  shpod                                        1/1     Running   0             23h

As you can see, -A is equivalent to --all-namespaces:

shpod:~# kubectl get pods -A
NAMESPACE              NAME                                         READY   STATUS    RESTARTS      AGE
kube-system            coredns-78fcd69978-lq24r                     1/1     Running   2 (25h ago)   18d
kube-system            etcd-minikube                                1/1     Running   2 (25h ago)   18d
kube-system            kube-apiserver-minikube                      1/1     Running   2 (25h ago)   18d
kube-system            kube-controller-manager-minikube             1/1     Running   2 (25h ago)   18d
kube-system            kube-proxy-79r5q                             1/1     Running   2 (25h ago)   18d
kube-system            kube-scheduler-minikube                      1/1     Running   2 (25h ago)   18d
kube-system            storage-provisioner                          1/1     Running   4 (25h ago)   18d
kubernetes-dashboard   dashboard-metrics-scraper-5594458c94-qbght   1/1     Running   0             25h
kubernetes-dashboard   kubernetes-dashboard-654cf69797-hrkmr        1/1     Running   0             25h
shpod                  shpod                                        1/1     Running   0             23h


shpod:~# kubectl get pods -n kube-public
No resources found in kube-public namespace.

Interestingly, if we list the pods in that kube-public namespace, the api tells you that there is nothing,
but we know there is only one called configmaps.
In fact, I run this command, the api says there is nothing:

shpod:~# kubectl get all -n kube-public
No resources found in kube-public namespace.

But if there is such a configmaps object, which, in turn, contains a cluster-info object

shpod:~# kubectl get configmaps -n kube-public
NAME               DATA   AGE
cluster-info       1      18d
kube-root-ca.crt   1      18d

I can even examine cluster-info information:

shpod:~# kubectl get configmaps -n kube-public cluster-info -o yaml
apiVersion: v1
data:
  kubeconfig: |
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: LOTS OF NUMBERS AND LETTERS THAT I DELETE FOR SECURITY REASONS
        server: https:control-plane.minikube.internal:8443
      name: ""
    contexts: null
    current-context: ""
    kind: Config
    preferences: {}
    users: null
kind: ConfigMap
metadata:
  creationTimestamp: "2021-09-09T11:45:49Z"
  name: cluster-info
  namespace: kube-public
  resourceVersion: "1165"
  uid: 550649f2-1aaf-4313-9e50-e08c7d044b6d

I can even examine the kube-root-ca.crt information.:

shpod:~# kubectl get configmaps -n kube-public kube-root-ca.crt -o json
  {
      "apiVersion": "v1",
      "data": {
          "ca.crt": "-----BEGIN CERTIFICATE-----\LOTS OF NUMBERS AND LETTERS THAT I DELETE FOR SECURITY REASONS\n-----END CERTIFICATE-----\n"
      },
      "kind": "ConfigMap",
      "metadata": {
          "annotations": {
              "kubernetes.io/description": "Contains a CA bundle that can be used to verify the kube-apiserver when using internal endpoints such as the internal service IP or kubernetes.default.svc. No other usage is guaranteed across distributions of Kubernetes clusters."
          },
          "creationTimestamp": "2021-09-09T11:46:01Z",
          "name": "kube-root-ca.crt",
          "namespace": "kube-public",
          "resourceVersion": "402",
          "uid": "79f3b7ca-9d4c-4b6c-a080-6a6af987d664"
      }
  }

  When you create a Deployment object, you do not create a Pod object.
  ReplicaSet object, which does create a Pod object.

  For example, the command:

kubectl create deployment pingpong --image alpine -- ping 1.1.1.1

will create all these objects:

shpod:~# kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/pingpong-98f6d5899-d9pq2   1/1     Running   0          8m59s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   19d

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   1/1     1            1           8m59s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/pingpong-98f6d5899   1         1         1       8m59s

As you can see, you have created a Deployment object, which in turn creates a replicaset object,
which in turn creates a pod, which in turn creates a Docker container with an alpine image that's
executing the ping 1.1.1.1.1 command over and over again.

That's the necessary hierarchy:

  Deployment
    ReplicaSet
      Pod
        Container

        We are making a lot of requests to the server with the ip 1.1.1.1.1.

Let's check what's going on...

shpod:~# kubectl logs deploy/pingpong --tail 10 --follow
PING 1.1.1.1 (1.1.1.1): 56 data bytes

Note that if you want to create all that hierarchy to finally execute a bash command in a linux image,
you have to put the -- at the end of the command.
That is, this will work:

  kubectl create deployment pingpong --image alpine -- ping 1.1.1.1

Not this:

  kubectl create deployment pingpong --image alpine ping 1.1.1.1

or even this!

  kubectl create deployment pingpong --image alpine

  One would think that with the last command you would create the whole hierarchy of objects to finally have an alpine container,
  but you don't. If you don't specify the command you are going to execute, k8s will not properly create the object hierarchy.

  Now, let's try to scale the containers in the pod. You might think at first that you have to specify something to the Replicaset,
  but no, you have to tell the Deployment object to do that task, at least in my version of k8s.

  Translated with www.DeepL.com/Translator (free version)

shpod:~# kubectl scale deployment pingpong --replicas 3
deployment.apps/pingpong scaled
shpod:~# kubectl get deployments
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
pingpong   3/3     3            3           24m
shpod:~# kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/pingpong-98f6d5899-bgrq7   1/1     Running   0          14s
pod/pingpong-98f6d5899-bzvmc   1/1     Running   0          14s
pod/pingpong-98f6d5899-d9pq2   1/1     Running   0          24m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   19d

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   3/3     3            3           24m

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/pingpong-98f6d5899   3         3         3       24m

shpod:~# kubectl logs deploy/pingpong
Found 3 pods, using pod/pingpong-98f6d5899-d9pq2
PING 1.1.1.1 (1.1.1.1): 56 data bytes

If we want a bit more detail by looking at the pod logs:

shpod:~# kubectl logs pingpong-98f6d5899-
pingpong-98f6d5899-bgrq7  pingpong-98f6d5899-bzvmc  pingpong-98f6d5899-d9pq2
shpod:~# kubectl logs pingpong-98f6d5899-bgrq7
PING 1.1.1.1 (1.1.1.1): 56 data bytes
shpod:~# kubectl logs pingpong-98f6d5899-bzvmc
PING 1.1.1.1 (1.1.1.1): 56 data bytes
shpod:~# kubectl logs pingpong-98f6d5899-d9pq2
PING 1.1.1.1 (1.1.1.1): 56 data bytes

Remember that we have a utility called watch that allows you to see the output of any linux/unix command.
In  Bret's shpod container it comes by default, among many other utilities, as helm.
Let's try it while trying to delete a container using the kubectl delete command:

On the one hand, we run:

shpod:~# watch kubectl get pods

It will look something like this:

Every 2.0s: kubectl get pods                                                                                                                       2021-09-28 17:01:44

NAME                       READY   STATUS    RESTARTS   AGE
pingpong-98f6d5899-7fnnc   1/1     Running   0          6m43s
pingpong-98f6d5899-bgrq7   1/1     Running   0          19m
pingpong-98f6d5899-bzvmc   1/1     Running   0          19m

On the other hand, in another terminal, we run

zsh 1646 % kubectl delete pod pingpong-98f6d5899-7fnnc
pod "pingpong-98f6d5899-7fnnc" deleted

In the first tab we can see that the pod is put in Terminating state and immediately k8s tries to create another container until we finally have the number of replicas indicated in the ReplicaSet object.
until we finally have the number of replicas indicated in the ReplicaSet object.

Note that due to the hierarchy of objects controlled by the Deployment object, you could never delete the containers,
in the sense that you want to delete and stop the execution of the container.
If you want to do something like that, you have to scale to zero containers, using the command:

zsh 1649 % kubectl scale deployment pingpong --replicas 0
deployment.apps/pingpong scaled

on the screen where you are running the watch kubectl get pods command you will see something like this:

Every 2.0s: kubectl get pods                                                                                                                       2021-09-28 17:08:35

No resources found in default namespace.

zsh 1650 % kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   19d

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   0/0     0            0           51m

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/pingpong-98f6d5899   0         0         0       51m

shpod:~# exit
logout
Session ended, resume using 'kubectl attach shpod -c shpod -i -t' command when the pod is running

Cron tasks.

zsh 1715 % kubectl apply -f https:raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/cronjob.yaml
cronjob.batch/hello created

The yaml file looks like this:

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure

In another tab, running watch kubectl get cronjobs:

Every 2.0s: kubectl get cronjobs                                                                                                                   2021-09-29 14:35:55

NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   False     0        <none>          46s

We can see how a new pod is created by running the sh command described in cronjob.yaml,
every minute a new one...

To delete the cronjob:

zsh 1718 % kubectl delete cronjobs.batch hello
cronjob.batch "hello" deleted

Note that a cronjob will create a Job which in turn will create a pod, where the container will be hosted.

We can check the logs of all pods at once.
The key is to use -l app=pingpong

zsh 1730 % kubectl logs -l app=pingpong --tail 1 -f
PING 1.1.1.1 (1.1.1.1): 56 data bytes
PING 1.1.1.1 (1.1.1.1): 56 data bytes
PING 1.1.1.1 (1.1.1.1): 56 data bytes

zsh 1731 [130] % kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/pingpong-98f6d5899-9v85n   1/1     Running   0          5h17m
pod/pingpong-98f6d5899-s2pd4   1/1     Running   0          5h17m
pod/pingpong-98f6d5899-z784p   1/1     Running   0          5h17m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   20d

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   3/3     3            3           22h

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/pingpong-98f6d5899   3         3         3       22h

Note that there is a limit, maximum 5!

I will scale the pods up to 8,

Every 2.0s: kubectl get pods                                                                                                                       2021-09-29 15:09:23

NAME                       READY   STATUS    RESTARTS   AGE
pingpong-98f6d5899-2584f   1/1     Running   0          80s
pingpong-98f6d5899-9v85n   1/1     Running   0          5h22m
pingpong-98f6d5899-d2m8r   1/1     Running   0          80s
pingpong-98f6d5899-mcpnm   1/1     Running   0          80s
pingpong-98f6d5899-n7c6m   1/1     Running   0          80s
pingpong-98f6d5899-r5ftn   1/1     Running   0          80s
pingpong-98f6d5899-s2pd4   1/1     Running   0          5h22m
pingpong-98f6d5899-z784p   1/1     Running   0          5h22m

zsh 1734 % kubectl logs -l app=pingpong --tail 1 -f
error: you are attempting to follow 8 log streams, but maximum allowed concurrency is 5, use --max-log-requests to increase the limit

As we can see, kubectl logs has limitations when it comes to accessing logs, but it is for a good reason, to protect the resources of the
api resources. For this, we can use stern.

shpod:~# stern --tail 1 --timestamps --all-namespaces pingpong
+ default pingpong-98f6d5899-z784p › alpine
+ default pingpong-98f6d5899-d2m8r › alpine
+ default pingpong-98f6d5899-n7c6m › alpine
+ default pingpong-98f6d5899-s2pd4 › alpine
+ default pingpong-98f6d5899-mcpnm › alpine
default pingpong-98f6d5899-z784p alpine 2021-09-29T09:47:23.831054441Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
+ default pingpong-98f6d5899-r5ftn › alpine
default pingpong-98f6d5899-d2m8r alpine 2021-09-29T15:08:12.905561352Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
+ default pingpong-98f6d5899-9v85n › alpine
+ default pingpong-98f6d5899-2584f › alpine
default pingpong-98f6d5899-n7c6m alpine 2021-09-29T15:08:08.819615419Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-s2pd4 alpine 2021-09-29T09:47:26.665132690Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-r5ftn alpine 2021-09-29T15:08:11.594289256Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-mcpnm alpine 2021-09-29T15:08:07.562812985Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-9v85n alpine 2021-09-29T09:47:25.291573172Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-2584f alpine 2021-09-29T15:08:10.149623566Z PING 1.1.1.1 (1.1.1.1): 56 data bytes

Some questions...

How many nodes does your cluster have?
(the answer should be the kubectl command you used to get the answer)

  Probably it is not necessary to use -o wide.

  shpod:~# kubectl get nodes -o wide
  NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
  minikube   Ready    control-plane,master   20d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker:20.10.8

What kernel version and what container engine is each node running?
(the answer should be the kubectl command you used to get the answer)

  shpod:~# kubectl get nodes -o wide
  NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
  minikube   Ready    control-plane,master   20d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker:20.10.8

List only the pods in the kube-system namespace.
(the answer should be the kubectl command you used to get the answer)

  shpod:~# kubectl get pods -n kube-system
  NAME                               READY   STATUS    RESTARTS        AGE
  coredns-78fcd69978-lq24r           1/1     Running   2 (2d23h ago)   20d
  etcd-minikube                      1/1     Running   2 (2d23h ago)   20d
  kube-apiserver-minikube            1/1     Running   2 (2d23h ago)   20d
  kube-controller-manager-minikube   1/1     Running   2 (2d23h ago)   20d
  kube-proxy-79r5q                   1/1     Running   2 (2d23h ago)   20d
  kube-scheduler-minikube            1/1     Running   2 (2d23h ago)   20d
  storage-provisioner                1/1     Running   4 (2d23h ago)   20d

Explain the role of some of these pods.

  coredns-78fcd69978-lq24r is the dns server of k8s. basically it translates full qualified domain names to ip addresses.

  etcd-minikube is basically a distributed high available key/value system where we store important k8s data.
  The nodes, the pods, every object and relevant data.

  kube-apiserver-minikube is basically a control plane component, a rest server which exposes the k8s api-server,
  which interacts with etcd component. It is designed for high availability, as everything i k8s.

  kube-controller-manager-minikube is basically another control plane component which controls every controller in k8s. It is compiled as a
  only one component, for eficiency motives.

  It controls the nodes, like when a new node comes into service or it goes down, the replicasets,
  controlling when a pod is added or deleted from the replicaset object itself,
  the endpoints between services and pods, i mean, when k8s adds an ip address to that pod,
  it controls too the tokens and account services, so that the api server can access to new namespaces...


If there are few or no pods in kube-system, why could that be?

  By default, i understand that above described are the minimum components.
  One less should indicate a system malfunction, or,

  (reychel-nicole-zuniga-rojas)
  On some clusters, the control plane is located outside the cluster itself, and not running as containers.

  In that case, the control plane won't show up in kube-system, but you can find on host with

  ps aux | grep kube.

Create a deployment using kubectl create that runs the image bretfisher/clock and name it ticktock.
  (the answer should be the kubectl command you used)

  kubectl create deployment ticktock --images=bretfisher/clock --

Increase the number of pods running in that deployment to three.
(the answer should be the kubectl command you used)

  zsh 1742 % kubectl scale deployment ticktock --replicas=3
  deployment.apps/ticktock scaled

Use a selector to output only the last line of logs of each container.
(the answer should be the kubectl command you used)

  zsh 1754 [1] % kubectl logs deployment/ticktock --tail=1 -f
  Found 3 pods, using pod/ticktock-5cc6bf699-ks67z
  Thu Sep 30 08:12:45 UTC 2021
  Thu Sep 30 08:12:46 UTC 2021
  Thu Sep 30 08:12:47 UTC 2021
  ...

  shpod:~# stern ticktock --tail 1 -t -A

   Probably this is what Bret asked...
  kubectl logs --selector=app=ticktock --tail=1
  kubectl logs -l app=ticktock --tail 1

  Exposing those containers running in pods to the outside world.

    To do this we use services. Basically it's assigning an ip address to the pod.
    Note that they are assigned to the pod, not to the container, as is the case with Docker.
    Once the service is created, the CoreDNS component will be in charge of
    translate the name of the service to the ip assigned to the pod.

    We have four types of services:

    clusterIP or internal nodes inaccessible from outside: The ip assigned to the service can only be accessed from inside the cluster (nodes and ports).
              inside the cluster (nodes and ports).
              The port initially assigned by k8s can be accessed.
              It is the default service when using the expose command.
              They can be ideal for forming the nodes of a kafka or spark cluster, as these technologies have nodes that compose them.
              these technologies have nodes that make them up.
              Ideally, you would only have to expose to the outside (NodePort?) the spark Driver
              Driver, the dashboard, the kafka master

    NodePort or externally accessible nodes: The assigned ip can be accessible from inside and outside.
              You are assigned a higher port range (30000-32768).
              The code must be changed to access that port. -> Bret, what the fuck do you mean by this?

    LoadBalancer: As the name suggests, it is used to access a NodePort in a balanced way.
                  It can only be used through a third party tool, aws, gcp, Azure,...

    ExternalName: to be defined.

    Once we are clear that we need to create a service to be able to expose a container running on a
    a container running in a pod to the world, we have to use the kubectl expose command.
    This command creates a service.

    Let's try to expose a clusterIp. We create a deployment object and scale it to 10 instances.
    httpenv is a small web server using port 8888.

  zsh 1774 % kubectl create deployment httpenv --image bretfisher/httpenv
  deployment.apps/httpenv created

  zsh 1776 % kubectl logs -l app=httpenv --follow
  Starting httpenv listening on port 8888.

  zsh 1778 [130] % kubectl scale deployment httpenv --replicas 10
  deployment.apps/httpenv scaled

  If we do a watch, we can see the logs, we can also use stern to see the logs.

  shpod:~# kubectl get pods -w
  NAME                       READY   STATUS    RESTARTS   AGE
  httpenv-6fdc8554fb-79dlf   1/1     Running   0          7m6s
  httpenv-6fdc8554fb-7dg5m   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-cw22l   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-f7h7j   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-kpj8b   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-ngrjg   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-np77c   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-nwk6r   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-tkdrv   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-wgtwx   1/1     Running   0          5m13s

  <aironman@MacBook-Pro-de-Alonso:~>
  zsh 1782 [1] % stern httpenv --tail 1 -t -A
  + default httpenv-6fdc8554fb-cw22l › httpenv
  + default httpenv-6fdc8554fb-ngrjg › httpenv
  + default httpenv-6fdc8554fb-kpj8b › httpenv
  + default httpenv-6fdc8554fb-wgtwx › httpenv
  + default httpenv-6fdc8554fb-nwk6r › httpenv
  + default httpenv-6fdc8554fb-np77c › httpenv
  + default httpenv-6fdc8554fb-7dg5m › httpenv
  + default httpenv-6fdc8554fb-79dlf › httpenv
  + default httpenv-6fdc8554fb-tkdrv › httpenv
  + default httpenv-6fdc8554fb-f7h7j › httpenv
  default httpenv-6fdc8554fb-kpj8b httpenv 2021-10-01T11:39:33.853969232+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-ngrjg httpenv 2021-10-01T11:39:28.381945077+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-79dlf httpenv 2021-10-01T11:37:29.710767791+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-nwk6r httpenv 2021-10-01T11:39:24.749041061+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-7dg5m httpenv 2021-10-01T11:39:31.261413710+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-np77c httpenv 2021-10-01T11:39:25.722419797+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-cw22l httpenv 2021-10-01T11:39:35.150719857+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-f7h7j httpenv 2021-10-01T11:39:27.088736244+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-tkdrv httpenv 2021-10-01T11:39:32.492885962+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-wgtwx httpenv 2021-10-01T11:39:29.749934080+02:00 Starting httpenv listening on port 8888.


  zsh 1783 % kubectl expose deployment httpenv --port 8888
  service/httpenv exposed

  We see how to do an expose, it creates a ClusterIp service by default:

  zsh 1784 % kubectl get service
  NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
  httpenv      ClusterIP   10.98.94.183   <none>        8888/TCP   51s
  kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP    21d

  Let's test the clusterIp service.

  Remember that this will only work if you are launching the commands inside the cluster.
  Also remember that we are going to make requests via curl to the httpenv web server that is
  running in 10 containers, so, by default the service will use through the kube-proxy
  a round robin algorithm to make requests, i.e., when curling, it will make a request, and when curling again it will ask the next one.
  curl again it will ask the next one, and so on until we get to request 11 which will ask the first container again.
  to the first container. Round-Robin is not the only algorithm that kube-proxy will use to access containers.

  Let's find out the ip, well, we parse it to have it in a variable.
  If you look at it, it's easier to find it out by doing a kubectl get svc

  shpod:~# IP=$(kubectl get svc httpenv -o go-template --template '{{ .spec.clusterIP }}')
  shpod:~# echo $IP
  10.98.94.183
  shpod:~# curl http:10.98.94.183:8888/
  {"HOME":"/root","HOSTNAME":"httpenv-6fdc8554fb-7dg5m","KUBERNETES_PORT":"tcp:10.96.0.1:443","KUBERNETES_PORT_443_TCP":"tcp:10.96.0.1:443","KUBERNETES_PORT_443_TCP_ADDR":"10.96.0.1","KUBERNETES_PORT_443_TCP_PORT":"443","KUBERNETES_PORT_443_TCP_PROTO":"tcp","KUBERNETES_SERVICE_HOST":"10.96.0.1","KUBERNETES_SERVICE_PORT":"443","KUBERNETES_SERVICE_PORT_HTTPS":"443","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"}shpod:~#

  We see that the curl request pulls a lot of info. We can use jq to keep something that interests us.

  shpod:~# curl http:10.98.94.183:8888/ | jq .HOSTNAME
    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                   Dload  Upload   Total   Spent    Left  Speed
  100   438  100   438    0     0   108k      0 --:--:-- --:--:-- --:--:--  142k
  "httpenv-6fdc8554fb-cw22l"
  shpod:~# curl http:10.98.94.183:8888/ | jq .KUBERNETES_PORT
    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                   Dload  Upload   Total   Spent    Left  Speed
  100   438  100   438    0     0   176k      0 --:--:-- --:--:-- --:--:--  427k
  "tcp:10.96.0.1:443"
  shpod:~# curl http:10.98.94.183:8888/ | jq .KUBERNETES_PORT_TCP
    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                   Dload  Upload   Total   Spent    Left  Speed
  100   438  100   438    0     0   179k      0 --:--:-- --:--:-- --:--:--  427k
  null

  Basically, using round robin in the proxy implies that we are using a LoadBalancer type service,
  but there are times when you don't want to use a load balancer, you want to access the container directly.
  to the container. You want to use a headless service.
  To do that, we need to use something like this:

  kubectl expose deployment httpenv-none --port 8888 --cluster-ip=None

  as a result, the service will not be assigned a virtual ip address, so kube-proxy will not apply a
  round-robin to access the container, so CoreDNS will return the different ip addresses of the containers as records.
  as records. Something like this is useful to find out the replicas that the container has.

  If you try to run the above command on an existing deployment object, you will get an error:

  shpod:~# kubectl expose deployment httpenv --port 8888 --cluster-ip=None
  Error from server (AlreadyExists): services "httpenv" already exists

  Let's create what is needed:

  shpod:~# kubectl create deployment httpenv-none --image bretfisher/httpenv
  deployment.apps/httpenv-none created

  shpod:~# kubectl scale deployment httpenv-none --replicas 5
  deployment.apps/httpenv-none scaled

  shpod:~# kubectl expose deployment httpenv-none --port 8888 --cluster-ip=None
  service/httpenv-none exposed

  shpod:~# kubectl get deployments
  NAME           READY   UP-TO-DATE   AVAILABLE   AGE
  httpenv        10/10   10           10          98m
  httpenv-none   5/5     5            5           58s
  ticktock       3/3     3            3           27h

  We can see that the service does not have an ip address assigned to it,
  kube-proxy will not access it.
  So how the fuck can we access it?

  https:kubernetes.io/docs/concepts/services-networking/service/#headless-services

  shpod:~# kubectl get services
  NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
  httpenv        ClusterIP   10.98.94.183   <none>        8888/TCP   84m
  httpenv-none   ClusterIP   None           <none>        8888/TCP   31s
  kubernetes     ClusterIP   10.96.0.1      <none>        443/TCP    21d

  A service has a set number of k8s managed endpoints.
  An endpoint is the combination of host and port, ip address and port.
  k8s will continuously update these endpoints.

  shpod:~# kubectl describe service httpenv
Name:              httpenv
Namespace:         default
Labels:            app=httpenv
Annotations:       <none>
Selector:          app=httpenv
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.98.94.183
IPs:               10.98.94.183
Port:              <unset>  8888/TCP
TargetPort:        8888/TCP
Endpoints:         172.17.0.10:8888,172.17.0.11:8888,172.17.0.12:8888 + 7 more...
Session Affinity:  None
Events:            <none>

And we see the other deployment, if you want to...

shpod:~# kubectl describe service httpenv-none
Name:              httpenv-none
Namespace:         default
Labels:            app=httpenv-none
Annotations:       <none>
Selector:          app=httpenv-none
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                None
IPs:               None
Port:              <unset>  8888/TCP
TargetPort:        8888/TCP
Endpoints:         172.17.0.19:8888,172.17.0.20:8888,172.17.0.21:8888 + 2 more...
Session Affinity:  None
Events:            <none>

immediately you look at the Endpoints section, it doesn't show you all of them...
To do this, we launch:

shpod:~# kubectl get endpoints
NAME           ENDPOINTS                                                        AGE
httpenv        172.17.0.10:8888,172.17.0.11:8888,172.17.0.12:8888 + 7 more...   105m
httpenv-none   172.17.0.19:8888,172.17.0.20:8888,172.17.0.21:8888 + 2 more...   21m
kubernetes     192.168.64.3:8443                                                21d

shpod:~# kubectl get endpoints -o wide
NAME           ENDPOINTS                                                        AGE
httpenv        172.17.0.10:8888,172.17.0.11:8888,172.17.0.12:8888 + 7 more...   105m
httpenv-none   172.17.0.19:8888,172.17.0.20:8888,172.17.0.21:8888 + 2 more...   21m
kubernetes     192.168.64.3:8443                                                21d

Not all of them are visible, so it is better to use:

shpod:~# kubectl get endpoints -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2021-10-01T09:51:49Z"
    creationTimestamp: "2021-10-01T09:51:49Z"
    labels:
      app: httpenv
    name: httpenv
    namespace: default
    resourceVersion: "193961"
    uid: fcacc51d-05ef-4a23-9e67-b933bea173cc
  subsets:
  - addresses:
    - ip: 172.17.0.10
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-nwk6r
        namespace: default
        resourceVersion: "193389"
        uid: 9a3caeae-daaa-4231-b1f0-fe9b1bb9addb
    - ip: 172.17.0.11
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-np77c
        namespace: default
        resourceVersion: "193395"
        uid: 3ed504e0-47aa-4349-a78b-52b395f588db
    - ip: 172.17.0.12
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-ngrjg
        namespace: default
        resourceVersion: "193409"
        uid: 03991f72-2d53-4ce9-b67a-22fca2ad22e8
    - ip: 172.17.0.13
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-kpj8b
        namespace: default
        resourceVersion: "193437"
        uid: 15b4d296-fa1c-4330-9367-996a15a42d1f
    - ip: 172.17.0.14
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-wgtwx
        namespace: default
        resourceVersion: "193417"
        uid: ef608ff1-55b4-46b3-968e-aca15a56b39a
    - ip: 172.17.0.15
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-cw22l
        namespace: default
        resourceVersion: "193444"
        uid: 7e3440a9-a0d3-452b-9612-5af58e592584
    - ip: 172.17.0.16
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-tkdrv
        namespace: default
        resourceVersion: "193431"
        uid: 599de533-7de2-47b0-b1f5-860438ad00e8
    - ip: 172.17.0.17
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-f7h7j
        namespace: default
        resourceVersion: "193402"
        uid: ac01c364-6655-4764-a03b-cf98b13453f6
    - ip: 172.17.0.18
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-7dg5m
        namespace: default
        resourceVersion: "193422"
        uid: 8da0b27c-be2b-4015-86d2-ae2c67b212f8
    - ip: 172.17.0.9
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-79dlf
        namespace: default
        resourceVersion: "193239"
        uid: 2c20d45d-8175-4975-87f9-679fbff467f4
    ports:
    - port: 8888
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2021-10-01T11:15:50Z"
    creationTimestamp: "2021-10-01T11:15:50Z"
    labels:
      app: httpenv-none
      service.kubernetes.io/headless: ""
    name: httpenv-none
    namespace: default
    resourceVersion: "197571"
    uid: 81064e1f-879b-4434-bfe1-2cfcb41f61aa
  subsets:
  - addresses:
    - ip: 172.17.0.19
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-fc8zh
        namespace: default
        resourceVersion: "197482"
        uid: c623fb82-3027-4cef-a588-7463ae3f767a
    - ip: 172.17.0.20
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-5dvzd
        namespace: default
        resourceVersion: "197535"
        uid: a9a3f0ad-abcd-4d16-a821-a3fe4ba36292
    - ip: 172.17.0.21
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-95t8j
        namespace: default
        resourceVersion: "197549"
        uid: f3a34a7e-b165-495d-85d1-9f9c7f57fc5d
    - ip: 172.17.0.22
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-ncwx8
        namespace: default
        resourceVersion: "197541"
        uid: fe5f0594-2fbc-45b4-b06a-d2f15a291ee6
    - ip: 172.17.0.23
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-n4pfg
        namespace: default
        resourceVersion: "197555"
        uid: 62e2571d-93ca-49f6-a7d8-0a7b412c2e33
    ports:
    - port: 8888
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: "2021-09-09T11:45:48Z"
    labels:
      endpointslice.kubernetes.io/skip-mirror: "true"
    name: kubernetes
    namespace: default
    resourceVersion: "210"
    uid: 6ce695ed-ea95-45a0-8645-045cd27ccee7
  subsets:
  - addresses:
    - ip: 192.168.64.3
    ports:
    - name: https
      port: 8443
      protocol: TCP
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

The pods must match:

shpod:~# kubectl get pods -l app=httpenv -o wide
NAME                       READY   STATUS    RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
httpenv-6fdc8554fb-79dlf   1/1     Running   0          120m   172.17.0.9    minikube   <none>           <none>
httpenv-6fdc8554fb-7dg5m   1/1     Running   0          118m   172.17.0.18   minikube   <none>           <none>
httpenv-6fdc8554fb-cw22l   1/1     Running   0          118m   172.17.0.15   minikube   <none>           <none>
httpenv-6fdc8554fb-f7h7j   1/1     Running   0          118m   172.17.0.17   minikube   <none>           <none>
httpenv-6fdc8554fb-kpj8b   1/1     Running   0          118m   172.17.0.13   minikube   <none>           <none>
httpenv-6fdc8554fb-ngrjg   1/1     Running   0          118m   172.17.0.12   minikube   <none>           <none>
httpenv-6fdc8554fb-np77c   1/1     Running   0          118m   172.17.0.11   minikube   <none>           <none>
httpenv-6fdc8554fb-nwk6r   1/1     Running   0          118m   172.17.0.10   minikube   <none>           <none>
httpenv-6fdc8554fb-tkdrv   1/1     Running   0          118m   172.17.0.16   minikube   <none>           <none>
httpenv-6fdc8554fb-wgtwx   1/1     Running   0          118m   172.17.0.14   minikube   <none>           <none>

If you notice, doing the latter does not tell you the port, only the ip.

Remember that we have been working so far with a clusterIp type service, which is only for internal cluster traffic.
It does not serve to expose services to the outside world. For that we need services of type:

  NodePort: exposes a service through TCP using a port in the range 30000-32768.
  LoadBalancer: Expose a load balancer in the cluster to access that service. Defaults to RoundRobin.
  ExternalIP: Uses an external IP address. Lets you assign a physical IP address to the node, to be defined later.
  Ingress: It is a special mechanism for reverse proxy HTTP services.

Questions for this task:

  Create a deployment called littletomcat using the tomcat image.

    shpod:~# kubectl create deployment littletomcat --image tomcat --
    deployment.apps/littletomcat created

    shpod:~# kubectl expose deployment littletomcat --port 8080
    service/littletomcat exposed

  What command will help you get the IP address of that Tomcat server?

    IP=$(kubectl get svc littletomcat -o go-template --template '{{ .spec.clusterIP }}')
    echo $IP

    curl $IP:8080

    <!doctype html><html lang="en"><head><title>HTTP Status 404 – Not Found</title><style type="text/css">body {font-family:Tahoma,Arial,sans-serif;} h1, h2, h3, b {color:white;background-color:#525D76;} h1 {font-size:22px;} h2 {font-size:16px;} h3 {font-size:14px;} p {font-size:12px;} a {color:black;} .line {height:1px;background-color:#525D76;border:none;}</style></head><body><h1>HTTP Status 404 – Not Found</h1><hr class="line" /><p><b>Type</b> Status Report</p><p><b>Description</b> The origin server did not find a current representation for the target resource or is not willing to disclose that one exists.</p><hr class="line" /><h3>Apache Tomcat/10.0.11</h3></body></html>shpod:~#

  What steps would you take to ping it from another container?

  (Use the shpod environment if necessary)

    First, i find out which containers i have:

    zsh 1854 % kubectl get pods -o wide
    NAME                               READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
    hello-minikube1-7f754574c6-4m6pd   1/1     Running   0          30m   172.17.0.9   minikube   <none>           <none>
    hello-minikube1-7f754574c6-b44jh   1/1     Running   0          30m   172.17.0.8   minikube   <none>           <none>
    hello-minikube1-7f754574c6-d425b   1/1     Running   0          30m   172.17.0.7   minikube   <none>           <none>
    hello-minikube1-7f754574c6-kwt7n   1/1     Running   0          32m   172.17.0.6   minikube   <none>           <none>

    Then, i run commands like this, from the container with ip 172.17.0.9, I curl the others.
    In this image I don't have the ping command.

    <aironman@MacBook-Pro-de-Alonso:~>
    zsh 1855 % kubectl exec hello-minikube1-7f754574c6-4m6pd curl 172.17.0.8:8080
    kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
    CLIENT VALUES:
    client_address=172.17.0.9
    command=GET
    real path=/
    query=nil
    request_version=1.1
    request_uri=http:172.17.0.8:8080/

    SERVER VALUES:
    server_version=nginx: 1.10.0 - lua: 10001

    HEADERS RECEIVED:
    accept=*/*
    host=172.17.0.8:8080
    user-agent=curl/7.47.0
    BODY:
    -no body in request-  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   289    0   289    0     0   151k      0 --:--:-- --:--:-- --:--:--  282k
    [lun 21/10/04 13:55 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
    <aironman@MacBook-Pro-de-Alonso:~>
    zsh 1856 % kubectl exec hello-minikube1-7f754574c6-4m6pd curl 172.17.0.7:8080
    kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   28CLIENT VALUES:  0     0      0      0 --:--:-- --:--:-- --:--:--     0
    client_address=172.17.0.9
    command=GET
    real path=/
    query=nil
    request_version=1.1
    request_uri=http:172.17.0.7:8080/

    SERVER VALUES:
    server_version=nginx: 1.10.0 - lua: 10001

    HEADERS RECEIVED:
    accept=*/*
    host=172.17.0.7:8080
    user-agent=curl/7.47.0
    BODY:
    -no body in request-9    0   289    0     0  53737      0 --:--:-- --:--:-- --:--:-- 72250
    [lun 21/10/04 13:55 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
    <aironman@MacBook-Pro-de-Alonso:~>
    zsh 1857 % kubectl exec hello-minikube1-7f754574c6-4m6pd curl 172.17.0.6:8080
    kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   289    0   289    0     0  56833      0 --:--:-- --:--:-- --:--:-- 72250
    CLIENT VALUES:
    client_address=172.17.0.9
    command=GET
    real path=/
    query=nil
    request_version=1.1
    request_uri=http:172.17.0.6:8080/

    SERVER VALUES:
    server_version=nginx: 1.10.0 - lua: 10001

    HEADERS RECEIVED:
    accept=*/*
    host=172.17.0.6:8080
    user-agent=curl/7.47.0
    BODY:
    -no body in request-%

  What command would delete the running pod inside that deployment?

    shpod:~# kubectl get pods -l app=littletomcat -o wide
    NAME                            READY   STATUS    RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
    littletomcat-7b6c88c67d-446r9   1/1     Running   0          71s    172.17.0.7    minikube   <none>           <none>
    littletomcat-7b6c88c67d-nrg4x   1/1     Running   0          71s    172.17.0.10   minikube   <none>           <none>
    littletomcat-7b6c88c67d-sghgl   1/1     Running   0          8m9s   172.17.0.6    minikube   <none>           <none>
    littletomcat-7b6c88c67d-x2kw4   1/1     Running   0          71s    172.17.0.9    minikube   <none>           <none>
    littletomcat-7b6c88c67d-xgpkk   1/1     Running   0          71s    172.17.0.8    minikube   <none>           <none>

    kubectl delete pod littletomcat-7b6c88c67d-446r9

  What happens if we delete the pod that holds Tomcat, while the ping is running?

    Another container should be created while Deployment object

  What command can give our Tomcat server a stable DNS name and IP address?

  (An address that doesn't change when something bad happens to the container)

    kubectl expose deployment littletomcat --port 8080 --target-port 8080 --name littletomcat-lb --type LoadBalancer
    Be sure that you have a real load balancer or run minikube tunnel --cleanup

  What commands would you run to curl Tomcat with that DNS address?

  (Use the shpod environment if necessary)

  If we delete the pod that holds Tomcat, does the IP address still work? How could we test that?

    kubectl patch svc littletomcat -p '{"spec": {"ports": [{"port": 443,"targetPort": 443,"name": "https"},{"port": 8080,"targetPort": 8080,"name": "http"}],"type": "LoadBalancer"}}'

    Or we can launch these two commands. It creates two balancers, because we have to provide two different names, but LoadBalancer
    must be provided by the cloud provider where we host the cluster, as it will be their DNS who will resolve the names...
    This probably won't work well locally, in minikube, but by launching the following commands:

     create the rule of exposure to the http port
    kubectl expose deployment littletomcat --port=8080 --target-port=8080 --name=littletomcat-lb --type=LoadBalancer

     we created the safe harbour exposure rule
    kubectl expose deployment littletomcat --port=443 --target-port=443 --name=littletomcat-lb-https --type=LoadBalancer

    Another example:

    kubectl create deployment hello-minikube1 --image=k8s.gcr.io/echoserver:1.4

    kubectl expose deployment hello-minikube1 --type=LoadBalancer --port=8080

    In a real cluster, in AWS, GCP, etc., when you create a LoadBalancer type service, the provider would manage for you
    the load balancing capability for you. On local, we have to make do, if we are with minikube, with tunnel:
    Something like this will appear.
    Careful what you read, if you look, a service appears that is not called littletomcat.
    This is because at this moment, I created another deployment and another service called littletomcat.
    Before launching this, I deleted the previous deployment and service.

    zsh 1825 % minikube tunnel --cleanup
    Status:
    	machine: minikube
    	pid: 65928
    	route: 10.96.0.0/12 -> 192.168.64.3
    	minikube: Running
    	services: [hello-minikube1]
        errors:
    		minikube: no errors
    		router: no errors
    		loadbalancer emulator: no errors


    We check the external ip that minikube has given us in the unsecured port.

    [lun 21/10/04 11:09 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
    <aironman@MacBook-Pro-de-Alonso:~>
    zsh 1804 % minikube service littletomcat-lb
    |-----------|-----------------|-------------|---------------------------|
    | NAMESPACE |      NAME       | TARGET PORT |            URL            |
    |-----------|-----------------|-------------|---------------------------|
    | default   | littletomcat-lb |        8080 | http:192.168.64.3:31074 |
    |-----------|-----------------|-------------|---------------------------|
    🎉  Opening service default/littletomcat-lb in default browser...

    We check the external ip that minikube has given us in the secure port.
    See how it is the same as the previous one?

    [lun 21/10/04 11:09 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
    <aironman@MacBook-Pro-de-Alonso:~>
    zsh 1805 % minikube service littletomcat-lb-https
    |-----------|-----------------------|-------------|---------------------------|
    | NAMESPACE |         NAME          | TARGET PORT |            URL            |
    |-----------|-----------------------|-------------|---------------------------|
    | default   | littletomcat-lb-https |         443 | http:192.168.64.3:32004 |
    |-----------|-----------------------|-------------|---------------------------|
    🎉  Opening service default/littletomcat-lb-https in default browser...

     we can ping from localhost...
    [lun 21/10/04 11:09 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
    <aironman@MacBook-Pro-de-Alonso:~>
    zsh 1806 % ping 192.168.64.3
    PING 192.168.64.3 (192.168.64.3): 56 data bytes
    64 bytes from 192.168.64.3: icmp_seq=0 ttl=64 time=0.245 ms
    64 bytes from 192.168.64.3: icmp_seq=1 ttl=64 time=0.374 ms
    64 bytes from 192.168.64.3: icmp_seq=2 ttl=64 time=0.461 ms
    64 bytes from 192.168.64.3: icmp_seq=3 ttl=64 time=0.298 ms
    64 bytes from 192.168.64.3: icmp_seq=4 ttl=64 time=0.293 ms
    64 bytes from 192.168.64.3: icmp_seq=5 ttl=64 time=0.499 ms

We are going to play with a somewhat more real application, one created by Bret's
colleagues, composed of several components.

Its purpose is to mine a virtual currency
the url is located at https://github.com/BretFisher/kubernetes-mastery/tree/mastery/dockercoins

This link is usefull too. It shows the diagram architecture: https://revconf19.bretfisher.com/#32

So, we clone it and we can see there is a docker-compose.yml file. So lets run the processes via docker-compose up command.
After some time, we will see something like this:

zsh 1890 mastery% docker-compose up
[+] Running 5/5
 ⠿ Container dockercoins-rng-1     Created                                                                                                                        0.0s
 ⠿ Container dockercoins-redis-1   Recreated                                                                                                                      0.2s
 ⠿ Container dockercoins-worker-1  Created                                                                                                                        0.0s
 ⠿ Container dockercoins-webui-1   Created                                                                                                                        0.0s
 ⠿ Container dockercoins-hasher-1  Created                                                                                                                        0.0s
Attaching to dockercoins-hasher-1, dockercoins-redis-1, dockercoins-rng-1, dockercoins-webui-1, dockercoins-worker-1
...
dockercoins-worker-1  | INFO:__main__:4 units of work done, updating hash counter
dockercoins-hasher-1  | 172.19.0.3 - - [05/Oct/2021:08:55:42 +0000] "POST / HTTP/1.1" 200 64 0.1012
dockercoins-rng-1     | 172.19.0.3 - - [05/Oct/2021 08:55:42] "GET /32 HTTP/1.1" 200 -
dockercoins-hasher-1  | 172.19.0.3 - - [05/Oct/202
...

zsh 1890 % docker ps -a
CONTAINER ID   IMAGE                      COMMAND                  CREATED          STATUS                     PORTS                  NAMES
910413bcdb4c   redis                      "docker-entrypoint.s…"   43 seconds ago   Up 41 seconds              6379/tcp               dockercoins-redis-1
8b220ff08cdd   dockercoins_webui          "node webui.js"          15 minutes ago   Up 40 seconds              0.0.0.0:8000->80/tcp   dockercoins-webui-1
0ddcba1f35f6   dockercoins_rng            "python rng.py"          15 minutes ago   Up 40 seconds              0.0.0.0:8001->80/tcp   dockercoins-rng-1
ad07cb4ef85e   dockercoins_worker         "python worker.py"       15 minutes ago   Up 40 seconds                                     dockercoins-worker-1
baa28a27dbb1   dockercoins_hasher         "ruby hasher.rb"         15 minutes ago   Up 40 seconds              0.0.0.0:8002->80/tcp   dockercoins-hasher-1

We can see the webui running in a safari tab using this url 0.0.0.0:8000

Now, i am going to do the same but with k8s. I am going to create some deployments objects.

kubectl create deployment redis --image redis
kubectl create deployment hasher --image dockercoins/hasher:v0.1
kubectl create deployment rng --image dockercoins/rng:v0.1
kubectl create deployment webui --image dockercoins/webui:v0.1
kubectl create deployment worker --image dockercoins/worker:v0.1

shpod:~# kubectl get deployments -w -o wide
NAME              READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                      SELECTOR
hasher            1/1     1            1           4m41s   hasher       dockercoins/hasher:v0.1     app=hasher
hello-minikube1   4/4     4            4           23h     echoserver   k8s.gcr.io/echoserver:1.4   app=hello-minikube1
redis             1/1     1            1           7m37s   redis        redis                       app=redis
rng               1/1     1            1           4m41s   rng          dockercoins/rng:v0.1        app=rng
webui             1/1     1            1           4m41s   webui        dockercoins/webui:v0.1      app=webui
worker            1/1     1            1           4m40s   worker       dockercoins/worker:v0.1     app=worker

Now, lets check its logs:

zsh 1902 master% kubectl logs deploy/redis
1:C 05 Oct 2021 10:32:22.625 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
1:C 05 Oct 2021 10:32:22.625 # Redis version=6.2.5, bits=64, commit=00000000, modified=0, pid=1, just started
1:C 05 Oct 2021 10:32:22.625 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
1:M 05 Oct 2021 10:32:22.626 * monotonic clock: POSIX clock_gettime
1:M 05 Oct 2021 10:32:22.626 * Running mode=standalone, port=6379.
1:M 05 Oct 2021 10:32:22.626 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
1:M 05 Oct 2021 10:32:22.626 # Server initialized
1:M 05 Oct 2021 10:32:22.626 * Ready to accept connections

redis is apparently looking fine, but probably we will have to do something with its port, 6379.

zsh 1903 master% kubectl logs deploy/hasher
== Sinatra (v2.0.1) has taken the stage on 80 for development with backup from Thin

hasher is looking fine.

zsh 1904 master% kubectl logs deploy/rng
 * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)

rng is looking fine.

zsh 1905 master% kubectl logs deploy/webui
WEBUI running on port 80
Redis error { [Error: Redis connection to redis:6379 failed - getaddrinfo ENOTFOUND redis redis:6379]
  code: 'ENOTFOUND',
  errno: 'ENOTFOUND',
  syscall: 'getaddrinfo',
  hostname: 'redis',
  host: 'redis',
  port: 6379 }

WebUi looks is not running fine.

zsh 1907 master% kubectl logs deploy/worker | more
INFO:__main__:0 units of work done, updating hash counter
ERROR:__main__:In work loop:
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/redis/connection.py", line 484, in connect
    sock = self._connect()
  File "/usr/local/lib/python3.6/site-packages/redis/connection.py", line 511, in _connect
    socket.SOCK_STREAM):
  File "/usr/local/lib/python3.6/socket.py", line 745, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name does not resolve
...

worker is not looking fine too. Definetly there are some connections problems.
I have to expose some ports. Creating some ClusterIP services.

zsh 1909 master% kubectl expose deployment redis --port 6379
service/redis exposed

zsh 1910 master% kubectl expose deployment rng --port 80
service/rng exposed

zsh 1911 master% kubectl expose deployment hasher --port 80
service/hasher exposed

After that, i am going to run kubectl logs deploy/worker --follow command

kubectl logs deploy/worker --follow command
...
ERROR:__main__:Waiting 10s and restarting.
INFO:__main__:0 units of work done, updating hash counter
INFO:__main__:4 units of work done, updating hash counter
INFO:__main__:Coin found: 04420bff...
INFO:__main__:4 units of work done, updating hash counter
INFO:__main__:Coin found: 06f2c9e1...
INFO:__main__:Coin found: 0ffe8f95...
INFO:__main__:4 units of work done, updating hash counter

What happened? i have exposed into the cluster the necessary ports between internal services.
Please, remember how the services are connected between them.

Please, remember the Architecture

https://revconf19.bretfisher.com/#32

rng, hasher and redis needs to talk with worker, so i opened their working ports, exposing them
to the cluster, only to the cluster, not outside of it (ClusterIP).

Now, i am going to expose the webui to the exterior, so i will expose it as a NodePort.

zsh 1913 [130] master% kubectl expose deployment webui --port 80 --type NodePort
service/webui exposed

[mar 21/10/05 13:01 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/alonsoir.github.io/Devop>
zsh 1914 master% kubectl get svc
NAME              TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)          AGE
hasher            ClusterIP      10.110.187.36    <none>           80/TCP           11m
hello-minikube1   LoadBalancer   10.110.114.191   10.110.114.191   8080:30747/TCP   23h
kubernetes        ClusterIP      10.96.0.1        <none>           443/TCP          25d
redis             ClusterIP      10.97.123.66     <none>           6379/TCP         12m
rng               ClusterIP      10.103.101.112   <none>           80/TCP           11m
webui             NodePort       10.104.105.61    <none>           80:31329/TCP     7s

As i have minikube running in my mac, i have to run this in order to have the webui
ip address.

[mar 21/10/05 13:01 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/alonsoir.github.io/Devop>
zsh 1915 master% minikube service webui
|-----------|-------|-------------|---------------------------|
| NAMESPACE | NAME  | TARGET PORT |            URL            |
|-----------|-------|-------------|---------------------------|
| default   | webui |          80 | http://192.168.64.3:31329 |
|-----------|-------|-------------|---------------------------|
🎉  Opening service default/webui in default browser...

You should see in Safari or whatever browser you have the same web page showing
DockerCoin miner webui.

Now i am going to scale containers in order to get more than 4 hashes per second.
How about if i scale the worker deployment until two replicas?

zsh 1917 % kubectl scale deployment worker --replicas 2
deployment.apps/worker scaled

Running watch commands. I can see the new pods
Every 2,0s: kubectl get pods                                                                                      MacBook-Pro-de-Alonso.local: Tue Oct  5 13:17:40 2021

NAME                               READY   STATUS    RESTARTS   AGE
hasher-ccc9f44ff-dp8mp             1/1     Running   0          42m
hello-minikube1-7f754574c6-4m6pd   1/1     Running   0          23h
hello-minikube1-7f754574c6-b44jh   1/1     Running   0          23h
hello-minikube1-7f754574c6-d425b   1/1     Running   0          23h
hello-minikube1-7f754574c6-kwt7n   1/1     Running   0          23h
redis-6749d7bd65-8xmcq             1/1     Running   0          45m
rng-5d8b6c4cff-ctbwp               1/1     Running   0          42m
webui-5f69bbf966-6gk77             1/1     Running   0          42m
worker-699dc8c88-6csgw             1/1     Running   0          42m
worker-699dc8c88-g7wrj             1/1     Running   0          108s

Every 2,0s: kubectl get deployments                                                                               MacBook-Pro-de-Alonso.local: Tue Oct  5 13:17:58 2021

NAME              READY   UP-TO-DATE   AVAILABLE   AGE
hasher            1/1     1            1           42m
hello-minikube1   4/4     4            4           23h
redis             1/1     1            1           45m
rng               1/1     1            1           42m
webui             1/1     1            1           42m
worker            2/2     2            2           42m


In the webpage i can see that i have 8 hashes per second, oscillating between 4 and 8. Great.

If you try to scale the worker service up and down, you will notice that one more container will give you
four more hashes per second, and one less will substract four hashes per second.
Now you can think that we can increase more workers to the cluster, more processing power, more money, dont you?

i am going to increase to 10 workers, the performance should be 40 hashes per second right?
It is stuck at 10 hashes per second! what happened?

In the real world, we have tools like DataDog, honeycomb, new relic, statsd, sumologic,...

In shpod container we have two useful apps, ab and httping, in order to stress the webui server.

zsh 1930 % kubectl get svc
NAME              TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)          AGE
hasher            ClusterIP      10.110.187.36    <none>           80/TCP           61m
hello-minikube1   LoadBalancer   10.110.114.191   10.110.114.191   8080:30747/TCP   24h
kubernetes        ClusterIP      10.96.0.1        <none>           443/TCP          26d
redis             ClusterIP      10.97.123.66     <none>           6379/TCP         63m
rng               ClusterIP      10.103.101.112   <none>           80/TCP           62m
webui             NodePort       10.104.105.61    <none>           80:31329/TCP     50m

Or, i can figure it out hasher ip address and rng ip too...

zsh 1928 [1] % kubectl get svc hasher -o go-template={{.spec.clusterIP}}
10.110.187.36

[mar 21/10/05 13:50 CEST][s008][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~>
zsh 1929 % kubectl get svc rng -o go-template={{.spec.clusterIP}}
10.103.101.112

or programmatically...

RNG=$(kubectl get svc rng -o go-template={{.spec.clusterIP}})
echo $RNG

HASHER=$(kubectl get svc hasher -o go-template={{.spec.clusterIP}})
echo $HASHER

httping -c 3 $HASHER

httping -c 3 $RNG

shpod:~# httping -c 3 $HASHER
PING 10.110.187.36:80 (/):
connected to 10.110.187.36:80 (210 bytes), seq=0 time=  2.33 ms
connected to 10.110.187.36:80 (210 bytes), seq=1 time=  2.00 ms
connected to 10.110.187.36:80 (210 bytes), seq=2 time=  1.38 ms
--- http://10.110.187.36/ ping statistics ---
3 connects, 3 ok, 0.00% failed, time 3008ms
round-trip min/avg/max = 1.4/1.9/2.3 ms

shpod:~# httping -c 3 $R
$RANDOM  $RNG
shpod:~# httping -c 3 $RNG
PING 10.103.101.112:80 (/):
connected to 10.103.101.112:80 (158 bytes), seq=0 time=767.88 ms
connected to 10.103.101.112:80 (158 bytes), seq=1 time=752.14 ms
connected to 10.103.101.112:80 (158 bytes), seq=2 time=752.75 ms
--- http://10.103.101.112/ ping statistics ---
3 connects, 3 ok, 0.00% failed, time 5276ms
round-trip min/avg/max = 752.1/757.6/767.9 ms

It looks like rng service is the problem. So, how about if i try to scale it up
in a multinode cluster?

In my mac, if i scale rng service to two replicas, i got the double perfomance,
like 20 hashes per second. Three containers hashes per second ranges from 10 to 30.

shpod:~# httping -c 4 $RNG
PING 10.103.101.112:80 (/):
connected to 10.103.101.112:80 (158 bytes), seq=0 time= 98.01 ms
connected to 10.103.101.112:80 (158 bytes), seq=1 time= 11.83 ms
connected to 10.103.101.112:80 (158 bytes), seq=2 time= 45.07 ms
connected to 10.103.101.112:80 (158 bytes), seq=3 time=  9.21 ms
--- http://10.103.101.112/ ping statistics ---
4 connects, 4 ok, 0.00% failed, time 4171ms

Times are much better with three containers, three deployments.
What about with 10 rng containers?

I can see that there are only 19 to 38 hashes per second in the webpage

shpod:~# httping -c 4 $RNG
PING 10.103.101.112:80 (/):
connected to 10.103.101.112:80 (158 bytes), seq=0 time=  3.22 ms
connected to 10.103.101.112:80 (158 bytes), seq=1 time=  4.38 ms
connected to 10.103.101.112:80 (158 bytes), seq=2 time=  2.56 ms
connected to 10.103.101.112:80 (158 bytes), seq=3 time=  4.83 ms
--- http://10.103.101.112/ ping statistics ---
4 connects, 4 ok, 0.00% failed, time 4019ms
round-trip min/avg/max = 2.6/3.7/4.8 ms

ping times are much better than using three rng containers, so what happened?

Before that, some tasks...

Let's deploy another application called wordsmith
Wordsmith has 3 components:

a web frontend bretfisher/wordsmith-web

an API backend bretfisher/wordsmith-words

a postgres DB bretfisher/wordsmith-db

These images have been built and pushed to Docker Hub

We want to deploy all 3 components on Kubernetes

Here's how the parts of this app communicate with each other:

The web frontend listens on port 80

The web frontend should be public (available on a high port from outside the cluster)

The web frontend connects to the API at the address http://words:8080

The API backend listens on port 8080

The API connects to the database with the connection string pgsql://db:5432

The database listens on port 5432

Your Assignment is to create the kubectl create commands to make this all work
This is what we should see when we bring up the web frontend on our browser:


  (You will probably see a different sentence, though.)

- Yes, there is some repetition in that sentence; that's OK for now

- If you see empty LEGO bricks, something's wrong ...

Preguntas de esta tarea
What deployment commands did you use to create the pods?

Alonso´solution
// backend deployment
kubectl create deployment wordsmith-db --image bretfisher/wordsmith-db
// frontend deployment
kubectl create deployment wordsmith-web --image bretfisher/wordsmith-web
// API deployment
kubectl create deployment wordsmith-words --image bretfisher/wordsmith-words

Bret`s solution:

kubectl create deployment db --image=bretfisher/wordsmith-db
kubectl create deployment web --image=bretfisher/wordsmith-web
kubectl create deployment words --image=bretfisher/wordsmith-words

What service commands did you use to make the pods available on a friendly DNS name?

kubectl expose deployment wordsmith-web --type NodePort --ports 8080
kubectl expose deployment wordsmith-words --ports 8080
kubectl expose deployment wordsmith-db --ports 5432

Bret´s solution:
kubectl expose deployment db --port=5432
kubectl expose deployment web --port=80 --type=NodePort
kubectl expose deployment words --port=8080

or

kubectl create service clusterip db --tcp=5432
kubectl create service nodeport web --tcp=80
kubectl create service clusterip words --tcp=8080

If we add more wordsmith-words API pods, then when the browser is refreshed, you'll see different words.
What is the command to scale that deployment up to 5 pods? Test it to ensure a browser refresh shows different words.

kubectl scale deployment wordsmith-words --replicas 5

Bret´s solution:
kubectl scale deployment words --replicas=5

Ok, mine is not correct because DNS is hardcoded inside the code,
so frontend will never connect to API, API will not connect to db...

Deploying with yaml.

kubectl apply -f my-manifest.yaml

example: https://github.com/BretFisher/kubernetes-mastery/blob/mastery/k8s/dockercoins.yaml

Deployments vs daemonsets vs StatefulSets

https://medium.com/stakater/k8s-deployments-vs-statefulsets-vs-daemonsets-60582f0c62d4

Añadir al label de un pod que ya tenga un label con app=rng la cadena enabled=yes

zsh 2023 mastery% kubectl label pods -l app=rng enabled=yes
pod/rng-5d8b6c4cff-lgm7s labeled
pod/rng-x85c5 labeled

Then, run this command:

shpod:~# kubectl edit service rng

Look for selector tag, add

enabled: "yes"

Last login: Thu Oct  7 16:47:30 on ttys002
Zelda
              _
             /_\
_            )_(            _
|`-.___,.-~'`|=|`'~-.,___,-'|
|  __________|=|__________  |
| |    ______|=|__________| | ___      _      _  _   _             _
| |   |  ____|=|_____     / |  |  |_| |_  |  |_ | _ |_ |\| |\  /\ |_
| |   | /    |=|    /    /| |  |  | | |_  |_ |_ |_| |_ | | |/  \/ |
| |   |/   ,-|_|-. / /  /_|_|______ ______     _______        ____
| |      ,' _____ / // / \    ___  |\    /     \      `.      \   \
| |     / ,'| A |/ // /   |  |   \ | |  |       |  |`.  \     /    \
| |    /_// |/V\/ // /    |  |    \| |  |       |  |  \  \   /  /\  \
| |      /__| |/  / /     |  |       |  |       |  |   \  | /  /  \  \
| |     /\  | / /| /\     |  |__/|   |  |       |  |   |  ||  |    |  |
| |    /  \ |/ // // \    |   __ |   |  |       |  |   |  ||  |____|  |
| |   /    \/ |/ //   \   |  |  \|   |  |       |  |   |  ||   ____   |
| |  /     /    //     \  |  |       |  |       |  |   /  ||  |    |  |
| | /     / /  /|       \ |  |    /| |  |    /| |  |  /  / |  |    |  |
| |/_____/ // / |________\|  |___/ | |  |___/ | |  |,'  /  |  |    |  |     aironman@MacBook-Pro-de-Alonso.local
| |     / // /| |        /_________|/_________|/______,'  /____\  /____\    ------------------------------------
\ \    / // / | |       /|/ /_               ___  ___    _   _  _  _ ___    OS: macOS 11.6 20G165 x86_64
 \ \  /  / /| | |______/ | //_\  |  | |\| |/  |/\  | |_||_  |_)/_\(_` |     Host: MacBookPro15,3
  \ \/______| | |________|/ | |  |_ | | | |\  |\/  | | ||_  |  | |._) |     Kernel: 20.6.0
   `.`.     | | |     ,','                                                  Uptime: 1 day, 3 hours, 5 mins
     `.`.   | | |   ,','    _        _        _        _                    Packages: 195 (brew)
       `.`-.| | |,-','     |.\      |.\      |.\      |.\                   Shell: zsh 5.8
         `-.| | |,-'        \\      \\      \\      \\                      Resolution: 1680x1050
            | | |    ________\\______\\______\\______\\_____________        DE: Aqua
            | | |   | .--  __        __    ,--.        __   __   __| _  |   WM: Quartz Compositor
            | | |   | |-  /  \ |  | |  `   `--. | | | /  \ |  ` /  |(_` |   WM Theme: Blue (Light)
            | | |   | |   \__/ \_/| |      .__/ \/ \/ \__/ |    \__|._) |   Terminal: iTerm2
            | | |   '---------------------------------------------------'   Terminal Font: MesloLGMForPowerline-Italic 16 (normal) / Hack-Regular 12 (non-ascii)
             \|/                  \\ /|   \\ /|   \\ /|   \\ /|             CPU: Intel i9-9980HK (16) @ 2.40GHz
              V                  __\V /  __\V /  __\V /  __\V /             GPU: Intel UHD Graphics 630, Radeon Pro Vega 20
                                 \___O/   \___O/   \___O/   \___O/          Memory: 15124MiB / 32768MiB
                                     \/\      \/\      \/\      \/\
                                      \/\      \/\      \/\      \/\
                                       (O)      (O)      (O)      (O)

Opening a dashboard!

<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2002 mastery% kubectl apply -f insecure-dashboard.yaml
namespace/kubernetes-dashboard unchanged
serviceaccount/kubernetes-dashboard unchanged
service/kubernetes-dashboard unchanged
secret/kubernetes-dashboard-certs unchanged
secret/kubernetes-dashboard-csrf configured
secret/kubernetes-dashboard-key-holder unchanged
configmap/kubernetes-dashboard-settings unchanged
role.rbac.authorization.k8s.io/kubernetes-dashboard unchanged
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard unchanged
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard unchanged
deployment.apps/kubernetes-dashboard unchanged
service/dashboard-metrics-scraper unchanged
deployment.apps/dashboard-metrics-scraper unchanged
deployment.apps/dashboard created
service/dashboard created
clusterrolebinding.rbac.authorization.k8s.io/insecure-dashboard unchanged
The ClusterRoleBinding "kubernetes-dashboard" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:"rbac.authorization.k8s.io", Kind:"ClusterRole", Name:"kubernetes-dashboard"}: cannot change roleRef

[jue 21/10/07 16:50 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2004 mastery% kubectl apply -f dockercoins.yaml
deployment.apps/hasher created
service/hasher created
deployment.apps/redis created
service/redis created
deployment.apps/rng created
service/rng created
deployment.apps/webui created
service/webui created
deployment.apps/worker created
[jue 21/10/07 16:51 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>

Please minikube, open it in a browser.

zsh 2005 mastery% minikube service dashboard
|-----------|-----------|-------------|---------------------------|
| NAMESPACE |   NAME    | TARGET PORT |            URL            |
|-----------|-----------|-------------|---------------------------|
| default   | dashboard |          80 | http://192.168.64.3:32763 |
|-----------|-----------|-------------|---------------------------|
🎉  Opening service default/dashboard in default browser...

[jue 21/10/07 16:51 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2006 mastery% kubectl version --short
Client Version: v1.22.1
Server Version: v1.22.1

[jue 21/10/07 16:53 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2007 mastery% kubectl get all --all-namespaces
NAMESPACE              NAME                                             READY   STATUS    RESTARTS      AGE
default                pod/dashboard-79c6957b6f-5bdrr                   1/1     Running   0             3m29s
default                pod/hasher-ccc9f44ff-xgm8n                       1/1     Running   0             3m3s
default                pod/redis-6749d7bd65-2dnvj                       1/1     Running   0             3m3s
default                pod/rng-5d8b6c4cff-lgm7s                         1/1     Running   0             3m3s
default                pod/webui-5f69bbf966-2ddqg                       1/1     Running   0             3m3s
default                pod/worker-699dc8c88-2wzxw                       1/1     Running   0             3m2s
default                pod/worker-699dc8c88-4tqp5                       1/1     Running   0             3m2s
default                pod/worker-699dc8c88-5nqjp                       1/1     Running   0             3m2s
default                pod/worker-699dc8c88-bgbrk                       1/1     Running   0             3m2s
default                pod/worker-699dc8c88-fv6qw                       1/1     Running   0             3m2s
default                pod/worker-699dc8c88-fzqqk                       1/1     Running   0             3m2s
default                pod/worker-699dc8c88-mhcsq                       1/1     Running   0             3m2s
default                pod/worker-699dc8c88-n4kk8                       1/1     Running   0             3m2s
default                pod/worker-699dc8c88-p6gqq                       1/1     Running   0             3m2s
default                pod/worker-699dc8c88-pc9cq                       1/1     Running   0             3m2s
kube-system            pod/coredns-78fcd69978-lq24r                     1/1     Running   4 (24h ago)   28d
kube-system            pod/etcd-minikube                                1/1     Running   4 (24h ago)   28d
kube-system            pod/kube-apiserver-minikube                      1/1     Running   4 (24h ago)   28d
kube-system            pod/kube-controller-manager-minikube             1/1     Running   4 (24h ago)   28d
kube-system            pod/kube-proxy-79r5q                             1/1     Running   4 (24h ago)   28d
kube-system            pod/kube-scheduler-minikube                      1/1     Running   4 (24h ago)   28d
kube-system            pod/storage-provisioner                          1/1     Running   7 (24h ago)   28d
kubernetes-dashboard   pod/dashboard-metrics-scraper-869f64985b-xv5qj   1/1     Running   0             24h
kubernetes-dashboard   pod/kubernetes-dashboard-67b97d586c-qq6tb        1/1     Running   0             24h
shpod                  pod/shpod                                        1/1     Running   6 (24h ago)   10d

NAMESPACE              NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default                service/dashboard                   NodePort    10.103.74.228   <none>        80:32763/TCP             3m29s
default                service/hasher                      ClusterIP   10.110.195.5    <none>        80/TCP                   3m3s
default                service/kubernetes                  ClusterIP   10.96.0.1       <none>        443/TCP                  28d
default                service/redis                       ClusterIP   10.109.170.70   <none>        6379/TCP                 3m3s
default                service/rng                         ClusterIP   10.99.167.224   <none>        80/TCP                   3m3s
default                service/webui                       NodePort    10.97.244.1     <none>        80:32125/TCP             3m2s
kube-system            service/kube-dns                    ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   28d
kubernetes-dashboard   service/dashboard-metrics-scraper   ClusterIP   10.106.0.137    <none>        8000/TCP                 23d
kubernetes-dashboard   service/kubernetes-dashboard        ClusterIP   10.110.32.200   <none>        443/TCP                  23d

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   28d

NAMESPACE              NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
default                deployment.apps/dashboard                   1/1     1            1           3m29s
default                deployment.apps/hasher                      1/1     1            1           3m3s
default                deployment.apps/redis                       1/1     1            1           3m3s
default                deployment.apps/rng                         1/1     1            1           3m3s
default                deployment.apps/webui                       1/1     1            1           3m3s
default                deployment.apps/worker                      10/10   10           10          3m2s
kube-system            deployment.apps/coredns                     1/1     1            1           28d
kubernetes-dashboard   deployment.apps/dashboard-metrics-scraper   1/1     1            1           23d
kubernetes-dashboard   deployment.apps/kubernetes-dashboard        1/1     1            1           23d

NAMESPACE              NAME                                                   DESIRED   CURRENT   READY   AGE
default                replicaset.apps/dashboard-79c6957b6f                   1         1         1       3m29s
default                replicaset.apps/hasher-ccc9f44ff                       1         1         1       3m3s
default                replicaset.apps/redis-6749d7bd65                       1         1         1       3m3s
default                replicaset.apps/rng-5d8b6c4cff                         1         1         1       3m3s
default                replicaset.apps/webui-5f69bbf966                       1         1         1       3m3s
default                replicaset.apps/worker-699dc8c88                       10        10        10      3m2s
kube-system            replicaset.apps/coredns-78fcd69978                     1         1         1       28d
kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-5594458c94   0         0         0       10d
kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-7976b667d4   0         0         0       23d
kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-869f64985b   1         1         1       24h
kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-654cf69797        0         0         0       10d
kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-67b97d586c        1         1         1       24h
kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-6fcdf4f6d         0         0         0       23d
[jue 21/10/07 16:53 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2008 mastery% kubectl get all --all-namespaces
NAMESPACE              NAME                                             READY   STATUS    RESTARTS      AGE
default                pod/dashboard-79c6957b6f-5bdrr                   1/1     Running   0             14m
default                pod/hasher-ccc9f44ff-xgm8n                       1/1     Running   0             13m
default                pod/my-ingress-nginx-ingress-b896685d9-jntjm     1/1     Running   0             16s
default                pod/redis-6749d7bd65-2dnvj                       1/1     Running   0             13m
default                pod/rng-5d8b6c4cff-lgm7s                         1/1     Running   0             13m
default                pod/webui-5f69bbf966-2ddqg                       1/1     Running   0             13m
default                pod/worker-699dc8c88-2wzxw                       1/1     Running   0             13m
default                pod/worker-699dc8c88-4tqp5                       1/1     Running   0             13m
default                pod/worker-699dc8c88-5nqjp                       1/1     Running   0             13m
default                pod/worker-699dc8c88-bgbrk                       1/1     Running   0             13m
default                pod/worker-699dc8c88-fv6qw                       1/1     Running   0             13m
default                pod/worker-699dc8c88-fzqqk                       1/1     Running   0             13m
default                pod/worker-699dc8c88-mhcsq                       1/1     Running   0             13m
default                pod/worker-699dc8c88-n4kk8                       1/1     Running   0             13m
default                pod/worker-699dc8c88-p6gqq                       1/1     Running   0             13m
default                pod/worker-699dc8c88-pc9cq                       1/1     Running   0             13m
kube-system            pod/coredns-78fcd69978-lq24r                     1/1     Running   4 (25h ago)   28d
kube-system            pod/etcd-minikube                                1/1     Running   4 (25h ago)   28d
kube-system            pod/kube-apiserver-minikube                      1/1     Running   4 (25h ago)   28d
kube-system            pod/kube-controller-manager-minikube             1/1     Running   4 (25h ago)   28d
kube-system            pod/kube-proxy-79r5q                             1/1     Running   4 (25h ago)   28d
kube-system            pod/kube-scheduler-minikube                      1/1     Running   4 (25h ago)   28d
kube-system            pod/storage-provisioner                          1/1     Running   7 (25h ago)   28d
kubernetes-dashboard   pod/dashboard-metrics-scraper-869f64985b-xv5qj   1/1     Running   0             24h
kubernetes-dashboard   pod/kubernetes-dashboard-67b97d586c-qq6tb        1/1     Running   0             24h
shpod                  pod/shpod                                        1/1     Running   6 (24h ago)   10d

NAMESPACE              NAME                                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
default                service/dashboard                   NodePort       10.103.74.228   <none>        80:32763/TCP                 14m
default                service/hasher                      ClusterIP      10.110.195.5    <none>        80/TCP                       13m
default                service/kubernetes                  ClusterIP      10.96.0.1       <none>        443/TCP                      28d
default                service/my-ingress-nginx-ingress    LoadBalancer   10.98.97.50     <pending>     80:32330/TCP,443:32493/TCP   16s
default                service/redis                       ClusterIP      10.109.170.70   <none>        6379/TCP                     13m
default                service/rng                         ClusterIP      10.99.167.224   <none>        80/TCP                       13m
default                service/webui                       NodePort       10.97.244.1     <none>        80:32125/TCP                 13m
kube-system            service/kube-dns                    ClusterIP      10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP       28d
kubernetes-dashboard   service/dashboard-metrics-scraper   ClusterIP      10.106.0.137    <none>        8000/TCP                     23d
kubernetes-dashboard   service/kubernetes-dashboard        ClusterIP      10.110.32.200   <none>        443/TCP                      23d

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   28d

NAMESPACE              NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
default                deployment.apps/dashboard                   1/1     1            1           14m
default                deployment.apps/hasher                      1/1     1            1           13m
default                deployment.apps/my-ingress-nginx-ingress    1/1     1            1           16s
default                deployment.apps/redis                       1/1     1            1           13m
default                deployment.apps/rng                         1/1     1            1           13m
default                deployment.apps/webui                       1/1     1            1           13m
default                deployment.apps/worker                      10/10   10           10          13m
kube-system            deployment.apps/coredns                     1/1     1            1           28d
kubernetes-dashboard   deployment.apps/dashboard-metrics-scraper   1/1     1            1           23d
kubernetes-dashboard   deployment.apps/kubernetes-dashboard        1/1     1            1           23d

NAMESPACE              NAME                                                   DESIRED   CURRENT   READY   AGE
default                replicaset.apps/dashboard-79c6957b6f                   1         1         1       14m
default                replicaset.apps/hasher-ccc9f44ff                       1         1         1       13m
default                replicaset.apps/my-ingress-nginx-ingress-b896685d9     1         1         1       16s
default                replicaset.apps/redis-6749d7bd65                       1         1         1       13m
default                replicaset.apps/rng-5d8b6c4cff                         1         1         1       13m
default                replicaset.apps/webui-5f69bbf966                       1         1         1       13m
default                replicaset.apps/worker-699dc8c88                       10        10        10      13m
kube-system            replicaset.apps/coredns-78fcd69978                     1         1         1       28d
kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-5594458c94   0         0         0       10d
kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-7976b667d4   0         0         0       23d
kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-869f64985b   1         1         1       24h
kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-654cf69797        0         0         0       10d
kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-67b97d586c        1         1         1       24h
kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-6fcdf4f6d         0         0         0       23d


zsh 2012 mastery% kubectl get deployment rng -o yaml > rng.yaml
[jue 21/10/07 17:07 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2013 mastery% cat rng.yaml
apiVersion: apps/v1
kind: Deployment ---> i am going to change this, to Daemonset
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"rng"},"name":"rng","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rng"}},"template":{"metadata":{"labels":{"app":"rng"}},"spec":{"containers":[{"image":"dockercoins/rng:v0.1","name":"rng"}]}}}}
  creationTimestamp: "2021-10-07T14:50:47Z"
  generation: 1
  labels:
    app: rng
  name: rng
  namespace: default
  resourceVersion: "259054"
  uid: 3bd05032-c08d-4170-96c0-4dd79392551a
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: rng
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: rng
    spec:
      containers:
      - image: dockercoins/rng:v0.1
        imagePullPolicy: IfNotPresent
        name: rng
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
 rng.yaml                                                                                                                                                    buffers
1   apiVersion: apps/v1
  1 kind: Deployment
  2 metadata:
  3   annotations:
  4   . deployment.kubernetes.io/revision: "1"
  5   . kubectl.kubernetes.io/last-applied-configuration: |
  6   . . {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"rng"},"name":"rng","namespace":"default"},"spec":{"replicas":1,"sel
  7   creationTimestamp: "2021-10-07T14:50:47Z"
  8   generation: 1
  9   labels:
 10   . app: rng
 11   name: rng
 12   namespace: default
 13   resourceVersion: "259054"
 14   uid: 3bd05032-c08d-4170-96c0-4dd79392551a
 15 spec:
 16   progressDeadlineSeconds: 600
 17   replicas: 1
 NORMAL  rng.yaml                                                                                                      yaml  utf-8[unix]    1% ☰    1/64  :  1 
"rng.yaml" 64L, 1952B
 rng.yaml                                                                                                                                                    buffers
1   apiVersion: apps/v1
  1 kind: Deployment
  2 metadata:
  3   annotations:
  4   . deployment.kubernetes.io/revision: "1"
  5   . kubectl.kubernetes.io/last-applied-configuration: |
  6   . . {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"rng"},"name":"rng","namespace":"default"},"spec":{"replicas":1,"sel
  7   creationTimestamp: "2021-10-07T14:50:47Z"
  8   generation: 1
  9   labels:
 10   . app: rng
 11   name: rng
 12   namespace: default
 13   resourceVersion: "259054"
 14   uid: 3bd05032-c08d-4170-96c0-4dd79392551a
 15 spec:
 16   progressDeadlineSeconds: 600
 17   replicas: 1
 18   revisionHistoryLimit: 10
 NORMAL  rng.yaml                                                                                                      yaml  utf-8[unix]    1% ☰    1/64  :  1 
 rng.yaml                                                                                                                                                    buffers
1   apiVersion: apps/v1
  1 kind: Deployment
  2 metadata:
  3   annotations:
  4   . deployment.kubernetes.io/revision: "1"
  5   . kubectl.kubernetes.io/last-applied-configuration: |
  6   . . {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"rng"},"name":"rng","namespace":"default"},"spec":{"replicas":1,"sel
  7   creationTimestamp: "2021-10-07T14:50:47Z"
  8   generation: 1
  9   labels:
 10   . app: rng
 11   name: rng
 12   namespace: default
 13   resourceVersion: "259054"
 14   uid: 3bd05032-c08d-4170-96c0-4dd79392551a
 15 spec:
 16   progressDeadlineSeconds: 600
 17   replicas: 1
 18   revisionHistoryLimit: 10
 19   selector:
 20   . matchLabels:
 21   . . app: rng
 22   strategy:
 23   . rollingUpdate:
 24   . . maxSurge: 25%
 25   . . maxUnavailable: 25%
 26   . type: RollingUpdate
 NORMAL  rng.yaml                                                                                                      yaml  utf-8[unix]    1% ☰    1/64  :  1 
 rng.yaml                                                                                                                                                    buffers
1   apiVersion: apps/v1
  1 kind: Deployment
  2 metadata:
  3   annotations:
  4   . deployment.kubernetes.io/revision: "1"
  5   . kubectl.kubernetes.io/last-applied-configuration: |
  6   . . {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"rng"},"name":"rng","namespace":"default"},"spec":{"replicas":1,"sel
  7   creationTimestamp: "2021-10-07T14:50:47Z"
  8   generation: 1
  9   labels:
 10   . app: rng
 11   name: rng
 12   namespace: default
 13   resourceVersion: "259054"
 14   uid: 3bd05032-c08d-4170-96c0-4dd79392551a
 15 spec:
 16   progressDeadlineSeconds: 600
 17   replicas: 1
 18   revisionHistoryLimit: 10
 19   selector:
 20   . matchLabels:
 21   . . app: rng
 22   strategy:
 23   . rollingUpdate:
 24   . . maxSurge: 25%
 25   . . maxUnavailable: 25%
 26   . type: RollingUpdate
 27   template:
 NORMAL  rng.yaml                                                                                                      yaml  utf-8[unix]    1% ☰    1/64  :  1 
 rng.yaml+                                                                                                                                                   buffers
  conditions:
  - lastTransitionTime: "2021-10-07T14:51:00Z"
    lastUpdateTime: "2021-10-07T14:51:00Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2021-10-07T14:50:47Z"
    lastUpdateTime: "2021-10-07T14:51:00Z"
    message: ReplicaSet "rng-5d8b6c4cff" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
[jue 21/10/07 17:08 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2014 mastery% vi rng.yaml
[jue 21/10/07 17:09 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2015 mastery% kubectl apply -f rng.yaml
error: error validating "rng.yaml": error validating data: [ValidationError(DaemonSet.spec): unknown field "progressDeadlineSeconds" in io.k8s.api.apps.v1.DaemonSetSpec, ValidationError(DaemonSet.spec): unknown field "replicas" in io.k8s.api.apps.v1.DaemonSetSpec, ValidationError(DaemonSet.spec): unknown field "strategy" in io.k8s.api.apps.v1.DaemonSetSpec, ValidationError(DaemonSet.status): unknown field "availableReplicas" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError(DaemonSet.status.conditions[0]): unknown field "lastUpdateTime" in io.k8s.api.apps.v1.DaemonSetCondition, ValidationError(DaemonSet.status.conditions[1]): unknown field "lastUpdateTime" in io.k8s.api.apps.v1.DaemonSetCondition, ValidationError(DaemonSet.status): unknown field "readyReplicas" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError(DaemonSet.status): unknown field "replicas" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError(DaemonSet.status): unknown field "updatedReplicas" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError(DaemonSet.status): missing required field "currentNumberScheduled" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError(DaemonSet.status): missing required field "numberMisscheduled" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError(DaemonSet.status): missing required field "desiredNumberScheduled" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError(DaemonSet.status): missing required field "numberReady" in io.k8s.api.apps.v1.DaemonSetStatus]; if you choose to ignore these errors, turn validation off with --validate=false
[jue 21/10/07 17:10 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2016 [1] mastery% vi rng.yaml
[jue 21/10/07 17:10 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2017 mastery% minikube service webui
|-----------|-------|-------------|---------------------------|
| NAMESPACE | NAME  | TARGET PORT |            URL            |
|-----------|-------|-------------|---------------------------|
| default   | webui |          80 | http://192.168.64.3:32125 |
|-----------|-------|-------------|---------------------------|
🎉  Opening service default/webui in default browser...

What is it a DaemonSet?

It is like a Deployment, without a ReplicaSet, k8s ensures that only one container
is running in every node. Useful to run loggers in every node, or monitors...

I am going to modify rng.yaml file, changing Deployment type to DaemonSet type:

Generate the yaml file from rng Service:

kubectl get deployment rng -o yaml > rng.yml
Edit the the file, changing Deployment type to DaemonSet type.
Aply changes:

kubectl apply -f rng-yml
It wont work because some server validations. Fix it, or apply without validations.
On your own risk!

[jue 21/10/07 17:16 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2019 [1] mastery% kubectl apply -f rng.yaml --validate=false
daemonset.apps/rng created
[jue 21/10/07 17:17 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2020 mastery% kubectl describe service rng
...
 NORMAL  /private/var/folders/0x/gmzly6vn2f7ggfk47bdcsbnc0000gn/T/kubectl-edit-2929862863.yaml                         yaml  utf-8[unix]    2% ☰    1/36  :  1 
 /p/v/f/0/g/T/kubectl-edit-2929862863.yaml                                                                                                                   buffers
1   # Please edit the object below. Lines beginning with a '#' will be ignored,
  1 # and an empty file will abort the edit. If an error occurs while saving this file will be
  2 # reopened with the relevant failures.
  3 #
  4 apiVersion: v1
  5 kind: Service
  6 metadata:
  7   annotations:
  8   . kubectl.kubernetes.io/last-applied-configuration: |
  9   . . {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"rng"},"name":"rng","namespace":"default"},"spec":{"ports":[{"port":80,"prot
 10   creationTimestamp: "2021-10-07T14:50:47Z"
 11   labels:
 12   . app: rng
 13   name: rng
 14   namespace: default
 15   resourceVersion: "270394"
 16   uid: 851fce11-7c00-4330-b41a-e324b299a72b
 17 spec:
 18   clusterIP: 10.99.167.224
 19   clusterIPs:
 20   - 10.99.167.224
 21   internalTrafficPolicy: Cluster
 22   ipFamilies:
 23   - IPv4
 24   ipFamilyPolicy: SingleStack
 25   ports:
 26   - port: 80
 27   . protocol: TCP
 28   . targetPort: 80
 29   selector:
 30   . app: rng
 31   . enabled: "yes" --> NEW LINE!!
 32   sessionAffinity: None
 33   type: ClusterIP
 34 status:
 35   loadBalancer: {}
 NORMAL  /private/var/folders/0x/gmzly6vn2f7ggfk47bdcsbnc0000gn/T/kubectl-edit-2929862863.yaml

Now, i am going to detect, select and change the behaviour of previous Deployment rng object:

zsh 2032 [1] mastery% POD=$(kubectl get pod -l app=rng,pod-template-hash -o name)
[vie 21/10/08 12:23 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2033 mastery% echo $POD
pod/rng-5d8b6c4cff-lgm7s

[vie 21/10/08 12:24 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2035 [1] mastery% kubectl logs --tail 1 --follow $POD
172.17.0.1 - - [08/Oct/2021 10:24:46] "GET /32 HTTP/1.1" 200 -
172.17.0.1 - - [08/Oct/2021 10:24:46] "GET /32 HTTP/1.1" 200 -
...

in another tab, i will remove the label enabled: "true" from the pod marked with
label app=rng and pod-template-hash.

zsh 2036 % kubectl label pod -l app=rng,pod-template-hash enabled-
pod/rng-5d8b6c4cff-lgm7s labeled


The get petitions should be off inmediately! You should see that in the browser that
number of hashes are going down dramatically, to the half.
With this action, changing the label of a pod which belongs to a Deployment,
the requests from worker pod will not arrive to that pod! without the need of deleting the pod.

That is the purpose of editing labels, when we detect something weird in that pod, we can
change the label, the pod will not operate through its upper object, so it will be disconected,
so we can examinate it later with calm.
If i delete it, the ReplicaSet will recreate it again, that is the reason why we
change the labels in production, we will be able to debug the container, find the bug,
attach to a debugger...


Our goal here will be to create a service that load balances connections to two different deployments.
You might use this as a simplistic way to run two versions of your apps in parallel for a period of time.

In the real world, you'll likely use a 3rd party load balancer to provide advanced blue/green or
canary-style deployments, but this assignment will help further your understanding of how service
selectors are used to find pods and use them as service endpoints.

For simplicity, version 1 of our application will be using the NGINX image,
and version 2 of our application will be using the Apache image.
They both listen on port 80 by default and have different HTML by default so that
it's easy to distinguish which is being accessed.

Once properly set up, when we connect to the service we expect to see some
requests being served by NGINX and some requests being served by Apache.

Objectives:
We need to create two deployments: one for v1 (NGINX), another for v2 (Apache).

They will be exposed through a single service.

The selector of that service will need to match the pods created by *both* deployments.

For that, we will need to change the deployment specification to add an extra label, to be used solely by the service.

That label should be different from the pre-existing labels of our deployments, otherwise, our deployments will step on each other's toes.

We're not at the point of writing our own YAML from scratch, so you'll need to use the kubectl edit command to modify existing resources.

Preguntas de esta tarea
What commands did you use to perform the following?

1. Create a deployment running one pod using the official NGINX image.

kubectl create deployment my-nginx --image nginx

2. Expose that deployment.

kubectl expose deployment my-nginx --port=80 --type=NodePort

Bret`s solution:

  kubectl create deployment v1-nginx --image=nginx

  kubectl expose deployment v1-nginx --port=80

  or

  kubectl create service v1-nginx --tcp=80

  kubectl get svc v1-nginx
  curl http://A.B.C.D

3. Check that you can successfully connect to the exposed service.

What commands did you use to perform the following?

I am using minikube, so i used:

minikube service my-nginx

1. Change (edit) the service definition to use a label/value of myapp: web

kubectl edit service my-nginx

Go to Selector tag, add enabled: "yes"

which selector tag????

zsh 2046 % kubectl edit service my-nginx
service/my-nginx edited


2. Check that you cannot connect to the exposed service anymore.

I cant

3. Change (edit) the deployment definition to add that label/value to the pod template.

kubectl edit deployment my-nginx

4. Check that you *can* connect to the exposed service again.

What commands did you use to perform the following?

1. Create a deployment running one pod using the official Apache image (httpd).

kubectl create deployment apache --image httpd

kubectl expose deployment apache --port 80 --type NodePort

2. Change (edit) the deployment definition to add the label/value picked previously.

kubectl edit deployment apache

3. Connect to the exposed service again.

(It should now yield responses from both Apache and NGINX.)

kubectl label pod -l app=apache,pod-template-hash enabled-

kubectl label pod -l app=my-nginx,pod-template-hash enabled-


Bret`s

What commands did you use to perform the following?

1. Create a deployment running one pod using the official NGINX image.

  kubectl create deployment v1-nginx --image=nginx

2. Expose that deployment.

  kubectl expose deployment v1-nginx --port=80
  or
  kubectl create service v1-nginx --tcp=80
3. Check that you can successfully connect to the exposed service.

If you are using `shpod`, or if you are running directly on the cluster:

  kubectl get svc v1-nginx
  curl http://A.B.C.D
  You can also run a program like `curl` in a container:

  kubectl run --restart=Never --image=alpine -ti --rm testcontainer
  ### Then, once you get a prompt, install curl
  apk add curl
  curl v1-nginx

What commands did you use to perform the following?

1. Change (edit) the service definition to use a label/value of myapp: web

    Edit the YAML manifest of the service with kubectl edit service v1-nginx.
    Look for the selector: section, and change app: v1-nginx to myapp: web.
    Make sure to change the selector: section, not the labels: section!
    After making the change, save and quit.

2. Check that you cannot connect to the exposed service anymore.

    The reason is doing above, i broke the way to connect the objects belonging
    to a hierarchy. Remember:

      Deployment
        ReplicaSet
          Pod
            containers.

    A service creates an ip address to that Deployment, and the way to connect them is
    using the selectors. I am adding a new one, so k8s doesnt do the match between the
    service and Deployment object.

3. Change (edit) the deployment definition to add that label/value to the pod template.

    The curl command should now time out. The service can't find a pod with that label yet.

4. Check that you *can* connect to the exposed service again.

    Edit the YAML manifest of the deployment with the next command:

      kubectl edit deployment v1-nginx.

    Look for the labels: section within the template: section, as we want to change
    the labels of the pods created by the deployment, not of the deployment itself.

    Ignore the matchLabels: one. Add myapp: web just below app: v1-nginx, with the
    same indentation level. After making the change, save and quit.

    We need both labels here, unlike the service selector.
    The app label keeps the pod "linked" to the deployment/replicaset, and the new
    one will cause the service to match to this pod.

5. The `curl` command should now work again.

  (It might need a minute, since changing the label will trigger a rolling update and create a new pod.)

What commands did you use to perform the following?

1. Create a deployment running one pod using the official Apache image (httpd).

  kubectl create deployment v2-apache --image=httpd

2. Change (edit) the deployment definition to add the label/value picked previously.

  Same as previously: kubectl edit deployment v2-apache, then add the label myapp: web below app: v2-apache.
  Again, make sure to change the labels in the pod template, not of the deployment itself.

3. Connect to the exposed service again.

(It should now yield responses from both Apache and NGINX.)

3. The curl command show now yield responses from NGINX and Apache.

(Note: you won't see a perfect round-robin, i.e. NGINX/Apache/NGINX/Apache etc.,
but on average, Apache and NGINX should serve approximately 50% of the requests each.)


YAML files!

https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html

How to create a template file, using --dry-run

zsh 2094 % kubectl create deployment web --image nginx -o yaml --dry-run=client
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: web
  name: web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: web
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

Another one:

zsh 2097 % kubectl create namespace awesome-app -o yaml --dry-run=client
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: awesome-app
spec: {}
status: {}

Using kubectl diff

[mié 21/10/13 09:16 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~>
zsh 2127 % curl -O https://k8smastery.com/just-a-pod.yaml
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   124  100   124    0     0     89      0  0:00:01  0:00:01 --:--:--    89

[mié 21/10/13 09:28 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~>
zsh 2128 % kubectl apply -f just-a-pod.yaml
pod/hello created

# Change nginx version to 1.17
[mié 21/10/13 09:28 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~>
zsh 2129 % vim just-a-pod.yaml

[mié 21/10/13 09:30 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~>
zsh 2131 [2] % kubectl diff -f just-a-pod.yaml
diff -u -N /var/folders/0x/gmzly6vn2f7ggfk47bdcsbnc0000gn/T/LIVE-762420065/v1.Pod.default.hello /var/folders/0x/gmzly6vn2f7ggfk47bdcsbnc0000gn/T/MERGED-220878156/v1.Pod.default.hello
--- /var/folders/0x/gmzly6vn2f7ggfk47bdcsbnc0000gn/T/LIVE-762420065/v1.Pod.default.hello	2021-10-13 09:30:11.000000000 +0200
+++ /var/folders/0x/gmzly6vn2f7ggfk47bdcsbnc0000gn/T/MERGED-220878156/v1.Pod.default.hello	2021-10-13 09:30:11.000000000 +0200
@@ -75,7 +75,7 @@
   uid: 82c7ee65-6ee0-43e0-9e49-37f891bad31c
 spec:
   containers:
-  - image: nginx
+  - image: nginx:1.17
     imagePullPolicy: Always
     name: hello
     resources: {}
[mié 21/10/13 09:30 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~>
zsh 2132 [1] %

Strategy Rolling Updates!

I can do rolling updates to Deployment, DaemonSets and statefulsets!
Editing them, the yaml file, will result in a rolling update
The resources can be monitored using a command like this:

shpod:~# kubectl get deployments -o json | jq ".items[] | {name:.metadata.name} + .spec.strategy.rollingUpdate"
{
  "name": "dashboard",
  "maxSurge": "25%",
  "maxUnavailable": "25%"
}
{
  "name": "hasher",
  "maxSurge": "25%",
  "maxUnavailable": "25%"
}
{
  "name": "redis",
  "maxSurge": "25%",
  "maxUnavailable": "25%"
}
{
  "name": "rng",
  "maxSurge": "25%",
  "maxUnavailable": "25%"
}
{
  "name": "webui",
  "maxSurge": "25%",
  "maxUnavailable": "25%"
}
{
  "name": "worker",
  "maxSurge": "25%",
  "maxUnavailable": "25%"
}

or using the command kubectl rollout

shpod:~# kubectl get deployments
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
dashboard   1/1     1            1           5d17h
hasher      1/1     1            1           10m
redis       1/1     1            1           10m
rng         1/1     1            1           10m
webui       1/1     1            1           10m
worker      10/10   10           10          10m
shpod:~# kubectl rollout status deployment worker
deployment "worker" successfully rolled out
shpod:~# kubectl rollout history deployment worker
deployment.apps/worker
REVISION  CHANGE-CAUSE
1         <none>

What happen if you try to update a deployment in a wrong way?

shpod:~# kubectl set image deploy worker worker=dockercoins/worker:v0.3
deployment.apps/worker image updated
shpod:~# kubectl rollout status deploy worker
Waiting for deployment "worker" rollout to finish: 5 out of 10 new replicas have been updated...

[mié 21/10/13 10:44 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~>
zsh 2135 % kubectl get deployments -w
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
dashboard   1/1     1            1           5d17h
hasher      1/1     1            1           19m
redis       1/1     1            1           19m
rng         1/1     1            1           19m
webui       1/1     1            1           19m
worker      8/10    5            8           19m

[mié 21/10/13 10:44 CEST][s003][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~>
zsh 2136 % kubectl get pods -w
NAME                         READY   STATUS             RESTARTS      AGE
dashboard-79c6957b6f-5bdrr   1/1     Running            2 (23h ago)   5d17h
hasher-ccc9f44ff-z56pz       1/1     Running            0             19m
redis-6749d7bd65-c6fng       1/1     Running            0             19m
rng-5d8b6c4cff-w4mw4         1/1     Running            0             19m
webui-5f69bbf966-vhk6m       1/1     Running            0             19m
worker-699dc8c88-f2j6j       1/1     Running            0             19m
worker-699dc8c88-h2nn2       1/1     Running            0             19m
worker-699dc8c88-ml96b       1/1     Running            0             19m
worker-699dc8c88-mv5rs       1/1     Running            0             19m
worker-699dc8c88-mxfpl       1/1     Running            0             19m
worker-699dc8c88-tq584       1/1     Running            0             19m
worker-699dc8c88-wrkpl       1/1     Running            0             19m
worker-699dc8c88-xhgrx       1/1     Running            0             19m
worker-6b6cd45fb6-4mjg9      0/1     ImagePullBackOff   0             48s
worker-6b6cd45fb6-6gzff      0/1     ImagePullBackOff   0             48s
worker-6b6cd45fb6-hnf5n      0/1     ErrImagePull       0             48s
worker-6b6cd45fb6-hxpn4      0/1     ErrImagePull       0             48s
worker-6b6cd45fb6-q62hb      0/1     ErrImagePull       0             48s
worker-6b6cd45fb6-q62hb      0/1     ImagePullBackOff   0             51s
worker-6b6cd45fb6-hnf5n      0/1     ImagePullBackOff   0             51s
worker-6b6cd45fb6-hxpn4      0/1     ImagePullBackOff   0             52s
worker-6b6cd45fb6-4mjg9      0/1     ErrImagePull       0             57s
worker-6b6cd45fb6-6gzff      0/1     ErrImagePull       0             60s
worker-6b6cd45fb6-hnf5n      0/1     ErrImagePull       0             65s
worker-6b6cd45fb6-q62hb      0/1     ErrImagePull       0             67s
worker-6b6cd45fb6-hxpn4      0/1     ErrImagePull       0             69s
worker-6b6cd45fb6-4mjg9      0/1     ImagePullBackOff   0             69s
worker-6b6cd45fb6-6gzff      0/1     ImagePullBackOff   0             71s
worker-6b6cd45fb6-hnf5n      0/1     ImagePullBackOff   0             78s
worker-6b6cd45fb6-hxpn4      0/1     ImagePullBackOff   0             80s
worker-6b6cd45fb6-q62hb      0/1     ImagePullBackOff   0             80s
worker-6b6cd45fb6-6gzff      0/1     ErrImagePull       0             110s
worker-6b6cd45fb6-4mjg9      0/1     ErrImagePull       0             112s
worker-6b6cd45fb6-q62hb      0/1     ErrImagePull       0             2m
worker-6b6cd45fb6-hnf5n      0/1     ErrImagePull       0             2m1s
worker-6b6cd45fb6-6gzff      0/1     ImagePullBackOff   0             2m2s
worker-6b6cd45fb6-hxpn4      0/1     ErrImagePull       0             2m2s
worker-6b6cd45fb6-4mjg9      0/1     ImagePullBackOff   0             2m7s
worker-6b6cd45fb6-hnf5n      0/1     ImagePullBackOff   0             2m15s
worker-6b6cd45fb6-q62hb      0/1     ImagePullBackOff   0             2m15s
worker-6b6cd45fb6-hxpn4      0/1     ImagePullBackOff   0             2m17s
worker-6b6cd45fb6-6gzff      0/1     ErrImagePull       0             3m9s
worker-6b6cd45fb6-q62hb      0/1     ErrImagePull       0             3m20s
worker-6b6cd45fb6-6gzff      0/1     ImagePullBackOff   0             3m22s
worker-6b6cd45fb6-4mjg9      0/1     ErrImagePull       0             3m25s
worker-6b6cd45fb6-hxpn4      0/1     ErrImagePull       0             3m28s
worker-6b6cd45fb6-q62hb      0/1     ImagePullBackOff   0             3m33s
worker-6b6cd45fb6-hnf5n      0/1     ErrImagePull       0             3m36s
worker-6b6cd45fb6-4mjg9      0/1     ImagePullBackOff   0             3m38s
worker-6b6cd45fb6-hxpn4      0/1     ImagePullBackOff   0             3m42s

you see that there are a lot of errors, basically there is no v.0.3 version,
so k8s cannot pull the image. But i know that because i provoked it.

I can go to the dashboard, or execute a command like this:

zsh 2158 mastery% kubectl describe deployment worker
Name:                   worker
Namespace:              default
CreationTimestamp:      Wed, 13 Oct 2021 10:25:13 +0200
Labels:                 app=worker
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=worker
Replicas:               10 desired | 5 updated | 13 total | 8 available | 5 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=worker
  Containers:
   worker:
    Image:        dockercoins/worker:v0.3
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    False   ProgressDeadlineExceeded
OldReplicaSets:  worker-699dc8c88 (8/8 replicas created)
NewReplicaSet:   worker-6b6cd45fb6 (5/5 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  53m   deployment-controller  Scaled up replica set worker-699dc8c88 to 10
  Normal  ScalingReplicaSet  34m   deployment-controller  Scaled up replica set worker-6b6cd45fb6 to 3
  Normal  ScalingReplicaSet  34m   deployment-controller  Scaled down replica set worker-699dc8c88 to 8
  Normal  ScalingReplicaSet  34m   deployment-controller  Scaled up replica set worker-6b6cd45fb6 to 5
[mié 21/10/13 11:18 CEST][s004][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2159 mastery%

how can i rollout to a previous deployment? i know that worker deployment is
causing problems, so i will rollout that deployment:

[mié 21/10/13 11:18 CEST][s004][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2159 mastery% kubectl rollout undo deployment worker
deployment.apps/worker rolled back

[mié 21/10/13 11:25 CEST][s004][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2160 mastery% kubectl rollout status deployment worker

deployment "worker" successfully rolled out

In the dashboard i can see now that i have 10 containers in the worker pod again.
What happen if i execute rollout undo a second time?
i will be at problems again because k8s will be in the previous state with the
deployment version that causes the problem! Thats a good reason to work with yaml files instead of
commands.

Listing history deployments:

[mié 21/10/13 11:37 CEST][s004][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2175 [1] mastery% kubectl rollout history deployment
deployment.apps/dashboard
REVISION  CHANGE-CAUSE
1         <none>

deployment.apps/hasher
REVISION  CHANGE-CAUSE
1         <none>

deployment.apps/redis
REVISION  CHANGE-CAUSE
1         <none>

deployment.apps/rng
REVISION  CHANGE-CAUSE
1         <none>

deployment.apps/webui
REVISION  CHANGE-CAUSE
1         <none>

deployment.apps/worker
REVISION  CHANGE-CAUSE
2         <none>
3         <none>

If we have too much data, we can filter:

[mié 21/10/13 12:05 CEST][s004][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2176 mastery% kubectl rollout history deployment worker
deployment.apps/worker
REVISION  CHANGE-CAUSE
2         <none>
3         <none>

Check the annotations from our replicasets:

[mié 21/10/13 12:11 CEST][s004][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2179 [1] mastery% kubectl describe replicasets -l app=worker | grep -A3 Annotations
Annotations:    deployment.kubernetes.io/desired-replicas: 10
                deployment.kubernetes.io/max-replicas: 13
                deployment.kubernetes.io/revision: 3
                deployment.kubernetes.io/revision-history: 1
--
Annotations:    deployment.kubernetes.io/desired-replicas: 10
                deployment.kubernetes.io/max-replicas: 13
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/worker

Rollback to the known good deployment version, in my case, i know that version 2 is fine:

[mié 21/10/13 12:17 CEST][s004][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/git/kubernetes-mastery/k8s>
zsh 2182 [1] mastery% kubectl rollout undo deployment worker --to-revision=2
deployment.apps/worker rolled back

Probably, in a real escenario, revision would be a hash, because it will be the same hash commit number.

Health checks!

livenessProbe example:

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: rng
  name: rng
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rng
  template:
    metadata:
      labels:
        app: rng
    spec:
      containers:
      - image: dockercoins/rng:v0.1
        name: rng
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 5

zsh 2199 master% kubectl get events -w
          0s          Warning   Unhealthy           pod/rng-7695b69f79-swjdf      Liveness probe failed: Get "http://172.17.0.21:80/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
          0s          Warning   Unhealthy           pod/rng-7695b69f79-swjdf      Liveness probe failed: Get "http://172.17.0.21:80/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
          0s          Warning   Unhealthy           pod/rng-7695b69f79-swjdf      Liveness probe failed: Get "http://172.17.0.21:80/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
          0s          Warning   Unhealthy           pod/rng-7695b69f79-swjdf      Liveness probe failed: Get "http://172.17.0.21:80/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
          0s          Normal    Killing             pod/rng-7695b69f79-swjdf      Container rng failed liveness probe, will be restarted
          0s          Normal    Pulled              pod/rng-7695b69f79-swjdf      Container image "dockercoins/rng:v0.1" already present on machine
          0s          Normal    Created             pod/rng-7695b69f79-swjdf      Created container rng
          0s          Normal    Started             pod/rng-7695b69f79-swjdf      Started container rng
          0s          Warning   Unhealthy           pod/rng-7695b69f79-swjdf      Liveness probe failed: Get "http://172.17.0.21:80/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
          0s          Warning   Unhealthy           pod/rng-7695b69f79-swjdf      Liveness probe failed: Get "http://172.17.0.21:80/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
          0s          Warning   Unhealthy           pod/rng-7695b69f79-swjdf      Liveness probe failed: Get "http://172.17.0.21:80/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
          0s          Normal    Killing             pod/rng-7695b69f79-swjdf      Container rng failed liveness probe, will be restarted
          0s          Normal    Pulled              pod/rng-7695b69f79-swjdf      Container image "dockercoins/rng:v0.1" already present on machine
          0s          Normal    Created             pod/rng-7695b69f79-swjdf      Created container rng
          0s          Normal    Started             pod/rng-7695b69f79-swjdf      Started container rng

[mié 21/10/13 18:26 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~>
zsh 2199 % kubectl get pods -w
          NAME                         READY   STATUS    RESTARTS   AGE
          dashboard-79c6957b6f-4zjfk   1/1     Running   0          7h19m
          hasher-ccc9f44ff-z56pz       1/1     Running   0          8h
          redis-6749d7bd65-c6fng       1/1     Running   0          8h
          rng-7695b69f79-swjdf         1/1     Running   0          3m33s
          webui-5f69bbf966-vhk6m       1/1     Running   0          8h
          worker-699dc8c88-tq584       1/1     Running   0          8h
          rng-7695b69f79-swjdf         1/1     Running   1 (0s ago)   6m32s
          rng-7695b69f79-swjdf         1/1     Running   2 (1s ago)   7m48s

shpod:~# ab -c 10 -n 1000 http://10.108.188.190/1
          This is ApacheBench, Version 2.3 <$Revision: 1879490 $>
          Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
          Licensed to The Apache Software Foundation, http://www.apache.org/

          Benchmarking 10.108.188.190 (be patient)
          Completed 100 requests
          Completed 200 requests
          Completed 300 requests
          Completed 400 requests
          ^C

          Server Software:        Werkzeug/0.14.1
          Server Hostname:        10.108.188.190
          Server Port:            80

          Document Path:          /1
          Document Length:        1 bytes

          Concurrency Level:      10
          Time taken for tests:   47.607 seconds
          Complete requests:      428
          Failed requests:        0
          Total transferred:      65912 bytes
          HTML transferred:       428 bytes
          Requests per second:    8.99 [#/sec] (mean)
          Time per request:       1112.324 [ms] (mean)
          Time per request:       111.232 [ms] (mean, across all concurrent requests)
          Transfer rate:          1.35 [Kbytes/sec] received

          Connection Times (ms)
                        min  mean[+/-sd] median   max
          Connect:        0    0   0.2      0       2
          Processing:   103 1098 112.6   1128    1154
          Waiting:      102 1098 112.7   1128    1154
          Total:        105 1099 112.5   1128    1154

          Percentage of the requests served within a certain time (ms)
            50%   1128
            66%   1130
            75%   1132
            80%   1133
            90%   1137
            95%   1142
            98%   1149
            99%   1151
           100%   1154 (longest request)

Again, 100 concurrent requests!

shpod:~# ab -c 100 -n 1000 http://10.108.188.190/1
          This is ApacheBench, Version 2.3 <$Revision: 1879490 $>
          Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
          Licensed to The Apache Software Foundation, http://www.apache.org/

          Benchmarking 10.108.188.190 (be patient)
          apr_socket_recv: Connection reset by peer (104)
          Total of 6 requests completed
          shpod:~# ab -c 100 -n 1000 http://10.108.188.190/1
          This is ApacheBench, Version 2.3 <$Revision: 1879490 $>
          Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
          Licensed to The Apache Software Foundation, http://www.apache.org/

          Benchmarking 10.108.188.190 (be patient)
          Completed 100 requests
          Completed 200 requests
          Completed 300 requests
          Completed 400 requests
          Completed 500 requests
          apr_socket_recv: Connection reset by peer (104)
          Total of 535 requests completed
          shpod:~#
