Conceptos k8s.

https://slides.kubernetesmastery.com/#63

0. Start the cluster (with minikube)

  minikube start --wait=false

Comprobamos que está todo ok:

  kubectl cluster-info
  kubect get nodes

shpod:

For a consistent Kubernetes experience You can use shpod for examples

shpod provides a shell running in a pod on the cluster

It comes with many tools pre-installed (helm, stern, curl, jq...)

These tools are used in many exercises in these slides

shpod also gives you shell completion and a fancy prompt

Create it with:

  kubectl apply -f https://k8smastery.com/shpod.yaml
    namespace/shpod created
    serviceaccount/shpod created
    clusterrolebinding.rbac.authorization.k8s.io/shpod created
    pod/shpod created

Una vez que has aplicado por primera vez el yaml de arriba, solo tienes que ejecutar
este comando la siguiente vez:

  kubectl attach -i -t shpod -n shpod

éste también funciona, menos abreviado:

  kubectl attach --namespace=shpod -ti shpod

    If you don't see a command prompt, try pressing enter.

  shpod:~# pwd
    /root
  shpod:~# uname -ra
    Linux shpod 4.19.202 #1 SMP Thu Sep 2 18:19:24 UTC 2021 x86_64 Linux

  Attach to shell with

    kubectl attach --namespace=shpod -ti shpod

  type exit if you want to leave the shpod shell...

  After finishing course

    kubectl delete -f https://k8smastery.com/shpod.yaml

  Review the pod in their namespace shpod...

    kubectl get pods -n shpod
    NAME    READY   STATUS    RESTARTS   AGE
    shpod   1/1     Running   0          4h51m

  Do you need a second shpod shell?

    kubectl exec --namespace=shpod -ti shpod -- bash -l

1. How to Deploy Containers in k8s

With a running Kubernetes cluster, containers can now be deployed.

Using kubectl run, it allows containers to be deployed onto the cluster -

  kubectl create deployment first-deployment --image=katacoda/docker-http-server

The status of the deployment can be discovered via the running Pods -

  kubectl get pods

Once the container is running it can be exposed via different networking options, depending on requirements.
One possible solution is NodePort, that provides a dynamic port to a container.

  kubectl expose deployment first-deployment --port=80 --type=NodePort

The command below finds the allocated port and executes a HTTP request.

  export PORT=$(kubectl get svc first-deployment -o go-template='{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{"\n"}}{{end}}{{end}}')

  echo "Accessing host01:$PORT"

  curl host01:$PORT

The result is the container that processed the request.

2. Dashboard

Enable the dashboard using Minikube with the command minikube addons enable dashboard

Make the Kubernetes Dashboard available by deploying the following YAML definition.
This should only be used on Katacoda. Busca el equivalente de Bret para abrir el
dashboard.

kubectl apply -f /opt/kubernetes-dashboard.yaml

The Kubernetes dashboard allows you to view your applications in a UI.
In this deployment, the dashboard has been made available on port 30000 but may take a while to start.

To see the progress of the Dashboard starting, watch the Pods within the kube-system namespace using

  kubectl get pods -n kubernetes-dashboard -w

Once running, the URL to the dashboard is

  https://2886795282-30000-kitek05.environments.katacoda.com/

3. Pod:

Un conjunto de uno o más contenedores accesibles a través de una direccion IP.
Esa direccion IP sirve para acceder al Pod, nunca a los contenedores por si, de hecho,
los contenedores no son accesibles directamente.

Un Pod es la unidad más pequeña en un cluster k8s.

Los contenedores de un pod comparten localhost y pueden compartir entre ellos
volumenes de datos.

kubectl uses this config file:

zsh 1609 % pwd
/Users/aironman/.kube
[lun 21/09/27 16:40 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/.kube>
zsh 1610 % ll
Executing ls -lh
total 16
drwxr-x---  4 aironman  staff   128B 28 abr 12:39 cache
-rw-------  1 aironman  staff   6,2K 27 sep 10:28 config

zsh 1616 % kubectl get node -o wide
NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube   Ready    control-plane,master   18d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker://20.10.8
[lun 21/09/27 17:03 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]

En plural tambien funciona:

kubectl get nodes -o wide
NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube   Ready    control-plane,master   18d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker://20.10.8

O usando una diminutivo:

zsh 1620 % kubectl get no -o wide
NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube   Ready    control-plane,master   18d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker://20.10.8

Podemos sacar la configuracion en formato YAML

[lun 21/09/27 17:03 CEST][s002][x86_64/darwin20.0/20.6.0][5.8]
<aironman@MacBook-Pro-de-Alonso:~/.kube>
zsh 1619 % kubectl get nodes -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2021-09-09T11:45:46Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: minikube
      kubernetes.io/os: linux
      minikube.k8s.io/commit: 5931455374810b1bbeb222a9713ae2c756daee10
      minikube.k8s.io/name: minikube
      minikube.k8s.io/updated_at: 2021_09_09T13_45_50_0700
      minikube.k8s.io/version: v1.23.0
      node-role.kubernetes.io/control-plane: ""
      node-role.kubernetes.io/master: ""
      node.kubernetes.io/exclude-from-external-load-balancers: ""
    name: minikube
    resourceVersion: "117812"
    uid: 5cdc10d6-f485-49f3-8a7d-51c3f4b40f86
  spec:
    podCIDR: 10.244.0.0/24
    podCIDRs:
    - 10.244.0.0/24
  status:
    addresses:
    - address: 192.168.64.3
      type: InternalIP
    - address: minikube
      type: Hostname
    allocatable:
      cpu: "2"
      ephemeral-storage: 17784752Ki
      hugepages-2Mi: "0"
      memory: 5952468Ki
      pods: "110"
    capacity:
      cpu: "2"
      ephemeral-storage: 17784752Ki
      hugepages-2Mi: "0"
      memory: 5952468Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2021-09-27T15:00:54Z"
      lastTransitionTime: "2021-09-17T01:39:31Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2021-09-27T15:00:54Z"
      lastTransitionTime: "2021-09-17T01:39:31Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2021-09-27T15:00:54Z"
      lastTransitionTime: "2021-09-17T01:39:31Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2021-09-27T15:00:54Z"
      lastTransitionTime: "2021-09-17T01:39:31Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - bretfisher/shpod@sha256:eb2cb1e1fc39478242e2e201057181618ea2eb0bee68aa50de5d60a4474e19f8
      - bretfisher/shpod:latest
      sizeBytes: 558201235
    - names:
      - k8s.gcr.io/etcd@sha256:9ce33ba33d8e738a5b85ed50b5080ac746deceed4a7496c550927a7a19ca3b6d
      - k8s.gcr.io/etcd:3.5.0-0
      sizeBytes: 294536887
    - names:
      - quay.io/radanalyticsio/spark-operator@sha256:7bfd8866134d864486ae18de72d79bf2699b1d08dab37a0e095bc86643863b8d
      - quay.io/radanalyticsio/spark-operator:latest-released
      sizeBytes: 272154571
    - names:
      - kubernetesui/dashboard@sha256:7f80b5ba141bead69c4fee8661464857af300d7d7ed0274cf7beecedc00322e6
      sizeBytes: 225733746
    - names:
      - kubernetesui/dashboard@sha256:ec27f462cf1946220f5a9ace416a84a57c18f98c777876a8054405d1428cc92e
      - kubernetesui/dashboard:v2.3.1
      sizeBytes: 220033604
    - names:
      - k8s.gcr.io/kube-apiserver@sha256:6862d5a70cea8f3ef49213d6a36b7bfbbf90f99fb37f7124505be55f0ef51364
      - k8s.gcr.io/kube-apiserver:v1.22.1
      sizeBytes: 128446877
    - names:
      - k8s.gcr.io/kube-controller-manager@sha256:3e4274dee8a122bdd5e3f3db6b1eb8db59404deda2bf1adb0fec1da5dd95400a
      - k8s.gcr.io/kube-controller-manager:v1.22.1
      sizeBytes: 121979567
    - names:
      - nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
      - nginx:1.14.2
      sizeBytes: 109129446
    - names:
      - k8s.gcr.io/kube-proxy@sha256:efcf1d5fb2fc95d28841f534f1385a4884230c7c876fb1b7cf66d2777ad6dc56
      - k8s.gcr.io/kube-proxy:v1.22.1
      sizeBytes: 103645121
    - names:
      - k8s.gcr.io/kube-scheduler@sha256:e1a999694bf4b9198bc220216680ef651fabe406445a93c2d354f9dd7e53c1fd
      - k8s.gcr.io/kube-scheduler:v1.22.1
      sizeBytes: 52658888
    - names:
      - k8s.gcr.io/coredns/coredns@sha256:6e5a02c21641597998b4be7cb5eb1e7b02c0d8d23cce4dd09f4682d463798890
      - k8s.gcr.io/coredns/coredns:v1.8.4
      sizeBytes: 47554275
    - names:
      - kubernetesui/metrics-scraper@sha256:555981a24f184420f3be0c79d4efb6c948a85cfce84034f85a563f4151a81cbf
      sizeBytes: 36937728
    - names:
      - kubernetesui/metrics-scraper@sha256:36d5b3f60e1a144cc5ada820910535074bdf5cf73fb70d1ff1681537eef4e172
      - kubernetesui/metrics-scraper:v1.0.7
      sizeBytes: 34446077
    - names:
      - gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944
      - gcr.io/k8s-minikube/storage-provisioner:v5
      sizeBytes: 31465472
    - names:
      - k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07
      - k8s.gcr.io/pause:3.5
      sizeBytes: 682696
    nodeInfo:
      architecture: amd64
      bootID: 4525a770-36c1-4027-8bc8-197fd21510e2
      containerRuntimeVersion: docker://20.10.8
      kernelVersion: 4.19.202
      kubeProxyVersion: v1.22.1
      kubeletVersion: v1.22.1
      machineID: cea931afcb7147aa92509f298edb734c
      operatingSystem: linux
      osImage: Buildroot 2021.02.4
      systemUUID: 5a7211ec-0000-0000-ac0a-acde48001122
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

Incluso en formato json, ideal para parsear con la herramienta jq: HORRIBLE!

shpod:~# kubectl get no -o json | jq ".items[] | {name:.metadata.name} + .status.capacity"
{
  "name": "minikube",
  "cpu": "2",
  "ephemeral-storage": "17784752Ki",
  "hugepages-2Mi": "0",
  "memory": "5952468Ki",
  "pods": "110"
}

Horrible en mi opinión, tenemos el comando siguiente:

shpod:~# kubectl get no
NAME       STATUS   ROLES                  AGE   VERSION
minikube   Ready    control-plane,master   18d   v1.22.1

shpod:~# kubectl describe node minikube
Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5931455374810b1bbeb222a9713ae2c756daee10
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/updated_at=2021_09_09T13_45_50_0700
                    minikube.k8s.io/version=v1.23.0
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 09 Sep 2021 11:45:46 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 27 Sep 2021 15:19:27 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 27 Sep 2021 15:15:59 +0000   Fri, 17 Sep 2021 01:39:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 27 Sep 2021 15:15:59 +0000   Fri, 17 Sep 2021 01:39:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 27 Sep 2021 15:15:59 +0000   Fri, 17 Sep 2021 01:39:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 27 Sep 2021 15:15:59 +0000   Fri, 17 Sep 2021 01:39:31 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.64.3
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             5952468Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             5952468Ki
  pods:               110
System Info:
  Machine ID:                 cea931afcb7147aa92509f298edb734c
  System UUID:                5a7211ec-0000-0000-ac0a-acde48001122
  Boot ID:                    4525a770-36c1-4027-8bc8-197fd21510e2
  Kernel Version:             4.19.202
  OS Image:                   Buildroot 2021.02.4
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.8
  Kubelet Version:            v1.22.1
  Kube-Proxy Version:         v1.22.1
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-78fcd69978-lq24r                      100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     18d
  kube-system                 etcd-minikube                                 100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         18d
  kube-system                 kube-apiserver-minikube                       250m (12%)    0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-controller-manager-minikube              200m (10%)    0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-proxy-79r5q                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-scheduler-minikube                       100m (5%)     0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  kubernetes-dashboard        dashboard-metrics-scraper-5594458c94-qbght    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h50m
  kubernetes-dashboard        kubernetes-dashboard-654cf69797-hrkmr         0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h50m
  shpod                       shpod                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h35m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>
shpod:~#

Listando tipos de recursos disponibles en un cluster:

shpod:~# kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v1                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                               true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
endpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta1   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta1   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
csistoragecapacities                           storage.k8s.io/v1beta1                 true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
shpod:~#

Madre mia, mogollon de recursos, para describir que hacen, podemos usar este comando:

shpod:~# kubectl explain deployment
KIND:     Deployment
VERSION:  apps/v1

DESCRIPTION:
     Deployment enables declarative updates for Pods and ReplicaSets.

FIELDS:
   apiVersion	<string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind	<string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata	<Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec	<Object>
     Specification of the desired behavior of the Deployment.

   status	<Object>
     Most recently observed status of the Deployment.

shpod:~# kubectl explain pod
     KIND:     Pod
     VERSION:  v1

     DESCRIPTION:
          Pod is a collection of containers that can run on a host. This resource is
          created by clients and scheduled onto hosts.

     FIELDS:
        apiVersion	<string>
          APIVersion defines the versioned schema of this representation of an
          object. Servers should convert recognized schemas to the latest internal
          value, and may reject unrecognized values. More info:
          https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

        kind	<string>
          Kind is a string value representing the REST resource this object
          represents. Servers may infer this from the endpoint the client submits
          requests to. Cannot be updated. In CamelCase. More info:
          https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

        metadata	<Object>
          Standard object's metadata. More info:
          https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

        spec	<Object>
          Specification of the desired behavior of the pod. More info:
          https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

        status	<Object>
          Most recently observed status of the pod. This data may not be up to date.
          Populated by the system. Read-only. More info:
          https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

shpod:~# kubectl explain  services
          KIND:     Service
          VERSION:  v1

          DESCRIPTION:
               Service is a named abstraction of software service (for example, mysql)
               consisting of local port (for example 3306) that the proxy listens on, and
               the selector that determines which pods will answer requests sent through
               the proxy.

          FIELDS:
             apiVersion	<string>
               APIVersion defines the versioned schema of this representation of an
               object. Servers should convert recognized schemas to the latest internal
               value, and may reject unrecognized values. More info:
               https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

             kind	<string>
               Kind is a string value representing the REST resource this object
               represents. Servers may infer this from the endpoint the client submits
               requests to. Cannot be updated. In CamelCase. More info:
               https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

             metadata	<Object>
               Standard object's metadata. More info:
               https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

             spec	<Object>
               Spec defines the behavior of a service.
               https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

             status	<Object>
               Most recently observed status of the service. Populated by the system.
               Read-only. More info:
               https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

Más completo!

shpod:~# kubectl explain  services --recursive
               KIND:     Service
               VERSION:  v1

               DESCRIPTION:
                    Service is a named abstraction of software service (for example, mysql)
                    consisting of local port (for example 3306) that the proxy listens on, and
                    the selector that determines which pods will answer requests sent through
                    the proxy.

               FIELDS:
                  apiVersion	<string>
                  kind	<string>
                  metadata	<Object>
                     annotations	<map[string]string>
                     clusterName	<string>
                     creationTimestamp	<string>
                     deletionGracePeriodSeconds	<integer>
                     deletionTimestamp	<string>
                     finalizers	<[]string>
                     generateName	<string>
                     generation	<integer>
                     labels	<map[string]string>
                     managedFields	<[]Object>
                        apiVersion	<string>
                        fieldsType	<string>
                        fieldsV1	<map[string]>
                        manager	<string>
                        operation	<string>
                        subresource	<string>
                        time	<string>
                     name	<string>
                     namespace	<string>
                     ownerReferences	<[]Object>
                        apiVersion	<string>
                        blockOwnerDeletion	<boolean>
                        controller	<boolean>
                        kind	<string>
                        name	<string>
                        uid	<string>
                     resourceVersion	<string>
                     selfLink	<string>
                     uid	<string>
                  spec	<Object>
                     allocateLoadBalancerNodePorts	<boolean>
                     clusterIP	<string>
                     clusterIPs	<[]string>
                     externalIPs	<[]string>
                     externalName	<string>
                     externalTrafficPolicy	<string>
                     healthCheckNodePort	<integer>
                     internalTrafficPolicy	<string>
                     ipFamilies	<[]string>
                     ipFamilyPolicy	<string>
                     loadBalancerClass	<string>
                     loadBalancerIP	<string>
                     loadBalancerSourceRanges	<[]string>
                     ports	<[]Object>
                        appProtocol	<string>
                        name	<string>
                        nodePort	<integer>
                        port	<integer>
                        protocol	<string>
                        targetPort	<string>
                     publishNotReadyAddresses	<boolean>
                     selector	<map[string]string>
                     sessionAffinity	<string>
                     sessionAffinityConfig	<Object>
                        clientIP	<Object>
                           timeoutSeconds	<integer>
                     type	<string>
                  status	<Object>
                     conditions	<[]Object>
                        lastTransitionTime	<string>
                        message	<string>
                        observedGeneration	<integer>
                        reason	<string>
                        status	<string>
                        type	<string>
                     loadBalancer	<Object>
                        ingress	<[]Object>
                           hostname	<string>
                           ip	<string>
                           ports	<[]Object>
                              error	<string>
                              port	<integer>
                              protocol	<string>

Un Servicio es un endpoint estable para acceder al contenedor que haya en un pod.
Puedes acceder a través de su ip. Tres formas de ver que tienes:

shpod:~# kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18d
shpod:~# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18d
shpod:~# kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18d

Un Pod es básicamente un grupo, un conjunto de contenedores, corriendo juntos en el mismo nodo.
Comparten recursos, como RAM, CPU, volúmenes de datos, red...

shpod:~# kubectl get pods
No resources found in default namespace.

Como puedes ver, no tengo pods en el namespace default, por lo que vamos a ver en otros namespaces:

shpod:~# kubectl get pods -n shpod
NAME    READY   STATUS    RESTARTS   AGE
shpod   1/1     Running   0          23h

shpod:~# kubectl get namespaces
NAME                   STATUS   AGE
default                Active   18d
kube-node-lease        Active   18d
kube-public            Active   18d
kube-system            Active   18d
kubernetes-dashboard   Active   13d
shpod                  Active   23h

shpod:~# kubectl get pods -n kubernetes-dashboard
NAME                                         READY   STATUS    RESTARTS   AGE
dashboard-metrics-scraper-5594458c94-qbght   1/1     Running   0          24h
kubernetes-dashboard-654cf69797-hrkmr        1/1     Running   0          24h

shpod:~# kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS      AGE
coredns-78fcd69978-lq24r           1/1     Running   2 (24h ago)   18d
etcd-minikube                      1/1     Running   2 (24h ago)   18d
kube-apiserver-minikube            1/1     Running   2 (24h ago)   18d
kube-controller-manager-minikube   1/1     Running   2 (24h ago)   18d
kube-proxy-79r5q                   1/1     Running   2 (24h ago)   18d
kube-scheduler-minikube            1/1     Running   2 (24h ago)   18d
storage-provisioner                1/1     Running   4 (24h ago)   18d

como podemos observar, k8s tiene varios namespaces por defecto donde corren
algunos pods.

Vamos a listar todos los pods en todos los namespaces:

shpod:~# kubectl get pods --all-namespaces
NAMESPACE              NAME                                         READY   STATUS    RESTARTS      AGE
kube-system            coredns-78fcd69978-lq24r                     1/1     Running   2 (25h ago)   18d
kube-system            etcd-minikube                                1/1     Running   2 (25h ago)   18d
kube-system            kube-apiserver-minikube                      1/1     Running   2 (25h ago)   18d
kube-system            kube-controller-manager-minikube             1/1     Running   2 (25h ago)   18d
kube-system            kube-proxy-79r5q                             1/1     Running   2 (25h ago)   18d
kube-system            kube-scheduler-minikube                      1/1     Running   2 (25h ago)   18d
kube-system            storage-provisioner                          1/1     Running   4 (25h ago)   18d
kubernetes-dashboard   dashboard-metrics-scraper-5594458c94-qbght   1/1     Running   0             25h
kubernetes-dashboard   kubernetes-dashboard-654cf69797-hrkmr        1/1     Running   0             25h
shpod                  shpod                                        1/1     Running   0             23h

Como puedes comprobar, -A es equivalente a --all-namespaces:

shpod:~# kubectl get pods -A
NAMESPACE              NAME                                         READY   STATUS    RESTARTS      AGE
kube-system            coredns-78fcd69978-lq24r                     1/1     Running   2 (25h ago)   18d
kube-system            etcd-minikube                                1/1     Running   2 (25h ago)   18d
kube-system            kube-apiserver-minikube                      1/1     Running   2 (25h ago)   18d
kube-system            kube-controller-manager-minikube             1/1     Running   2 (25h ago)   18d
kube-system            kube-proxy-79r5q                             1/1     Running   2 (25h ago)   18d
kube-system            kube-scheduler-minikube                      1/1     Running   2 (25h ago)   18d
kube-system            storage-provisioner                          1/1     Running   4 (25h ago)   18d
kubernetes-dashboard   dashboard-metrics-scraper-5594458c94-qbght   1/1     Running   0             25h
kubernetes-dashboard   kubernetes-dashboard-654cf69797-hrkmr        1/1     Running   0             25h
shpod                  shpod                                        1/1     Running   0             23h


shpod:~# kubectl get pods -n kube-public
No resources found in kube-public namespace.

Curiosamente, si listamos los pods en ese namespace kube-public, la api te dice
que no hay nada, pero sabemos que hay un único llamado configmaps.
De hecho, ejecuto este comando, la api dice que no hay nada:

shpod:~# kubectl get all -n kube-public
No resources found in kube-public namespace.

Pero si hay dicho objecto configmaps, que a su vez, contiene un objeto cluster-info

shpod:~# kubectl get configmaps -n kube-public
NAME               DATA   AGE
cluster-info       1      18d
kube-root-ca.crt   1      18d

Incluso puedo examinar la informacion de cluster-info:

shpod:~# kubectl get configmaps -n kube-public cluster-info -o yaml
apiVersion: v1
data:
  kubeconfig: |
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: MOGOLLON DE NUMEROS Y LETRAS QUE BORRO POR SEGURIDAD
        server: https://control-plane.minikube.internal:8443
      name: ""
    contexts: null
    current-context: ""
    kind: Config
    preferences: {}
    users: null
kind: ConfigMap
metadata:
  creationTimestamp: "2021-09-09T11:45:49Z"
  name: cluster-info
  namespace: kube-public
  resourceVersion: "1165"
  uid: 550649f2-1aaf-4313-9e50-e08c7d044b6d

Incluso puedo examinar la informacion de kube-root-ca.crt:

shpod:~# kubectl get configmaps -n kube-public kube-root-ca.crt -o json
  {
      "apiVersion": "v1",
      "data": {
          "ca.crt": "-----BEGIN CERTIFICATE-----\nMOGOLLON DE NUMEROS Y LETRAS QUE BORRO POR SEGURIDAD\n-----END CERTIFICATE-----\n"
      },
      "kind": "ConfigMap",
      "metadata": {
          "annotations": {
              "kubernetes.io/description": "Contains a CA bundle that can be used to verify the kube-apiserver when using internal endpoints such as the internal service IP or kubernetes.default.svc. No other usage is guaranteed across distributions of Kubernetes clusters."
          },
          "creationTimestamp": "2021-09-09T11:46:01Z",
          "name": "kube-root-ca.crt",
          "namespace": "kube-public",
          "resourceVersion": "402",
          "uid": "79f3b7ca-9d4c-4b6c-a080-6a6af987d664"
      }
  }

Cuando creas un objeto tipo Deployment, no creas un objeto tipo Pod, creas un objeto tipo
ReplicaSet que si crea un objeto tipo Pod.

Por ejemplo, el comando:

kubectl create deployment pingpong --image alpine -- ping 1.1.1.1

creará todos estos objetos:

shpod:~# kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/pingpong-98f6d5899-d9pq2   1/1     Running   0          8m59s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   19d

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   1/1     1            1           8m59s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/pingpong-98f6d5899   1         1         1       8m59s

como ves, has creado un objeto tipo Deployment, que a su vez crea un objeto tipo replicaset,
que a su vez crea un pod, que a su vez crea un contenedor Docker con una imagen alpine que está
ejecutando una y otra vez el comando ping 1.1.1.1

Esa es la jerarquia necesaria:

  Deployment
    ReplicaSet
      Pod
        Container

Vamos, que estamos haciendo mogollon de peticiones al servidor con la ip 1.1.1.1

Revisemos que está pasando...

shpod:~# kubectl logs deploy/pingpong --tail 10 --follow
PING 1.1.1.1 (1.1.1.1): 56 data bytes

Ojito, si queremos crear toda esa jerarquia que finalmente ejecute un comando bash en una imagen linux,
tienes que poner el -- al final del comando.
Es decir, ésto funcionará:

  kubectl create deployment pingpong --image alpine -- ping 1.1.1.1

Esto no:

  kubectl create deployment pingpong --image alpine ping 1.1.1.1

ni esto otro incluso!

  kubectl create deployment pingpong --image alpine

Uno podría pensar que con el último comando crearías toda la jerarquia de objetos para finalmente tener un contenedor alpine,
pero no. Si no indicas el comando que vas a ejecutar, k8s no creará adecuadamente la jerarquía de objetos.

Ahora, vamos a tratar de escalar los contenedores que hay en el pod. Uno podría pensar en principio que hay que indicar algo al Replicaset,
pero no, hay que indicar al objeto tipo Deployment que haga dicha tarea, por lo menos es así en mi version de k8s.

shpod:~# kubectl scale deployment pingpong --replicas 3
deployment.apps/pingpong scaled
shpod:~# kubectl get deployments
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
pingpong   3/3     3            3           24m
shpod:~# kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/pingpong-98f6d5899-bgrq7   1/1     Running   0          14s
pod/pingpong-98f6d5899-bzvmc   1/1     Running   0          14s
pod/pingpong-98f6d5899-d9pq2   1/1     Running   0          24m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   19d

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   3/3     3            3           24m

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/pingpong-98f6d5899   3         3         3       24m

shpod:~# kubectl logs deploy/pingpong
Found 3 pods, using pod/pingpong-98f6d5899-d9pq2
PING 1.1.1.1 (1.1.1.1): 56 data bytes

Si queremos algo más de detalle viendo los logs de los pods:

shpod:~# kubectl logs pingpong-98f6d5899-
pingpong-98f6d5899-bgrq7  pingpong-98f6d5899-bzvmc  pingpong-98f6d5899-d9pq2
shpod:~# kubectl logs pingpong-98f6d5899-bgrq7
PING 1.1.1.1 (1.1.1.1): 56 data bytes
shpod:~# kubectl logs pingpong-98f6d5899-bzvmc
PING 1.1.1.1 (1.1.1.1): 56 data bytes
shpod:~# kubectl logs pingpong-98f6d5899-d9pq2
PING 1.1.1.1 (1.1.1.1): 56 data bytes

Recuerda que tenemos una utilidad llamada watch que permite ver la salida de cualquier comando linux/unix.
En el contenedor de bret shpod viene por defecto, entre otras muchas utilidades, como helm.
Vamos a probarla mientras tratamos de borrar un contenedor usando el comando kubectl delete:

Por un lado, ejecutamos:

shpod:~# watch kubectl get pods

Aparecerá algo así:

Every 2.0s: kubectl get pods                                                                                                                       2021-09-28 17:01:44

NAME                       READY   STATUS    RESTARTS   AGE
pingpong-98f6d5899-7fnnc   1/1     Running   0          6m43s
pingpong-98f6d5899-bgrq7   1/1     Running   0          19m
pingpong-98f6d5899-bzvmc   1/1     Running   0          19m

Por otro lado, en otra terminal, ejecutamos

zsh 1646 % kubectl delete pod pingpong-98f6d5899-7fnnc
pod "pingpong-98f6d5899-7fnnc" deleted

En la primera pestaña podemos apreciar que ese pod se pone en estado Terminating e inmediatamente k8s trata de crear otro contenedor
hasta que finalmente tengamos el número de replicas indicado en el objeto ReplicaSet.

Date cuenta que debido a la jerarquía de objetos controlado por el objeto tipo Deployment, nunca podrías borrar los contenedores,
en el sentido de que quieres borrar y parar la ejecución del contenedor.
Si quieres hacer algo así, tienes que escalar a cero contenedores, usando el comando:

zsh 1649 % kubectl scale deployment pingpong --replicas 0
deployment.apps/pingpong scaled

en la pantalla donde estás ejecutando el comando watch kubectl get pods verás algo asi:

Every 2.0s: kubectl get pods                                                                                                                       2021-09-28 17:08:35

No resources found in default namespace.

zsh 1650 % kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   19d

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   0/0     0            0           51m

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/pingpong-98f6d5899   0         0         0       51m

shpod:~# exit
logout
Session ended, resume using 'kubectl attach shpod -c shpod -i -t' command when the pod is running

Tareas cron.

zsh 1715 % kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/cronjob.yaml
cronjob.batch/hello created

El fichero yaml es así:

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure

En otra pestaña, ejecutando watch kubectl get cronjobs:

Every 2.0s: kubectl get cronjobs                                                                                                                   2021-09-29 14:35:55

NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   False     0        <none>          46s

Podemos ver como se crea un pod nuevo ejecutando el comando sh descrito en el cronjob.yaml,
cada minuto uno nuevo...

Para borrar la tarea cronjob:

zsh 1718 % kubectl delete cronjobs.batch hello
cronjob.batch "hello" deleted

Tener en cuenta que un cronjob creará un Job que a su vez creará un pod, donde se alojará el contenedor.

Podemos comprobar los logs de todos los pods a la vez.
La clave es usar app=pingpong

zsh 1730 % kubectl logs -l app=pingpong --tail 1 -f
PING 1.1.1.1 (1.1.1.1): 56 data bytes
PING 1.1.1.1 (1.1.1.1): 56 data bytes
PING 1.1.1.1 (1.1.1.1): 56 data bytes

zsh 1731 [130] % kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/pingpong-98f6d5899-9v85n   1/1     Running   0          5h17m
pod/pingpong-98f6d5899-s2pd4   1/1     Running   0          5h17m
pod/pingpong-98f6d5899-z784p   1/1     Running   0          5h17m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   20d

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   3/3     3            3           22h

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/pingpong-98f6d5899   3         3         3       22h

Ojo, que tiene un límite, como máximo 5!

Voy a escalar los pods hasta 8,

Every 2.0s: kubectl get pods                                                                                                                       2021-09-29 15:09:23

NAME                       READY   STATUS    RESTARTS   AGE
pingpong-98f6d5899-2584f   1/1     Running   0          80s
pingpong-98f6d5899-9v85n   1/1     Running   0          5h22m
pingpong-98f6d5899-d2m8r   1/1     Running   0          80s
pingpong-98f6d5899-mcpnm   1/1     Running   0          80s
pingpong-98f6d5899-n7c6m   1/1     Running   0          80s
pingpong-98f6d5899-r5ftn   1/1     Running   0          80s
pingpong-98f6d5899-s2pd4   1/1     Running   0          5h22m
pingpong-98f6d5899-z784p   1/1     Running   0          5h22m

zsh 1734 % kubectl logs -l app=pingpong --tail 1 -f
error: you are attempting to follow 8 log streams, but maximum allowed concurrency is 5, use --max-log-requests to increase the limit

Como podemos ver, kubectl logs tiene limitaciones a la hora de acceder a los logs, pero es por una buena razón, para proteger los recursos de
la api. Para ello, podemos usar stern.

shpod:~# stern --tail 1 --timestamps --all-namespaces pingpong
+ default pingpong-98f6d5899-z784p › alpine
+ default pingpong-98f6d5899-d2m8r › alpine
+ default pingpong-98f6d5899-n7c6m › alpine
+ default pingpong-98f6d5899-s2pd4 › alpine
+ default pingpong-98f6d5899-mcpnm › alpine
default pingpong-98f6d5899-z784p alpine 2021-09-29T09:47:23.831054441Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
+ default pingpong-98f6d5899-r5ftn › alpine
default pingpong-98f6d5899-d2m8r alpine 2021-09-29T15:08:12.905561352Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
+ default pingpong-98f6d5899-9v85n › alpine
+ default pingpong-98f6d5899-2584f › alpine
default pingpong-98f6d5899-n7c6m alpine 2021-09-29T15:08:08.819615419Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-s2pd4 alpine 2021-09-29T09:47:26.665132690Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-r5ftn alpine 2021-09-29T15:08:11.594289256Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-mcpnm alpine 2021-09-29T15:08:07.562812985Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-9v85n alpine 2021-09-29T09:47:25.291573172Z PING 1.1.1.1 (1.1.1.1): 56 data bytes
default pingpong-98f6d5899-2584f alpine 2021-09-29T15:08:10.149623566Z PING 1.1.1.1 (1.1.1.1): 56 data bytes

Some questions...

How many nodes does your cluster have?
(the answer should be the kubectl command you used to get the answer)

  Probably it is not necessary to use -o wide.

  shpod:~# kubectl get nodes -o wide
  NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
  minikube   Ready    control-plane,master   20d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker://20.10.8

What kernel version and what container engine is each node running?
(the answer should be the kubectl command you used to get the answer)

  shpod:~# kubectl get nodes -o wide
  NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
  minikube   Ready    control-plane,master   20d   v1.22.1   192.168.64.3   <none>        Buildroot 2021.02.4   4.19.202         docker://20.10.8

List only the pods in the kube-system namespace.
(the answer should be the kubectl command you used to get the answer)

  shpod:~# kubectl get pods -n kube-system
  NAME                               READY   STATUS    RESTARTS        AGE
  coredns-78fcd69978-lq24r           1/1     Running   2 (2d23h ago)   20d
  etcd-minikube                      1/1     Running   2 (2d23h ago)   20d
  kube-apiserver-minikube            1/1     Running   2 (2d23h ago)   20d
  kube-controller-manager-minikube   1/1     Running   2 (2d23h ago)   20d
  kube-proxy-79r5q                   1/1     Running   2 (2d23h ago)   20d
  kube-scheduler-minikube            1/1     Running   2 (2d23h ago)   20d
  storage-provisioner                1/1     Running   4 (2d23h ago)   20d

Explain the role of some of these pods.

  coredns-78fcd69978-lq24r is the dns server of k8s. basically it translates full qualified domain names to ip addresses.

  etcd-minikube is basically a distributed high available key/value system where we store important k8s data.
  The nodes, the pods, every object and relevant data.

  kube-apiserver-minikube is basically a control plane component, a rest server which exposes the k8s api-server,
  which interacts with etcd component. It is designed for high availability, as everything i k8s.

  kube-controller-manager-minikube is basically another control plane component which controls every controller in k8s. It is compiled as a
  only one component, for eficiency motives.

  It controls the nodes, like when a new node comes into service or it goes down, the replicasets,
  controlling when a pod is added or deleted from the replicaset object itself,
  the endpoints between services and pods, i mean, when k8s adds an ip address to that pod,
  it controls too the tokens and account services, so that the api server can access to new namespaces...


If there are few or no pods in kube-system, why could that be?

  By default, i understand that above described are the minimum components.
  One less should indicate a system malfunction, or,

  (reychel-nicole-zuniga-rojas)
  On some clusters, the control plane is located outside the cluster itself, and not running as containers.

  In that case, the control plane won't show up in kube-system, but you can find on host with

  ps aux | grep kube.

Create a deployment using kubectl create that runs the image bretfisher/clock and name it ticktock.
  (the answer should be the kubectl command you used)

  kubectl create deployment ticktock --images=bretfisher/clock --

Increase the number of pods running in that deployment to three.
(the answer should be the kubectl command you used)

  zsh 1742 % kubectl scale deployment ticktock --replicas=3
  deployment.apps/ticktock scaled

Use a selector to output only the last line of logs of each container.
(the answer should be the kubectl command you used)

  zsh 1754 [1] % kubectl logs deployment/ticktock --tail=1 -f
  Found 3 pods, using pod/ticktock-5cc6bf699-ks67z
  Thu Sep 30 08:12:45 UTC 2021
  Thu Sep 30 08:12:46 UTC 2021
  Thu Sep 30 08:12:47 UTC 2021
  ...

  shpod:~# stern ticktock --tail 1 -t -A

  // Probably this is what Bret asked...
  kubectl logs --selector=app=ticktock --tail=1
  kubectl logs -l app=ticktock --tail 1

Exponiendo al mundo exterior esos contenedores que corren en pods.

  Para ello usamos los servicios. Basicamente es asignar una direccion ip al pod.
  Ojito, se asignan al pod, no al contenedor, como si ocurre con Docker.
  Una vez que el servicio está creado, el componente CoreDNS se encargará de
  traducir el nombre del servicio a la ip asignada al pod.

  Tenemos cuatro tipos de servicios:

  clusterIP o nodos internos inaccesibles desde el exterior: La ip asignada al servicio sólo puede ser accesible desde el
            interior del cluster (nodos y puertos).
            Se puede acceder al puerto asignado inicialmente por k8s.
            Es el servicio por defecto cuando usamos el comando expose.
            Pueden ser ideales para conformar los nodos de un cluster kafka, o spark, pues
            estas tecnologías tienen nodos que los componen.
            Idealmente, solo tendrías que exponer al exterior (NodePort?) el Driver spark
            , el del dashboard, el maestro kafka

  NodePort o nodos accesibles desde el exterior: La ip asignada puede ser accesible desde el interior y el exterior.
            Te asignan un puerto de rango superior (30000-32768).
            El código debe ser cambiado para acceder a ese puerto. -> Bret, que cojones quieres decir con ésto...

  LoadBalancer: Como su nombre indica, se usa para acceder a un NodePort de manera balanceada.
                Solo se puede usar a través de una herramienta de terceros (raro) aws, gcp, Azure
  ExternalName:

  Una vez que tenemos claro que tenemos que crear un servicio para poder exponer
  al mundo un contenedor corriendo en un pod, tenemos que usar el comando kubectl expose.
  Este comando crea un servicio.

  Vamos a tratar de exponer un clusterIp. Creamos un objeto deployment y lo escalamos a 10 instancias.
  httpenv es un pequeño servidor web que usa el puerto 8888.

  zsh 1774 % kubectl create deployment httpenv --image bretfisher/httpenv
  deployment.apps/httpenv created

  zsh 1776 % kubectl logs -l app=httpenv --follow
  Starting httpenv listening on port 8888.

  zsh 1778 [130] % kubectl scale deployment httpenv --replicas 10
  deployment.apps/httpenv scaled

  Si hacemos un watch, podemos ver los logs, también podemos usar stern para ver los logs.

  shpod:~# kubectl get pods -w
  NAME                       READY   STATUS    RESTARTS   AGE
  httpenv-6fdc8554fb-79dlf   1/1     Running   0          7m6s
  httpenv-6fdc8554fb-7dg5m   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-cw22l   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-f7h7j   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-kpj8b   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-ngrjg   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-np77c   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-nwk6r   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-tkdrv   1/1     Running   0          5m13s
  httpenv-6fdc8554fb-wgtwx   1/1     Running   0          5m13s

  <aironman@MacBook-Pro-de-Alonso:~>
  zsh 1782 [1] % stern httpenv --tail 1 -t -A
  + default httpenv-6fdc8554fb-cw22l › httpenv
  + default httpenv-6fdc8554fb-ngrjg › httpenv
  + default httpenv-6fdc8554fb-kpj8b › httpenv
  + default httpenv-6fdc8554fb-wgtwx › httpenv
  + default httpenv-6fdc8554fb-nwk6r › httpenv
  + default httpenv-6fdc8554fb-np77c › httpenv
  + default httpenv-6fdc8554fb-7dg5m › httpenv
  + default httpenv-6fdc8554fb-79dlf › httpenv
  + default httpenv-6fdc8554fb-tkdrv › httpenv
  + default httpenv-6fdc8554fb-f7h7j › httpenv
  default httpenv-6fdc8554fb-kpj8b httpenv 2021-10-01T11:39:33.853969232+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-ngrjg httpenv 2021-10-01T11:39:28.381945077+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-79dlf httpenv 2021-10-01T11:37:29.710767791+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-nwk6r httpenv 2021-10-01T11:39:24.749041061+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-7dg5m httpenv 2021-10-01T11:39:31.261413710+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-np77c httpenv 2021-10-01T11:39:25.722419797+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-cw22l httpenv 2021-10-01T11:39:35.150719857+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-f7h7j httpenv 2021-10-01T11:39:27.088736244+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-tkdrv httpenv 2021-10-01T11:39:32.492885962+02:00 Starting httpenv listening on port 8888.
  default httpenv-6fdc8554fb-wgtwx httpenv 2021-10-01T11:39:29.749934080+02:00 Starting httpenv listening on port 8888.


  zsh 1783 % kubectl expose deployment httpenv --port 8888
  service/httpenv exposed

  Vemos como hacer un expose, crea un servicio ClusterIp por defecto:

  zsh 1784 % kubectl get service
  NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
  httpenv      ClusterIP   10.98.94.183   <none>        8888/TCP   51s
  kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP    21d

  Vamos a probar el servicio clusterIp.

  Recuerda que esto solo funcionará si estás lanzando los comando dentro del cluster.
  También recordar que vamos a hacer peticiones mediante curl al servidor web httpenv que está
  corriendo en 10 contenedores, por lo que, por defecto el servicio usará a través del kube-proxy
  un algoritmo round robin para hacer peticiones, es decir, al hacer curl, hará una petición, y al hacer
  curl otra vez preguntará al siguiente, y asi hasta que lleguemos a la peticion 11 que volverá a preguntar
  al primer contenedor. Round-Robin no es el unico algoritmo que kube-proxy usará para acceder a los contenedores.

  Averiguemos la ip, bueno, la parseamos para tenerla en una variable.
  Si te fijas, vemos que es más fácil averiguarla haciendo un kubectl get svc

  shpod:~# IP=$(kubectl get svc httpenv -o go-template --template '{{ .spec.clusterIP }}')
  shpod:~# echo $IP
  10.98.94.183
  shpod:~# curl http://10.98.94.183:8888/
  {"HOME":"/root","HOSTNAME":"httpenv-6fdc8554fb-7dg5m","KUBERNETES_PORT":"tcp://10.96.0.1:443","KUBERNETES_PORT_443_TCP":"tcp://10.96.0.1:443","KUBERNETES_PORT_443_TCP_ADDR":"10.96.0.1","KUBERNETES_PORT_443_TCP_PORT":"443","KUBERNETES_PORT_443_TCP_PROTO":"tcp","KUBERNETES_SERVICE_HOST":"10.96.0.1","KUBERNETES_SERVICE_PORT":"443","KUBERNETES_SERVICE_PORT_HTTPS":"443","PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"}shpod:~#

  Vemos que la peticion curl saca mucha info. Podemos usar jq para quedarnos con algo que nos interese.
  shpod:~# curl http://10.98.94.183:8888/ | jq .HOSTNAME
    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                   Dload  Upload   Total   Spent    Left  Speed
  100   438  100   438    0     0   108k      0 --:--:-- --:--:-- --:--:--  142k
  "httpenv-6fdc8554fb-cw22l"
  shpod:~# curl http://10.98.94.183:8888/ | jq .KUBERNETES_PORT
    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                   Dload  Upload   Total   Spent    Left  Speed
  100   438  100   438    0     0   176k      0 --:--:-- --:--:-- --:--:--  427k
  "tcp://10.96.0.1:443"
  shpod:~# curl http://10.98.94.183:8888/ | jq .KUBERNETES_PORT_TCP
    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                   Dload  Upload   Total   Spent    Left  Speed
  100   438  100   438    0     0   179k      0 --:--:-- --:--:-- --:--:--  427k
  null

  Basicamente, usar round robin en el proxy implica que estamos usando un servicio de tipo LoadBalancer,
  pero hay veces que no querremos usar un balanceador de carga, si no que querremos acceder directamente
  al contenedor. Querremos usar un servicio headless.
  Para hacerlo, tenemos que usar algo así:

  kubectl expose deployment httpenv-none --port 8888 --cluster-ip=None

  como resultado, el servicio no tendrá asignado una direccion ip virtual, por lo que no kube-proxy tampoco aplicará
  round-robin para acceder al contenedor, por lo que CoreDNS devolverá las distintas direcciones ip de los contenedores
  como registros. Algo así nos sirve para averiguar las replicas que tiene el contenedor.

  Si intentas ejecutar el comando anterior sobre un objeto deployment ya existente, te dará error:

  shpod:~# kubectl expose deployment httpenv --port 8888 --cluster-ip=None
  Error from server (AlreadyExists): services "httpenv" already exists

  Vamos a crear lo necesario:

  shpod:~# kubectl create deployment httpenv-none --image bretfisher/httpenv
  deployment.apps/httpenv-none created

  shpod:~# kubectl scale deployment httpenv-none --replicas 5
  deployment.apps/httpenv-none scaled

  shpod:~# kubectl expose deployment httpenv-none --port 8888 --cluster-ip=None
  service/httpenv-none exposed

  shpod:~# kubectl get deployments
  NAME           READY   UP-TO-DATE   AVAILABLE   AGE
  httpenv        10/10   10           10          98m
  httpenv-none   5/5     5            5           58s
  ticktock       3/3     3            3           27h

  Podemos apreciar que el servicio no tiene asignado una direccion ip,
  kube-proxy no accederá a él.
  por lo que, como cojones podemos acceder a él?

  https://kubernetes.io/docs/concepts/services-networking/service/#headless-services

  shpod:~# kubectl get services
  NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
  httpenv        ClusterIP   10.98.94.183   <none>        8888/TCP   84m
  httpenv-none   ClusterIP   None           <none>        8888/TCP   31s
  kubernetes     ClusterIP   10.96.0.1      <none>        443/TCP    21d

  Un servicio tiene un número determinado y gestionado por k8s de endpoints.
  Un endpoint es la combinacion de anfitrion y puerto, direccion ip y puerto.
  k8s actualizará continuamente estos endpoints.

  shpod:~# kubectl describe service httpenv
Name:              httpenv
Namespace:         default
Labels:            app=httpenv
Annotations:       <none>
Selector:          app=httpenv
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.98.94.183
IPs:               10.98.94.183
Port:              <unset>  8888/TCP
TargetPort:        8888/TCP
Endpoints:         172.17.0.10:8888,172.17.0.11:8888,172.17.0.12:8888 + 7 more...
Session Affinity:  None
Events:            <none>

Y vemos el otro despliegue, ya puestos...

shpod:~# kubectl describe service httpenv-none
Name:              httpenv-none
Namespace:         default
Labels:            app=httpenv-none
Annotations:       <none>
Selector:          app=httpenv-none
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                None
IPs:               None
Port:              <unset>  8888/TCP
TargetPort:        8888/TCP
Endpoints:         172.17.0.19:8888,172.17.0.20:8888,172.17.0.21:8888 + 2 more...
Session Affinity:  None
Events:            <none>

inmediatamente te fijas en el apartado Endpoints, no te muestra todos...
Para ello, lanzamos:

shpod:~# kubectl get endpoints
NAME           ENDPOINTS                                                        AGE
httpenv        172.17.0.10:8888,172.17.0.11:8888,172.17.0.12:8888 + 7 more...   105m
httpenv-none   172.17.0.19:8888,172.17.0.20:8888,172.17.0.21:8888 + 2 more...   21m
kubernetes     192.168.64.3:8443                                                21d

shpod:~# kubectl get endpoints -o wide
NAME           ENDPOINTS                                                        AGE
httpenv        172.17.0.10:8888,172.17.0.11:8888,172.17.0.12:8888 + 7 more...   105m
httpenv-none   172.17.0.19:8888,172.17.0.20:8888,172.17.0.21:8888 + 2 more...   21m
kubernetes     192.168.64.3:8443                                                21d

No se ven todos, para ello mejor usar:

shpod:~# kubectl get endpoints -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2021-10-01T09:51:49Z"
    creationTimestamp: "2021-10-01T09:51:49Z"
    labels:
      app: httpenv
    name: httpenv
    namespace: default
    resourceVersion: "193961"
    uid: fcacc51d-05ef-4a23-9e67-b933bea173cc
  subsets:
  - addresses:
    - ip: 172.17.0.10
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-nwk6r
        namespace: default
        resourceVersion: "193389"
        uid: 9a3caeae-daaa-4231-b1f0-fe9b1bb9addb
    - ip: 172.17.0.11
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-np77c
        namespace: default
        resourceVersion: "193395"
        uid: 3ed504e0-47aa-4349-a78b-52b395f588db
    - ip: 172.17.0.12
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-ngrjg
        namespace: default
        resourceVersion: "193409"
        uid: 03991f72-2d53-4ce9-b67a-22fca2ad22e8
    - ip: 172.17.0.13
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-kpj8b
        namespace: default
        resourceVersion: "193437"
        uid: 15b4d296-fa1c-4330-9367-996a15a42d1f
    - ip: 172.17.0.14
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-wgtwx
        namespace: default
        resourceVersion: "193417"
        uid: ef608ff1-55b4-46b3-968e-aca15a56b39a
    - ip: 172.17.0.15
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-cw22l
        namespace: default
        resourceVersion: "193444"
        uid: 7e3440a9-a0d3-452b-9612-5af58e592584
    - ip: 172.17.0.16
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-tkdrv
        namespace: default
        resourceVersion: "193431"
        uid: 599de533-7de2-47b0-b1f5-860438ad00e8
    - ip: 172.17.0.17
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-f7h7j
        namespace: default
        resourceVersion: "193402"
        uid: ac01c364-6655-4764-a03b-cf98b13453f6
    - ip: 172.17.0.18
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-7dg5m
        namespace: default
        resourceVersion: "193422"
        uid: 8da0b27c-be2b-4015-86d2-ae2c67b212f8
    - ip: 172.17.0.9
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-6fdc8554fb-79dlf
        namespace: default
        resourceVersion: "193239"
        uid: 2c20d45d-8175-4975-87f9-679fbff467f4
    ports:
    - port: 8888
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2021-10-01T11:15:50Z"
    creationTimestamp: "2021-10-01T11:15:50Z"
    labels:
      app: httpenv-none
      service.kubernetes.io/headless: ""
    name: httpenv-none
    namespace: default
    resourceVersion: "197571"
    uid: 81064e1f-879b-4434-bfe1-2cfcb41f61aa
  subsets:
  - addresses:
    - ip: 172.17.0.19
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-fc8zh
        namespace: default
        resourceVersion: "197482"
        uid: c623fb82-3027-4cef-a588-7463ae3f767a
    - ip: 172.17.0.20
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-5dvzd
        namespace: default
        resourceVersion: "197535"
        uid: a9a3f0ad-abcd-4d16-a821-a3fe4ba36292
    - ip: 172.17.0.21
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-95t8j
        namespace: default
        resourceVersion: "197549"
        uid: f3a34a7e-b165-495d-85d1-9f9c7f57fc5d
    - ip: 172.17.0.22
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-ncwx8
        namespace: default
        resourceVersion: "197541"
        uid: fe5f0594-2fbc-45b4-b06a-d2f15a291ee6
    - ip: 172.17.0.23
      nodeName: minikube
      targetRef:
        kind: Pod
        name: httpenv-none-5d8c77785f-n4pfg
        namespace: default
        resourceVersion: "197555"
        uid: 62e2571d-93ca-49f6-a7d8-0a7b412c2e33
    ports:
    - port: 8888
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: "2021-09-09T11:45:48Z"
    labels:
      endpointslice.kubernetes.io/skip-mirror: "true"
    name: kubernetes
    namespace: default
    resourceVersion: "210"
    uid: 6ce695ed-ea95-45a0-8645-045cd27ccee7
  subsets:
  - addresses:
    - ip: 192.168.64.3
    ports:
    - name: https
      port: 8443
      protocol: TCP
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

Los pods deben coincidir:

shpod:~# kubectl get pods -l app=httpenv -o wide
NAME                       READY   STATUS    RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
httpenv-6fdc8554fb-79dlf   1/1     Running   0          120m   172.17.0.9    minikube   <none>           <none>
httpenv-6fdc8554fb-7dg5m   1/1     Running   0          118m   172.17.0.18   minikube   <none>           <none>
httpenv-6fdc8554fb-cw22l   1/1     Running   0          118m   172.17.0.15   minikube   <none>           <none>
httpenv-6fdc8554fb-f7h7j   1/1     Running   0          118m   172.17.0.17   minikube   <none>           <none>
httpenv-6fdc8554fb-kpj8b   1/1     Running   0          118m   172.17.0.13   minikube   <none>           <none>
httpenv-6fdc8554fb-ngrjg   1/1     Running   0          118m   172.17.0.12   minikube   <none>           <none>
httpenv-6fdc8554fb-np77c   1/1     Running   0          118m   172.17.0.11   minikube   <none>           <none>
httpenv-6fdc8554fb-nwk6r   1/1     Running   0          118m   172.17.0.10   minikube   <none>           <none>
httpenv-6fdc8554fb-tkdrv   1/1     Running   0          118m   172.17.0.16   minikube   <none>           <none>
httpenv-6fdc8554fb-wgtwx   1/1     Running   0          118m   172.17.0.14   minikube   <none>           <none>

Recuerda que hemos estado trabajando hasta ahora con un servicio de tipo clusterIp, que solo sirve para trafico interno del cluster.
No sirve para exponer servicios al mundo exterior. Para ello necesitamos servicios de tipo:

  NodePort: expone un servicio a través de TCP usando un puerto en el rango 30000-32768
  LoadBalancer: Expone un balanceador de carga en el cluster para acceder a ese servicio. Por defecto usa RoundRobin.
  ExternalIP: Usa una direccion IP externa. Te deja asignar una direccion Ip física al nodo.Por definir más adelante.
  Ingress: Es un mecanismo especial para servicios HTTP de tipo reverse proxy.
